{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8891c7ce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0ff7e4-cf5e-4586-a1f7-eab01ae63a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os # read and manipulate local files\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score as f1_score_report\n",
    "\n",
    "\n",
    "import random\n",
    "# from PIL import Image\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import tensorflow.keras as ks\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.applications import xception, MobileNet, MobileNetV2, Xception, Xception, EfficientNetV2M, InceptionV3\n",
    "# from tensorflow.keras.applications import EfficientNetB2\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from imgaug import augmenters as iaa # elastic_deformation\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Dense, BatchNormalization, Concatenate, GlobalAveragePooling2D\n",
    "from keras.initializers import RandomNormal# weights will be initialized according to a Gaussian distribution \n",
    "\n",
    "# hide wornings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "################################################################################################\n",
    "# SETTING F1 SCORE\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "f1_score = F1Score(num_classes=13, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f806ed-e9db-4199-b7ee-c2fa11bd0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'parallelViT'#'T2TViT'#    'InceptionV3'##'T2TViT', , 'MobileNet', 'Xception', \n",
    "\n",
    "################################################################################################\n",
    "\n",
    "## SEETING THE PATHS\n",
    "PATH_TO_CODE =''\n",
    "sys.path.append(PATH_TO_CODE)\n",
    "DATASET_PATH = 'paddy-doctor-diseases-small-split/'\n",
    "# DATASET_PATH =  '../paddy-doctor-diseases-small-400-split/'\n",
    "DATASET_PATH_TRAIN = os.path.join(DATASET_PATH, 'train')\n",
    "DATASET_PATH_TEST = os.path.join(DATASET_PATH, 'test')\n",
    "\n",
    "\n",
    "PATH_TO_SAVE_RESULT = os.path.join(PATH_TO_CODE, 'saved_outputs_basic_augment', model_name)\n",
    "PATH_BEST_SAVE_WEIGHT = os.path.join(PATH_TO_SAVE_RESULT,'saved_weights')\n",
    "PATH_TO_SAVE_MODEL = os.path.join(PATH_TO_SAVE_RESULT, 'saved_models')\n",
    "PATH_SAVE_HISTORY = os.path.join(PATH_TO_SAVE_RESULT, 'training_history.csv')\n",
    "PATH_SAVE_TIME = os.path.join(PATH_TO_SAVE_RESULT, 'training_time.csv')\n",
    "# PATH_TO_SAVE_TUNER = os.path.join(PATH_TO_RESULT, 'saved_tuner_model')\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "ORIGINAL_IMAGE_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
    "COLOR_CHANNEL = 3\n",
    "\n",
    "RESIZE_SHAPE = (128, 128)\n",
    "MODEL_INPUT_SIZE = (RESIZE_SHAPE[0], RESIZE_SHAPE[1], COLOR_CHANNEL)\n",
    "\n",
    "VALIDATION_SPLIT= 0.2\n",
    "NUM_CLASSES = 13 #len(target_labels) # 13\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 139 #120\n",
    "# EXTRA_RUN_AFTER_BEST_EPOCH = 10\n",
    "\n",
    "################################################################################################\n",
    "AUGMENT = True\n",
    "AUGMENT_TYPE ='basic' #'all' # basic, advanced,\n",
    "################################################################################################\n",
    "\n",
    "# METRICS_TO_RECORD = 'short'#'full' #'short'  # \"full\" or 'short'\n",
    "SAVE_RESULTS = True\n",
    "SHOW_RESULTS = True\n",
    "\n",
    "################################################################################################\n",
    "# Setting the seed\n",
    "SEED  = 123\n",
    "RNG = np.random.default_rng(SEED) # Random number generator\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "################################################################################################\n",
    "# Checkpoint parameters\n",
    "SCORE_TO_MONITOR = 'val_f1_score' # Score that checkpoints monitor during training\n",
    "SCORE_OBJECTIVE  = 'max'          # 'max' or 'min', specifies whether the objective is to maximize the score or minimize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc029c0-dfb5-4571-a579-7fe7ca89cc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_PATH paddy-doctor-diseases-small-split/\n",
      "DATASET_PATH_TRAIN paddy-doctor-diseases-small-split/train\n",
      "DATASET_PATH_TEST paddy-doctor-diseases-small-split/test\n",
      "PATH_TO_SAVE_RESULT saved_outputs_basic_augment\\parallelViT\n",
      "PATH_BEST_SAVE_WEIGHT saved_outputs_basic_augment\\parallelViT\\saved_weights\n",
      "PATH_TO_SAVE_MODEL saved_outputs_basic_augment\\parallelViT\\saved_models\n"
     ]
    }
   ],
   "source": [
    "print(\"DATASET_PATH\", DATASET_PATH)\n",
    "print(\"DATASET_PATH_TRAIN\", DATASET_PATH_TRAIN)\n",
    "print(\"DATASET_PATH_TEST\", DATASET_PATH_TEST)\n",
    "\n",
    "print(\"PATH_TO_SAVE_RESULT\",PATH_TO_SAVE_RESULT)\n",
    "print(\"PATH_BEST_SAVE_WEIGHT\", PATH_BEST_SAVE_WEIGHT)\n",
    "print(\"PATH_TO_SAVE_MODEL\", PATH_TO_SAVE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae2d29-d089-412a-88a5-7b5017dee4f8",
   "metadata": {},
   "source": [
    "## Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f4e78e-2715-4ec6-b86d-a032075066a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mahri\\AppData\\Local\\Temp\\ipykernel_9772\\984659479.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU Available:  True\n",
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30676387-9bdf-4015-83e8-db47734137d4",
   "metadata": {},
   "source": [
    "## Image Data Augmentation Options in Keras `ImageDataGenerator`\n",
    "\n",
    "The Keras `ImageDataGenerator` offers various options for image augmentation, enhancing training dataset diversity and improving model robustness. Below is a list of common augmentations available:\n",
    "\n",
    "1. **`rotation_range`**: Degrees range (0-180) for random rotations.\n",
    "\n",
    "2. **`width_shift_range` & `height_shift_range`**: Fraction of total width or height, or an integer number of pixels, for horizontal or vertical image shift.\n",
    "\n",
    "3. **`brightness_range`**: Tuple specifying a range for random brightness adjustment. Values < 1 darken the image, > 1 brighten it.\n",
    "\n",
    "4. **`shear_range`**: Shear Intensity (angle in counter-clockwise direction as radians) for shear transformations.\n",
    "\n",
    "5. **`zoom_range`**: Range for random zoom. If a float, `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n",
    "\n",
    "6. **`channel_shift_range`**: Range for random channel (color) shifts.\n",
    "\n",
    "7. **`fill_mode`**: Mode for filling points outside boundaries ('constant', 'nearest', 'reflect', 'wrap') after transformations.\n",
    "\n",
    "8. **`cval`**: Value for filling points outside boundaries when `fill_mode` is 'constant'.\n",
    "\n",
    "9. **`horizontal_flip` & `vertical_flip`**: Boolean. Randomly flip images horizontally or vertically.\n",
    "\n",
    "10. **`rescale`**: Rescaling factor, often 1/255 to scale pixel values to [0, 1].\n",
    "\n",
    "11. **`preprocessing_function`**: Function to be applied to each input after resizing and augmenting.\n",
    "\n",
    "12. **`validation_split`**: Fraction of images reserved for validation (between 0 and 1).\n",
    "\n",
    "13. **`dtype`**: Dtype to use for generated arrays.\n",
    "\n",
    "Note: The appropriateness of these augmentations depends on the specific data and use case. Some, like flipping, may not suit certain types of images (e.g., text)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dada5c-1141-4914-96bd-a8ff36749e6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ElasticTransformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce2108f-7c14-40de-b590-d7e14856ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an augmentation pipeline\n",
    "# Alpha parameter controls the intensity of the deformation\n",
    "# Sigma controls the smoothness of the deformation field.\n",
    "aug = iaa.Sequential([  \n",
    "    iaa.ElasticTransformation(alpha=50, sigma=10)  # Apply elastic transformations , sigma=1\n",
    "])\n",
    "\n",
    "def elastic_deformation(image):\n",
    "    image_aug = aug(image=image)\n",
    "    return image_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e823091d-ce18-4444-845c-8a9bc459800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for creating the generators with given augmentation type\n",
    "# images are alreagy rescaled--so no rescaling is needed\n",
    "def create_generators(dataset_path_train=DATASET_PATH_TRAIN, \n",
    "                      dataset_path_test=DATASET_PATH_TEST, \n",
    "                      valid_ratio=VALIDATION_SPLIT, \n",
    "                      augment= False, \n",
    "                      augment_type='all'):\n",
    "   \n",
    "    # If augmentation is True, apply data augmentation. Otherwise, only rescale.\n",
    "    if augment:\n",
    "        if augment_type == 'all':     \n",
    "            datagen = ImageDataGenerator(\n",
    "                rescale=1./255, # Normalize images\n",
    "                validation_split=valid_ratio,  # Split ratio for validation set\n",
    "\n",
    "                # Geometric Transformations\n",
    "                rotation_range=40,  # degrees\n",
    "                width_shift_range=0.2,  # fraction of total width\n",
    "                height_shift_range=0.3,  # fraction of total height\n",
    "                shear_range=0.4,  # shear angle in counter-clockwise direction as radians\n",
    "                zoom_range=0.25,  # zoom range for random zoom\n",
    "                horizontal_flip=True,  # randomly flip images horizontally\n",
    "                vertical_flip=True,  # randomly flip images vertically\n",
    "                \n",
    "                # Pixel-Level Transformations\n",
    "                brightness_range=[0.6, 1.1],  # range for picking a brightness shift value\n",
    "                channel_shift_range=0.3,  # range for random channel shifts\n",
    "\n",
    "                # Advanced functions\n",
    "                preprocessing_function=elastic_deformation\n",
    "            )\n",
    "        elif augment_type == 'basic':     \n",
    "            datagen = ImageDataGenerator(\n",
    "                rescale=1./255, # Normalize images\n",
    "                validation_split=valid_ratio,  # Split ratio for validation set\n",
    "\n",
    "                zoom_range=0.2,  # zoom range for random zoom\n",
    "                horizontal_flip=True,  # randomly flip images horizontally\n",
    "            )\n",
    "        elif augment_type == 'advanced':     \n",
    "            datagen = ImageDataGenerator(\n",
    "                rescale=1./255, # Normalize images\n",
    "                validation_split=valid_ratio,  # Split ratio for validation set\n",
    "\n",
    "                # Advanced functions\n",
    "                preprocessing_function=elastic_deformation\n",
    "            )\n",
    "            \n",
    "    else:\n",
    "        # No augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "            validation_split=valid_ratio,  # Split ratio for validation set\n",
    "            rescale=1./255, # Normalize images\n",
    "        )\n",
    "    \n",
    "    # Create training generator\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        dataset_path_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed = SEED,\n",
    "        class_mode=\"categorical\",  # Use \"categorical\" for multi-class classification\n",
    "        target_size=RESIZE_SHAPE,  # Adjust target size based on your model requirements\n",
    "        shuffle = True,\n",
    "        color_mode = 'rgb',\n",
    "        subset='training'  # Specify subset as 'training'\n",
    "    )\n",
    "    \n",
    "    # Create validation generator\n",
    "    valid_generator = datagen.flow_from_directory(\n",
    "        dataset_path_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed = SEED,\n",
    "        class_mode=\"categorical\",  # Use \"categorical\" for multi-class classification\n",
    "        target_size=RESIZE_SHAPE,  # Adjust target size based on your model requirements        \n",
    "        shuffle = True,        \n",
    "        color_mode = 'rgb',\n",
    "        subset='validation'  # Specify subset as 'validation'\n",
    "    )\n",
    "    \n",
    "    # For the test set, assuming no augmentation, just rescaling\n",
    "    test_datagen = ImageDataGenerator(\n",
    "                rescale=1./255# Normalize images\n",
    "    )\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        dataset_path_test,\n",
    "        class_mode=\"categorical\",  # Use \"categorical\" for multi-class classification\n",
    "        target_size=RESIZE_SHAPE,  # Adjust target size based on your model requirements        \n",
    "        seed = SEED,\n",
    "        shuffle = False,        \n",
    "        color_mode = 'rgb',\n",
    "        batch_size=BATCH_SIZE)\n",
    "    \n",
    "    return train_generator, valid_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "269c38da-a92b-47d1-918c-3b19f84aa19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10389 images belonging to 13 classes.\n",
      "Found 2591 images belonging to 13 classes.\n",
      "Found 3245 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "# Creating the generators using functions\n",
    "train_generator, valid_generator, test_generator = create_generators(dataset_path_train=DATASET_PATH_TRAIN,\n",
    "                                                                     dataset_path_test = DATASET_PATH_TEST,\n",
    "                                                                     augment=AUGMENT,\n",
    "                                                                     augment_type = AUGMENT_TYPE,\n",
    "                                                                     valid_ratio = VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ca5e3-a160-4ee9-a451-3041c841fad9",
   "metadata": {},
   "source": [
    "## Essential functions (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a25f3325-61a1-4d40-9672-f853c6f0d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_save_best_model_vit(model_name, model, epochs=EPOCHS):\n",
    "    ##########################################################################################################\n",
    "\n",
    "    saved_best_weights = 'best_'+model_name+'_weights'\n",
    "    PATH_BEST_WEIGHT_SAVE = os.path.join(PATH_BEST_SAVE_WEIGHT, saved_best_weights)\n",
    "    # Set up a model checkpoint to save the best model during training\n",
    "    best_weight_callback= ModelCheckpoint(filepath=PATH_BEST_WEIGHT_SAVE,\n",
    "                                          monitor=SCORE_TO_MONITOR, \n",
    "                                          save_best_only=True,\n",
    "                                          mode=SCORE_OBJECTIVE, \n",
    "                                          verbose=1,\n",
    "                                          save_weights_only=True )\n",
    "\n",
    "    ##########################################################################################################\n",
    "    # # Setup the ReduceLROnPlateau callback\n",
    "    reduce_LR = ReduceLROnPlateau(\n",
    "        factor=0.5,      # Factor by which the learning rate will be reduced. new_lr = lr * factor\n",
    "        patience=5,      # Number of epochs with no improvement after which learning rate will be reduced.\n",
    "        verbose=1,       # int. 0: quiet, 1: update messages.\n",
    "        min_lr=0.00001   # Lower bound on the learning rate.\n",
    "    )\n",
    "\n",
    "    my_callbacks = [ best_weight_callback, reduce_LR]#, EarlyStop_callback]  , reduce_LRbest_model_callback,\n",
    "    ##########################################################################################################\n",
    "    start_time = time.time()    \n",
    "    # Fitting the model\n",
    "    train_history = model.fit(\n",
    "        train_generator,\n",
    "        epochs= epochs,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=valid_generator,\n",
    "        callbacks=my_callbacks,\n",
    "    )\n",
    "    total_time = time.time() -start_time\n",
    "    return  PATH_BEST_WEIGHT_SAVE, train_history , total_time, model#model_saving_path,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f7a2c-c72e-46d6-bdf6-ea7c300419c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Essential functions (saving result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b86f6126",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_training_history(training_history_object, list_of_metrics=None, model_name=\"default_name\", SAVE_RESULTS = False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        training_history_object:: Object returned by model.fit() function in keras\n",
    "        list_of_metrics        :: A list of metrics to be plotted. Use if you only\n",
    "                                  want to plot a subset of the total set of metrics\n",
    "                                  in the training history object. By Default it will\n",
    "                                  plot all of them in individual subplots.\n",
    "    \"\"\"\n",
    "    history_dict = training_history_object.history\n",
    "\n",
    "    ###################ADDDED NEW################################################\n",
    "    # Remove 'lr' and 'val_lr' keys from history_dict if they exist\n",
    "    history_dict.pop('lr', None)\n",
    "    history_dict.pop('val_lr', None)\n",
    "    #############################################################################\n",
    "    \n",
    "    if list_of_metrics is None:\n",
    "        list_of_metrics = [key for key in list(history_dict.keys()) if 'val_' not in key]\n",
    "    trainHistDF = pd.DataFrame(history_dict)\n",
    "    # trainHistDF.head()\n",
    "    train_keys = list_of_metrics\n",
    "    valid_keys = ['val_' + key for key in train_keys]\n",
    "    nr_plots = len(train_keys)\n",
    "    fig, ax = plt.subplots(1,nr_plots,figsize=(5*nr_plots,4))\n",
    "    for i in range(len(train_keys)):\n",
    "        ax[i].plot(np.array(trainHistDF[train_keys[i]]), label='Training')\n",
    "        ax[i].plot(np.array(trainHistDF[valid_keys[i]]), label='Validation')\n",
    "        ax[i].set_xlabel('Epoch')\n",
    "        ax[i].set_title(train_keys[i])\n",
    "        ax[i].grid('on')\n",
    "        ax[i].legend()\n",
    "    fig.tight_layout\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    # if not os.path.exists(PATH_TO_SAVE_RESULT):\n",
    "    #     # If it does not exist, create it\n",
    "    #     os.makedirs(PATH_TO_SAVE_RESULT)\n",
    "    \n",
    "    if SAVE_RESULTS:\n",
    "        # Save the plot to a PDF\n",
    "        pdf_filename = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_training_history.pdf\")\n",
    "        with PdfPages(pdf_filename) as pdf:\n",
    "            pdf.savefig(fig)\n",
    "            if SHOW_RESULTS:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0848ff76-13fd-491e-bdda-546f69c4bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_save_confusion_matrix(predicted_labels, target_labels, model_name=\"default_name\", SAVE_RESULTS=False):\n",
    "    cm = confusion_matrix(target_labels, predicted_labels)\n",
    "    \n",
    "    plt.figure(figsize=(8, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='g',  cmap='Greens')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    # plt.show()\n",
    "\n",
    "    # Save the plot to a PDF\n",
    "    if SAVE_RESULTS:\n",
    "        # if not os.path.exists(PATH_TO_SAVE_RESULT):\n",
    "        #  # If it does not exist, create it\n",
    "        #     os.makedirs(PATH_TO_SAVE_RESULT)\n",
    "\n",
    "        pdf_filename = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_confusion_matrix.pdf\")\n",
    "        with PdfPages(pdf_filename) as pdf:\n",
    "            pdf.savefig()  # saves the current figure into a pdf page\n",
    "            # plt.close()\n",
    "            if SHOW_RESULTS:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94d4d2fe-fff3-404e-9b5d-dcb57f615d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_TF_TP_FP_FN(true_labels_y_test, predicted_labels):\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(true_labels_y_test, predicted_labels)\n",
    "    \n",
    "    class_to_performance_data = {}\n",
    "    # Calculate TP, FP, TN, FN for each class\n",
    "    num_classes = cm.shape[0]\n",
    "    for cls in range(num_classes):\n",
    "        TP = cm[cls, cls]\n",
    "        FP = cm[:, cls].sum() - TP\n",
    "        FN = cm[cls, :].sum() - TP\n",
    "        TN = cm.sum() - (FP + FN + TP)\n",
    "        \n",
    "        # Calculate support for each class\n",
    "        support = TP + FN\n",
    "        \n",
    "        class_to_performance_data[cls] = {'TP': TP, 'FP': FP, 'TN': TN, 'FN': FN, 'Support': support}\n",
    "    return class_to_performance_data\n",
    "\n",
    "\n",
    "def calculate_metrics(class_to_performance_data, accuracy, SHOW_RESULTS=True, SAVE_RESULTS=True):\n",
    "    metrics_summary = {\n",
    "        'overall':{},\n",
    "        'Macro': {},\n",
    "        'Weighted': {}\n",
    "    }\n",
    "\n",
    "    metrics_summary[ 'overall']= {'accuracy': accuracy}\n",
    "\n",
    "    # Lists to store metric values for macro averaging\n",
    "    precision_list, recall_list, f1_score_list = [], [], []\n",
    "    fpr_list, fnr_list, fdr_list, npv_list = [], [], [], []\n",
    "    \n",
    "    # Variables for weighted sum of metrics\n",
    "    weighted_precision, weighted_recall, weighted_f1 = 0, 0, 0\n",
    "    weighted_fpr, weighted_fnr, weighted_fdr, weighted_npv = 0, 0, 0, 0\n",
    "    total_support = 0\n",
    "\n",
    "    # Calculate metrics for each class\n",
    "    for class_id, metrics in class_to_performance_data.items():\n",
    "        tp = metrics['TP']\n",
    "        fp = metrics['FP']\n",
    "        tn = metrics['TN']\n",
    "        fn = metrics['FN']\n",
    "        support = metrics['Support']\n",
    "\n",
    "        # Basic evaluation metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        # Additional evaluation metrics\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fnr = fn / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fdr = fp / (fp + tp) if (fp + tp) > 0 else 0\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "\n",
    "        # Append to lists for macro averaging\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_score_list.append(f1_score)\n",
    "        fpr_list.append(fpr)\n",
    "        fnr_list.append(fnr)\n",
    "        fdr_list.append(fdr)\n",
    "        npv_list.append(npv)\n",
    "\n",
    "        # Weighted sum of metrics\n",
    "        weighted_precision += precision * support\n",
    "        weighted_recall += recall * support\n",
    "        weighted_f1 += f1_score * support\n",
    "        weighted_fpr += fpr * support\n",
    "        weighted_fnr += fnr * support\n",
    "        weighted_fdr += fdr * support\n",
    "        weighted_npv += npv * support\n",
    "        total_support += support\n",
    "\n",
    "    # Calculate macro averages and round to 5 decimal places\n",
    "    metrics_summary['Macro']['Precision'] = round(sum(precision_list) / len(precision_list), 5)\n",
    "    metrics_summary['Macro']['Recall'] = round(sum(recall_list) / len(recall_list), 5)\n",
    "    metrics_summary['Macro']['F1-Score'] = round(sum(f1_score_list) / len(f1_score_list), 5)\n",
    "    metrics_summary['Macro']['FPR'] = round(sum(fpr_list) / len(fpr_list), 5)\n",
    "    metrics_summary['Macro']['FNR'] = round(sum(fnr_list) / len(fnr_list), 5)\n",
    "    metrics_summary['Macro']['FDR'] = round(sum(fdr_list) / len(fdr_list), 5)\n",
    "    metrics_summary['Macro']['NPV'] = round(sum(npv_list) / len(npv_list), 5)\n",
    "\n",
    "    # Calculate weighted averages and round to 5 decimal places\n",
    "    if total_support > 0:\n",
    "        metrics_summary['Weighted']['Precision'] = round(weighted_precision / total_support, 5)\n",
    "        metrics_summary['Weighted']['Recall'] = round(weighted_recall / total_support, 5)\n",
    "        metrics_summary['Weighted']['F1-Score'] = round(weighted_f1 / total_support, 5)\n",
    "        metrics_summary['Weighted']['FPR'] = round(weighted_fpr / total_support, 5)\n",
    "        metrics_summary['Weighted']['FNR'] = round(weighted_fnr / total_support, 5)\n",
    "        metrics_summary['Weighted']['FDR'] = round(weighted_fdr / total_support, 5)\n",
    "        metrics_summary['Weighted']['NPV'] = round(weighted_npv / total_support, 5)\n",
    "\n",
    "    # Convert the nested dictionary into a DataFrame\n",
    "    report_df = pd.DataFrame.from_dict({(i+\" \"+j): metrics_summary[i][j]  for i in metrics_summary.keys()  for j in metrics_summary[i].keys()}, orient='index').reset_index()\n",
    "    # Rename columns for clarity\n",
    "    report_df.columns = ['Metric Type', 'Value']    \n",
    "    if SHOW_RESULTS:\n",
    "        display(report_df)\n",
    "\n",
    "    if SAVE_RESULTS:\n",
    "        # if not os.path.exists(PATH_TO_SAVE_RESULT):\n",
    "        # # If it does not exist, create it\n",
    "        #     os.makedirs(PATH_TO_SAVE_RESULT)\n",
    "                \n",
    "        prediction_report_filename = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_all_performance_report.csv\" )\n",
    "        # Save the DataFrame to a CSV file\n",
    "        report_df.to_csv(prediction_report_filename, index=True)\n",
    " \n",
    "    # return metrics_summary\n",
    "\n",
    "def calculate_accuracy(true_labels_y_test, predicted_labels):\n",
    "    # Ensure the inputs are NumPy arrays for element-wise comparison\n",
    "    true_labels_y_test = np.array(true_labels_y_test)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    \n",
    "    # Calculate the number of correct predictions\n",
    "    correct_predictions = np.sum(true_labels_y_test == predicted_labels)\n",
    "    \n",
    "    # Calculate the total number of predictions\n",
    "    total_predictions = len(true_labels_y_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    \n",
    "    # Round accuracy to 5 decimal places\n",
    "    accuracy = round(accuracy, 5)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11b63a55-74b4-4982-9feb-e6776d23f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_classification_report(predicted_labels, target_labels, model_name=\"default_name\", SAVE_RESULTS=False):\n",
    "    report_dict = classification_report(target_labels, predicted_labels, output_dict=True)\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    if SHOW_RESULTS:\n",
    "        display(report_df)\n",
    "\n",
    "    if SAVE_RESULTS:\n",
    "        # if not os.path.exists(PATH_TO_SAVE_RESULT):\n",
    "        # # If it does not exist, create it\n",
    "        #     os.makedirs(PATH_TO_SAVE_RESULT)\n",
    "                \n",
    "        prediction_report_filename = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_classification_report.csv\" )\n",
    "        # Save the DataFrame to a CSV file\n",
    "        report_df.to_csv(prediction_report_filename, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56b36c2e-1a4a-4895-ba57-fa7fe0e6caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_save_test_results(test_generator, best_model, SAVE_RESULTS=False):\n",
    "    # Evaluate the model on test data\n",
    "    test_loss, test_accuracy, test_f1_score = best_model.evaluate(test_generator) \n",
    "    \n",
    "    # print(f\"Test Loss: {test_loss}\")\n",
    "    # print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    # print(f\"Test F1 Score: {test_f1_score}\")\n",
    "    \n",
    "    test_results = {\n",
    "        'model_name': model_name,\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_f1_score': test_f1_score\n",
    "    }\n",
    "\n",
    "        # Convert the dictionary to a DataFrame\n",
    "    report_df = pd.DataFrame([test_results])\n",
    "    \n",
    "    if SHOW_RESULTS:\n",
    "        display(report_df)\n",
    "    \n",
    "    if SAVE_RESULTS:\n",
    "        path_to_save = os.path.join(PATH_TO_RESULT, f\"{model_name}_model_performance.csv\")\n",
    "        # Save the DataFrame to a CSV file\n",
    "        report_df.to_csv(path_to_save , index=False)\n",
    "        del report_df, test_results\n",
    "    return  test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2181fbed-1eed-4090-81b7-32e7b309aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_performance_metrics_all(predicted_labels, target_labels, test_loss, model_name=\"default_name\", SAVE_RESULTS=False ):\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(target_labels, predicted_labels)\n",
    "    precision_macro = precision_score(target_labels, predicted_labels, average='macro')\n",
    "    recall_macro = recall_score(target_labels, predicted_labels, average='macro')\n",
    "    f1_macro = f1_score_report(target_labels, predicted_labels, average='macro')\n",
    "\n",
    "    precision_weighted = precision_score(target_labels, predicted_labels, average='weighted')\n",
    "    recall_weighted = recall_score(target_labels, predicted_labels, average='weighted')\n",
    "    f1_weighted = f1_score_report(target_labels, predicted_labels, average='weighted')    \n",
    "    \n",
    "    # Create a dictionary to hold the metrics\n",
    "    test_results = {\n",
    "        'Model Name': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Macro Precision': precision_macro,\n",
    "        'Macro Recall': recall_macro,\n",
    "        'Macro F1-Score': f1_macro,\n",
    "        'Weighted Precision': precision_weighted,\n",
    "        'Weighted Recall': recall_weighted,\n",
    "        'Weighted F1-Score': f1_weighted,        \n",
    "        'Loss': test_loss\n",
    "    }\n",
    "    \n",
    "        # Convert the dictionary to a DataFrame\n",
    "    report_df = pd.DataFrame([test_results])\n",
    "    \n",
    "    if SHOW_RESULTS:\n",
    "        display(report_df)\n",
    "\n",
    "    if SAVE_RESULTS:\n",
    "        path_to_save = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_model_all_test_performance.csv\")\n",
    "        # Save the DataFrame to a CSV file\n",
    "        report_df.to_csv(path_to_save , index=False)\n",
    "        \n",
    "    del report_df, test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3608cfb",
   "metadata": {},
   "source": [
    "## MODEL CREATING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd7a6b-3ba9-4754-af22-fa0aff5bad4e",
   "metadata": {},
   "source": [
    "#### Original Vit models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d25cb5b6-ae1d-478c-98a9-073836a1cf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallel_vit import ViT as parallelViT\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "def get_parallel_vit(): \n",
    "    model = parallelViT( #v4 --Improving gradually with each epoch val_loss: 2.2053 - val_accuracy: 0.2600 - val_f1_score: 0.1379\n",
    "        image_size = max(MODEL_INPUT_SIZE[0], MODEL_INPUT_SIZE[1]),\n",
    "        patch_size = 16, # original 16\n",
    "        num_classes = NUM_CLASSES,\n",
    "        dim = 512,#original 512\n",
    "        depth = 6,\n",
    "        heads = 20, # original 8\n",
    "        mlp_dim = 1024,#original 1024\n",
    "        num_parallel_branches = 2,  # in paper, they claimed 2 was optimal\n",
    "        dropout = 0.2,\n",
    "        emb_dropout = 0.2\n",
    "    )\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(loss=CategoricalCrossentropy(from_logits=True), \n",
    "                  run_eagerly=True,\n",
    "                  metrics=['accuracy', f1_score],\n",
    "                  optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6308b58-680e-434c-ae75-07ea53e2cad2",
   "metadata": {},
   "source": [
    "# Train and Results 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06642652-4393-4874-b0eb-e5fed0234b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model_name=='DenseNet121':\n",
    "#     model= get_DenseNet121()\n",
    "# elif model_name=='MobileNet':\n",
    "#     model= get_MobileNet_original()\n",
    "# elif model_name=='Xception':\n",
    "#     model= get_Xception()\n",
    "# elif model_name=='InceptionV3':\n",
    "#     model= get_InceptionV3()\n",
    "\n",
    "# ##Getting the model\n",
    "# model= get_MobileNet_original()\n",
    "# model= get_DenseNet121()\n",
    "# model= get_Xception()\n",
    "# model= get_InceptionV3()\n",
    "# model = get_cait()\n",
    "model = get_parallel_vit()\n",
    "# model = get_T2TViT()\n",
    "# # ## Getting the model\n",
    "# model = get_basic_CNN() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a10a153-1811-4448-9634-e08608b0d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = get_parallel_vit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3b68166-6085-47f2-a43a-20152cef8268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x260e8ddc880>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.load_weights(\"final_saved_models_basic_augment\\\\parallelViT\\\\saved_weights\\\\best_parallelViT_weights\")\n",
    " \n",
    "# best_model.fit(\n",
    "#     train_generator,\n",
    "#     epochs= 1,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     validation_data=valid_generator,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a71ec9d2-c3fc-4343-ad66-42d582449842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 21s 621ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the model\n",
    "predictions = best_model.predict(test_generator, verbose=1)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "true_labels_y_test =  np.array(test_generator.classes) # y_test  \n",
    "############################################################################################################]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c91044b5-8f43-4ecc-a421-255a4bcc0f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bootstrapped Macro AUC (100 runs):\n",
      "Mean AUC     : 0.99260\n",
      "Std Deviation: 0.00101\n",
      "95% CI       : (0.99089, 0.99470)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9926003182551193,\n",
       " 0.0010123830020779597,\n",
       " (0.9908850963398171, 0.9946999934664049))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def bootstrap_auc_ci(true_labels, predicted_probs, num_classes=NUM_CLASSES, n_iterations=100, seed=SEED, save_path=None):\n",
    "    \"\"\"\n",
    "    Computes bootstrapped AUC with 95% confidence interval for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        true_labels (np.array): Ground truth class labels (e.g., [0, 1, 2, ...])\n",
    "        predicted_probs (np.array): Predicted probabilities, shape = (n_samples, n_classes)\n",
    "        num_classes (int): Number of classes\n",
    "        n_iterations (int): Number of bootstrap iterations\n",
    "        seed (int): Random seed for reproducibility\n",
    "        save_path (str): Optional path to save AUC scores as CSV\n",
    "\n",
    "    Returns:\n",
    "        Prints mean AUC, std, and 95% CI\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    auc_scores = []\n",
    "\n",
    "    # One-hot encode true labels\n",
    "    true_labels_bin = label_binarize(true_labels, classes=np.arange(num_classes))\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        indices = np.random.choice(len(true_labels), size=len(true_labels), replace=True)\n",
    "        y_true_sample = true_labels_bin[indices]\n",
    "        y_pred_sample = predicted_probs[indices]\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true_sample, y_pred_sample, multi_class='ovr', average='macro')\n",
    "            auc_scores.append(auc)\n",
    "        except ValueError:\n",
    "            continue  # Skip if a class is missing in the sample\n",
    "\n",
    "    auc_scores = np.array(auc_scores)\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    std_auc = np.std(auc_scores)\n",
    "    ci_lower = np.percentile(auc_scores, 2.5)\n",
    "    ci_upper = np.percentile(auc_scores, 97.5)\n",
    "\n",
    "    print(f\"\\nBootstrapped Macro AUC ({n_iterations} runs):\")\n",
    "    print(f\"Mean AUC     : {mean_auc:.5f}\")\n",
    "    print(f\"Std Deviation: {std_auc:.5f}\")\n",
    "    print(f\"95% CI       : ({ci_lower:.5f}, {ci_upper:.5f})\")\n",
    "\n",
    "    df = pd.DataFrame({'AUC Score': auc_scores})\n",
    "    df.to_csv(f\"data26_basic_augment_{model_name}_bootstrapped_auc.csv\", index=False)\n",
    "\n",
    "    return mean_auc, std_auc, (ci_lower, ci_upper)\n",
    "\n",
    "# Call the function\n",
    "bootstrap_auc_ci(\n",
    "    true_labels=true_labels_y_test,\n",
    "    predicted_probs=predictions\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-13T13:42:48.933576",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
