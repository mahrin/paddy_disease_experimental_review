{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab40bce-732d-4de1-b710-ad886e072808",
   "metadata": {},
   "source": [
    "- imbalance data: https://www.linkedin.com/pulse/some-tricks-handling-imbalanced-dataset-image-m-farhan-tandia\n",
    "- base model: https://www.youtube.com/watch?v=t8eSPbvl0vU\n",
    "- https://www.kaggle.com/code/hongjae6/simple-baseline-using-keras-resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8891c7ce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0ff7e4-cf5e-4586-a1f7-eab01ae63a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mahri\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mahri\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:585: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os # read and manipulate local files\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score as f1_score_report\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "# from PIL import Image\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import tensorflow.keras as ks\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.applications import xception, MobileNet, MobileNetV2, Xception, Xception, EfficientNetV2M, InceptionV3\n",
    "# from tensorflow.keras.applications import EfficientNetB2\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from imgaug import augmenters as iaa # elastic_deformation\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Dense, BatchNormalization, Concatenate, GlobalAveragePooling2D\n",
    "from keras.initializers import RandomNormal# weights will be initialized according to a Gaussian distribution \n",
    "\n",
    "# hide wornings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "################################################################################################\n",
    "# SETTING F1 SCORE\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "f1_score = F1Score(num_classes=13, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f806ed-e9db-4199-b7ee-c2fa11bd0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'DenseNet121'#'InceptionV3'#'MobileNet' #'Xception'#    'InceptionV3'##'T2TViT', , 'MobileNet', 'Xception', \n",
    "\n",
    "################################################################################################\n",
    "\n",
    "## SEETING THE PATHS\n",
    "PATH_TO_CODE =''\n",
    "sys.path.append(PATH_TO_CODE)\n",
    "DATASET_PATH = 'paddy-doctor-diseases-small-split/'\n",
    "# DATASET_PATH =  '../paddy-doctor-diseases-small-400-split/'\n",
    "DATASET_PATH_TRAIN = os.path.join(DATASET_PATH, 'train')\n",
    "DATASET_PATH_TEST = os.path.join(DATASET_PATH, 'test')\n",
    "\n",
    "\n",
    "PATH_TO_SAVE_RESULT = os.path.join(PATH_TO_CODE, 'saved_outputs_no_augment', model_name)\n",
    "PATH_BEST_SAVE_WEIGHT = os.path.join(PATH_TO_SAVE_RESULT,'saved_weights')\n",
    "PATH_TO_SAVE_MODEL = os.path.join(PATH_TO_SAVE_RESULT, 'saved_models')\n",
    "PATH_SAVE_HISTORY = os.path.join(PATH_TO_SAVE_RESULT, model_name+'_training_history.csv')\n",
    "PATH_SAVE_TIME = os.path.join(PATH_TO_SAVE_RESULT, model_name+'training_time.csv')\n",
    "# PATH_TO_SAVE_TUNER = os.path.join(PATH_TO_RESULT, 'saved_tuner_model')\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "ORIGINAL_IMAGE_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
    "COLOR_CHANNEL = 3\n",
    "\n",
    "RESIZE_SHAPE = (128, 128)\n",
    "MODEL_INPUT_SIZE = (RESIZE_SHAPE[0], RESIZE_SHAPE[1], COLOR_CHANNEL)\n",
    "\n",
    "VALIDATION_SPLIT= 0.2\n",
    "NUM_CLASSES = 13 #len(target_labels) # 13\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 120 #120\n",
    "# EXTRA_RUN_AFTER_BEST_EPOCH = 10\n",
    "\n",
    "################################################################################################\n",
    "AUGMENT = False\n",
    "AUGMENT_TYPE = 'basic' # basic, advanced,\n",
    "################################################################################################\n",
    "\n",
    "# METRICS_TO_RECORD = 'short'#'full' #'short'  # \"full\" or 'short'\n",
    "SAVE_RESULTS = True\n",
    "SHOW_RESULTS = True\n",
    "\n",
    "################################################################################################\n",
    "# Setting the seed\n",
    "SEED  = 123\n",
    "RNG = np.random.default_rng(SEED) # Random number generator\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "################################################################################################\n",
    "# Checkpoint parameters\n",
    "SCORE_TO_MONITOR = 'val_f1_score' # Score that checkpoints monitor during training\n",
    "SCORE_OBJECTIVE  = 'max'          # 'max' or 'min', specifies whether the objective is to maximize the score or minimize it. \n",
    "\n",
    "PATIENCE_EARLY_STOP = 20\n",
    "# Checkpoint parameters\n",
    "REDUCTION_FACTOR = 0.5            # Factor which lr will be reduced with at plateau\n",
    "EPOCH_PATIENCE_LR_INCREASE = 5              # For how many epochs must the score plateau before reducing lr\n",
    "MIN_LR = 1e-8\n",
    "INITIAL_LR=0.0001\n",
    "# COOLDOWN_EPOCHS  = 5              # How many epochs to wait after learning rate reduction before it can be reduced again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc029c0-dfb5-4571-a579-7fe7ca89cc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_PATH ../paddy-doctor-diseases-small-split/\n",
      "DATASET_PATH_TRAIN ../paddy-doctor-diseases-small-split/train\n",
      "DATASET_PATH_TEST ../paddy-doctor-diseases-small-split/test\n",
      "PATH_TO_SAVE_RESULT saved_outputs_no_augment\\DenseNet121\n",
      "PATH_BEST_SAVE_WEIGHT saved_outputs_no_augment\\DenseNet121\\saved_weights\n",
      "PATH_TO_SAVE_MODEL saved_outputs_no_augment\\DenseNet121\\saved_models\n"
     ]
    }
   ],
   "source": [
    "print(\"DATASET_PATH\", DATASET_PATH)\n",
    "print(\"DATASET_PATH_TRAIN\", DATASET_PATH_TRAIN)\n",
    "print(\"DATASET_PATH_TEST\", DATASET_PATH_TEST)\n",
    "\n",
    "print(\"PATH_TO_SAVE_RESULT\",PATH_TO_SAVE_RESULT)\n",
    "print(\"PATH_BEST_SAVE_WEIGHT\", PATH_BEST_SAVE_WEIGHT)\n",
    "print(\"PATH_TO_SAVE_MODEL\", PATH_TO_SAVE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae2d29-d089-412a-88a5-7b5017dee4f8",
   "metadata": {},
   "source": [
    "## Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f4e78e-2715-4ec6-b86d-a032075066a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  False\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30676387-9bdf-4015-83e8-db47734137d4",
   "metadata": {},
   "source": [
    "## Image Data Augmentation Options in Keras `ImageDataGenerator`\n",
    "\n",
    "The Keras `ImageDataGenerator` offers various options for image augmentation, enhancing training dataset diversity and improving model robustness. Below is a list of common augmentations available:\n",
    "\n",
    "1. **`rotation_range`**: Degrees range (0-180) for random rotations.\n",
    "\n",
    "2. **`width_shift_range` & `height_shift_range`**: Fraction of total width or height, or an integer number of pixels, for horizontal or vertical image shift.\n",
    "\n",
    "3. **`brightness_range`**: Tuple specifying a range for random brightness adjustment. Values < 1 darken the image, > 1 brighten it.\n",
    "\n",
    "4. **`shear_range`**: Shear Intensity (angle in counter-clockwise direction as radians) for shear transformations.\n",
    "\n",
    "5. **`zoom_range`**: Range for random zoom. If a float, `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n",
    "\n",
    "6. **`channel_shift_range`**: Range for random channel (color) shifts.\n",
    "\n",
    "7. **`fill_mode`**: Mode for filling points outside boundaries ('constant', 'nearest', 'reflect', 'wrap') after transformations.\n",
    "\n",
    "8. **`cval`**: Value for filling points outside boundaries when `fill_mode` is 'constant'.\n",
    "\n",
    "9. **`horizontal_flip` & `vertical_flip`**: Boolean. Randomly flip images horizontally or vertically.\n",
    "\n",
    "10. **`rescale`**: Rescaling factor, often 1/255 to scale pixel values to [0, 1].\n",
    "\n",
    "11. **`preprocessing_function`**: Function to be applied to each input after resizing and augmenting.\n",
    "\n",
    "12. **`validation_split`**: Fraction of images reserved for validation (between 0 and 1).\n",
    "\n",
    "13. **`dtype`**: Dtype to use for generated arrays.\n",
    "\n",
    "Note: The appropriateness of these augmentations depends on the specific data and use case. Some, like flipping, may not suit certain types of images (e.g., text)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dada5c-1141-4914-96bd-a8ff36749e6d",
   "metadata": {},
   "source": [
    "### ElasticTransformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cce2108f-7c14-40de-b590-d7e14856ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an augmentation pipeline\n",
    "# Alpha parameter controls the intensity of the deformation\n",
    "# Sigma controls the smoothness of the deformation field.\n",
    "aug = iaa.Sequential([  \n",
    "    iaa.ElasticTransformation(alpha=50, sigma=10)  # Apply elastic transformations , sigma=1\n",
    "])\n",
    "\n",
    "def elastic_deformation(image):\n",
    "    image_aug = aug(image=image)\n",
    "    return image_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e823091d-ce18-4444-845c-8a9bc459800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for creating the generators with given augmentation type\n",
    "# images are alreagy rescaled--so no rescaling is needed\n",
    "def create_generators(dataset_path_train=DATASET_PATH_TRAIN, \n",
    "                      dataset_path_test=DATASET_PATH_TEST, \n",
    "                      valid_ratio=VALIDATION_SPLIT, \n",
    "                      augment= False, \n",
    "                      augment_type='all'):\n",
    "   \n",
    "    # If augmentation is True, apply data augmentation. Otherwise, only rescale.\n",
    "    if augment:\n",
    "        if augment_type == 'all':     \n",
    "            datagen = ImageDataGenerator(\n",
    "                rescale=1./255, # Normalize images\n",
    "                validation_split=valid_ratio,  # Split ratio for validation set\n",
    "\n",
    "                # Geometric Transformations\n",
    "                rotation_range=40,  # degrees\n",
    "                width_shift_range=0.2,  # fraction of total width\n",
    "                height_shift_range=0.3,  # fraction of total height\n",
    "                shear_range=0.4,  # shear angle in counter-clockwise direction as radians\n",
    "                zoom_range=0.25,  # zoom range for random zoom\n",
    "                horizontal_flip=True,  # randomly flip images horizontally\n",
    "                vertical_flip=True,  # randomly flip images vertically\n",
    "                \n",
    "                # Pixel-Level Transformations\n",
    "                brightness_range=[0.6, 1.1],  # range for picking a brightness shift value\n",
    "                channel_shift_range=0.3,  # range for random channel shifts\n",
    "\n",
    "                # Advanced functions\n",
    "                preprocessing_function=elastic_deformation\n",
    "            )\n",
    "        elif augment_type == 'basic':     \n",
    "            datagen = ImageDataGenerator(\n",
    "                rescale=1./255, # Normalize images\n",
    "                validation_split=valid_ratio,  # Split ratio for validation set\n",
    "\n",
    "                # Geometric Transformations\n",
    "                rotation_range=40,  # degrees\n",
    "                horizontal_flip=True,  # randomly flip images horizontally\n",
    "            )\n",
    "        elif augment_type == 'advanced':     \n",
    "            datagen = ImageDataGenerator(\n",
    "                rescale=1./255, # Normalize images\n",
    "                validation_split=valid_ratio,  # Split ratio for validation set\n",
    "\n",
    "                # Advanced functions\n",
    "                preprocessing_function=elastic_deformation\n",
    "            )\n",
    "            \n",
    "    else:\n",
    "        # No augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "            validation_split=valid_ratio,  # Split ratio for validation set\n",
    "            rescale=1./255, # Normalize images\n",
    "        )\n",
    "    \n",
    "    # Create training generator\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        dataset_path_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed = SEED,\n",
    "        class_mode=\"categorical\",  # Use \"categorical\" for multi-class classification\n",
    "        target_size=RESIZE_SHAPE,  # Adjust target size based on your model requirements\n",
    "        shuffle = True,\n",
    "        color_mode = 'rgb',\n",
    "        subset='training'  # Specify subset as 'training'\n",
    "    )\n",
    "    \n",
    "    # Create validation generator\n",
    "    valid_generator = datagen.flow_from_directory(\n",
    "        dataset_path_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed = SEED,\n",
    "        class_mode=\"categorical\",  # Use \"categorical\" for multi-class classification\n",
    "        target_size=RESIZE_SHAPE,  # Adjust target size based on your model requirements        \n",
    "        shuffle = True,        \n",
    "        color_mode = 'rgb',\n",
    "        subset='validation'  # Specify subset as 'validation'\n",
    "    )\n",
    "    \n",
    "    # For the test set, assuming no augmentation, just rescaling\n",
    "    test_datagen = ImageDataGenerator(\n",
    "                rescale=1./255# Normalize images\n",
    "    )\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        dataset_path_test,\n",
    "        class_mode=\"categorical\",  # Use \"categorical\" for multi-class classification\n",
    "        target_size=RESIZE_SHAPE,  # Adjust target size based on your model requirements        \n",
    "        seed = SEED,\n",
    "        shuffle = False,        \n",
    "        color_mode = 'rgb',\n",
    "        batch_size=BATCH_SIZE)\n",
    "    \n",
    "    return train_generator, valid_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "269c38da-a92b-47d1-918c-3b19f84aa19c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../paddy-doctor-diseases-small-split/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Creating the generators using functions\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_generator, valid_generator, test_generator \u001b[38;5;241m=\u001b[39m create_generators(dataset_path_train\u001b[38;5;241m=\u001b[39mDATASET_PATH_TRAIN,\n\u001b[0;32m      3\u001b[0m                                                                      dataset_path_test \u001b[38;5;241m=\u001b[39m DATASET_PATH_TEST,\n\u001b[0;32m      4\u001b[0m                                                                      augment\u001b[38;5;241m=\u001b[39mAUGMENT,\n\u001b[0;32m      5\u001b[0m                                                                      augment_type \u001b[38;5;241m=\u001b[39m AUGMENT_TYPE,\n\u001b[0;32m      6\u001b[0m                                                                      valid_ratio \u001b[38;5;241m=\u001b[39m VALIDATION_SPLIT)\n",
      "Cell \u001b[1;32mIn[10], line 58\u001b[0m, in \u001b[0;36mcreate_generators\u001b[1;34m(dataset_path_train, dataset_path_test, valid_ratio, augment, augment_type)\u001b[0m\n\u001b[0;32m     52\u001b[0m     datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m     53\u001b[0m         validation_split\u001b[38;5;241m=\u001b[39mvalid_ratio,  \u001b[38;5;66;03m# Split ratio for validation set\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m, \u001b[38;5;66;03m# Normalize images\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     )\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Create training generator\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     59\u001b[0m     dataset_path_train,\n\u001b[0;32m     60\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[0;32m     61\u001b[0m     seed \u001b[38;5;241m=\u001b[39m SEED,\n\u001b[0;32m     62\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Use \"categorical\" for multi-class classification\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mRESIZE_SHAPE,  \u001b[38;5;66;03m# Adjust target size based on your model requirements\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     65\u001b[0m     color_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     66\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Specify subset as 'training'\u001b[39;00m\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Create validation generator\u001b[39;00m\n\u001b[0;32m     70\u001b[0m valid_generator \u001b[38;5;241m=\u001b[39m datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     71\u001b[0m     dataset_path_train,\n\u001b[0;32m     72\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Specify subset as 'validation'\u001b[39;00m\n\u001b[0;32m     79\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:1649\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[1;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m   1562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[0;32m   1563\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1564\u001b[0m     directory,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1578\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1579\u001b[0m ):\n\u001b[0;32m   1580\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Takes the path to a directory & generates batches of augmented data.\u001b[39;00m\n\u001b[0;32m   1581\u001b[0m \n\u001b[0;32m   1582\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m \u001b[38;5;124;03m        and `y` is a numpy array of corresponding labels.\u001b[39;00m\n\u001b[0;32m   1648\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DirectoryIterator(\n\u001b[0;32m   1650\u001b[0m         directory,\n\u001b[0;32m   1651\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1652\u001b[0m         target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[0;32m   1653\u001b[0m         color_mode\u001b[38;5;241m=\u001b[39mcolor_mode,\n\u001b[0;32m   1654\u001b[0m         keep_aspect_ratio\u001b[38;5;241m=\u001b[39mkeep_aspect_ratio,\n\u001b[0;32m   1655\u001b[0m         classes\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[0;32m   1656\u001b[0m         class_mode\u001b[38;5;241m=\u001b[39mclass_mode,\n\u001b[0;32m   1657\u001b[0m         data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format,\n\u001b[0;32m   1658\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1659\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m   1660\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m   1661\u001b[0m         save_to_dir\u001b[38;5;241m=\u001b[39msave_to_dir,\n\u001b[0;32m   1662\u001b[0m         save_prefix\u001b[38;5;241m=\u001b[39msave_prefix,\n\u001b[0;32m   1663\u001b[0m         save_format\u001b[38;5;241m=\u001b[39msave_format,\n\u001b[0;32m   1664\u001b[0m         follow_links\u001b[38;5;241m=\u001b[39mfollow_links,\n\u001b[0;32m   1665\u001b[0m         subset\u001b[38;5;241m=\u001b[39msubset,\n\u001b[0;32m   1666\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[0;32m   1667\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m   1668\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:563\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[1;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m    562\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(directory)):\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    565\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../paddy-doctor-diseases-small-split/train'"
     ]
    }
   ],
   "source": [
    "# Creating the generators using functions\n",
    "train_generator, valid_generator, test_generator = create_generators(dataset_path_train=DATASET_PATH_TRAIN,\n",
    "                                                                     dataset_path_test = DATASET_PATH_TEST,\n",
    "                                                                     augment=AUGMENT,\n",
    "                                                                     augment_type = AUGMENT_TYPE,\n",
    "                                                                     valid_ratio = VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ca5e3-a160-4ee9-a451-3041c841fad9",
   "metadata": {},
   "source": [
    "## Essential functions (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a134a6b-19df-438f-8d63-9e5c28dabaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_save_best_model(model_name, model, epochs=EPOCHS):\n",
    "    ##########################################################################################################\n",
    "    saved_best_model_name = 'best_'+model_name+'.h5'\n",
    "    # Criteria for early stopping\n",
    "    EarlyStop_callback = EarlyStopping(min_delta=0.001, patience=PATIENCE_EARLY_STOP, restore_best_weights=True)\n",
    "\n",
    "    model_saving_path = os.path.join(PATH_TO_SAVE_MODEL, saved_best_model_name)\n",
    "    # Set up a model checkpoint to save the best model during training\n",
    "    best_model_callback= ModelCheckpoint(model_saving_path,\n",
    "                                          monitor=SCORE_TO_MONITOR, \n",
    "                                          save_best_only=True,\n",
    "                                          mode=SCORE_OBJECTIVE, \n",
    "                                          verbose=1)\n",
    "\n",
    "    # Setup the ReduceLROnPlateau callback\n",
    "    reduce_LR = ReduceLROnPlateau(\n",
    "        factor=REDUCTION_FACTOR,      # Factor by which the learning rate will be reduced. new_lr = lr * factor\n",
    "        patience=EPOCH_PATIENCE_LR_INCREASE,      # Number of epochs with no improvement after which learning rate will be reduced.\n",
    "        verbose=1,       # int. 0: quiet, 1: update messages.\n",
    "        min_lr=MIN_LR    # Lower bound on the learning rate.\n",
    "    )\n",
    "\n",
    "    my_callbacks = [best_model_callback , reduce_LR, EarlyStop_callback]\n",
    "    ##########################################################################################################\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Fitting the model\n",
    "    train_history = model.fit(\n",
    "        train_generator,\n",
    "        epochs= epochs,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=valid_generator,\n",
    "        callbacks=my_callbacks,\n",
    "    )\n",
    "\n",
    "    total_time = time.time() -start_time\n",
    "    return saved_best_model_name, train_history, total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f7a2c-c72e-46d6-bdf6-ea7c300419c9",
   "metadata": {},
   "source": [
    "## Essential functions (saving result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f6126",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_training_history(training_history_object, list_of_metrics=None):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        training_history_object:: Object returned by model.fit() function in keras\n",
    "        list_of_metrics        :: A list of metrics to be plotted. Use if you only\n",
    "                                  want to plot a subset of the total set of metrics\n",
    "                                  in the training history object. By Default it will\n",
    "                                  plot all of them in individual subplots.\n",
    "    \"\"\"\n",
    "    history_dict = training_history_object.history\n",
    "\n",
    "    ###################ADDDED NEW################################################\n",
    "    # Remove 'lr' and 'val_lr' keys from history_dict if they exist\n",
    "    history_dict.pop('lr', None)\n",
    "    history_dict.pop('val_lr', None)\n",
    "    #############################################################################\n",
    "    \n",
    "    if list_of_metrics is None:\n",
    "        list_of_metrics = [key for key in list(history_dict.keys()) if 'val_' not in key]\n",
    "    trainHistDF = pd.DataFrame(history_dict)\n",
    "    # trainHistDF.head()\n",
    "    train_keys = list_of_metrics\n",
    "    valid_keys = ['val_' + key for key in train_keys]\n",
    "    nr_plots = len(train_keys)\n",
    "    fig, ax = plt.subplots(1,nr_plots,figsize=(5*nr_plots,4))\n",
    "    for i in range(len(train_keys)):\n",
    "        ax[i].plot(np.array(trainHistDF[train_keys[i]]), label='Training')\n",
    "        ax[i].plot(np.array(trainHistDF[valid_keys[i]]), label='Validation')\n",
    "        ax[i].set_xlabel('Epoch')\n",
    "        ax[i].set_title(train_keys[i])\n",
    "        ax[i].grid('on')\n",
    "        ax[i].legend()\n",
    "    fig.tight_layout\n",
    "    # plt.show()\n",
    "\n",
    "    if SAVE_RESULTS:\n",
    "        # Save the plot to a PDF\n",
    "        pdf_filename = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_training_history.pdf\")\n",
    "        with PdfPages(pdf_filename) as pdf:\n",
    "            pdf.savefig(fig)\n",
    "            if SHOW_RESULTS:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848ff76-13fd-491e-bdda-546f69c4bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_save_confusion_matrix(predicted_labels, target_labels):\n",
    "    cm = confusion_matrix(target_labels, predicted_labels)\n",
    "    \n",
    "    plt.figure(figsize=(8, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='g',  cmap='Greens')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    # plt.show()\n",
    "\n",
    "    # Save the plot to a PDF\n",
    "    if SAVE_RESULTS:\n",
    "        pdf_filename = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_confusion_matrix.pdf\")\n",
    "        with PdfPages(pdf_filename) as pdf:\n",
    "            pdf.savefig()  # saves the current figure into a pdf page\n",
    "            # plt.close()\n",
    "            if SHOW_RESULTS:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4d2fe-fff3-404e-9b5d-dcb57f615d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_TF_TP_FP_FN(true_labels_y_test, predicted_labels):\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(true_labels_y_test, predicted_labels)\n",
    "    \n",
    "    class_to_performance_data = {}\n",
    "    # Calculate TP, FP, TN, FN for each class\n",
    "    num_classes = cm.shape[0]\n",
    "    for cls in range(num_classes):\n",
    "        TP = cm[cls, cls]\n",
    "        FP = cm[:, cls].sum() - TP\n",
    "        FN = cm[cls, :].sum() - TP\n",
    "        TN = cm.sum() - (FP + FN + TP)\n",
    "        \n",
    "        # Calculate support for each class\n",
    "        support = TP + FN\n",
    "        \n",
    "        class_to_performance_data[cls] = {'TP': TP, 'FP': FP, 'TN': TN, 'FN': FN, 'Support': support}\n",
    "    return class_to_performance_data\n",
    "\n",
    "\n",
    "def calculate_metrics(class_to_performance_data, accuracy, SHOW_RESULTS=True, SAVE_RESULTS=True):\n",
    "    metrics_summary = {\n",
    "        'overall':{},\n",
    "        'Macro': {},\n",
    "        'Weighted': {}\n",
    "    }\n",
    "\n",
    "    metrics_summary[ 'overall']= {'accuracy': accuracy}\n",
    "\n",
    "    # Lists to store metric values for macro averaging\n",
    "    precision_list, recall_list, f1_score_list = [], [], []\n",
    "    fpr_list, fnr_list, fdr_list, npv_list = [], [], [], []\n",
    "    \n",
    "    # Variables for weighted sum of metrics\n",
    "    weighted_precision, weighted_recall, weighted_f1 = 0, 0, 0\n",
    "    weighted_fpr, weighted_fnr, weighted_fdr, weighted_npv = 0, 0, 0, 0\n",
    "    total_support = 0\n",
    "\n",
    "    # Calculate metrics for each class\n",
    "    for class_id, metrics in class_to_performance_data.items():\n",
    "        tp = metrics['TP']\n",
    "        fp = metrics['FP']\n",
    "        tn = metrics['TN']\n",
    "        fn = metrics['FN']\n",
    "        support = metrics['Support']\n",
    "\n",
    "        # Basic evaluation metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        # Additional evaluation metrics\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fnr = fn / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fdr = fp / (fp + tp) if (fp + tp) > 0 else 0\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "\n",
    "        # Append to lists for macro averaging\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_score_list.append(f1_score)\n",
    "        fpr_list.append(fpr)\n",
    "        fnr_list.append(fnr)\n",
    "        fdr_list.append(fdr)\n",
    "        npv_list.append(npv)\n",
    "\n",
    "        # Weighted sum of metrics\n",
    "        weighted_precision += precision * support\n",
    "        weighted_recall += recall * support\n",
    "        weighted_f1 += f1_score * support\n",
    "        weighted_fpr += fpr * support\n",
    "        weighted_fnr += fnr * support\n",
    "        weighted_fdr += fdr * support\n",
    "        weighted_npv += npv * support\n",
    "        total_support += support\n",
    "\n",
    "    # Calculate macro averages and round to 5 decimal places\n",
    "    metrics_summary['Macro']['Precision'] = round(sum(precision_list) / len(precision_list), 5)\n",
    "    metrics_summary['Macro']['Recall'] = round(sum(recall_list) / len(recall_list), 5)\n",
    "    metrics_summary['Macro']['F1-Score'] = round(sum(f1_score_list) / len(f1_score_list), 5)\n",
    "    metrics_summary['Macro']['FPR'] = round(sum(fpr_list) / len(fpr_list), 5)\n",
    "    metrics_summary['Macro']['FNR'] = round(sum(fnr_list) / len(fnr_list), 5)\n",
    "    metrics_summary['Macro']['FDR'] = round(sum(fdr_list) / len(fdr_list), 5)\n",
    "    metrics_summary['Macro']['NPV'] = round(sum(npv_list) / len(npv_list), 5)\n",
    "\n",
    "    # Calculate weighted averages and round to 5 decimal places\n",
    "    if total_support > 0:\n",
    "        metrics_summary['Weighted']['Precision'] = round(weighted_precision / total_support, 5)\n",
    "        metrics_summary['Weighted']['Recall'] = round(weighted_recall / total_support, 5)\n",
    "        metrics_summary['Weighted']['F1-Score'] = round(weighted_f1 / total_support, 5)\n",
    "        metrics_summary['Weighted']['FPR'] = round(weighted_fpr / total_support, 5)\n",
    "        metrics_summary['Weighted']['FNR'] = round(weighted_fnr / total_support, 5)\n",
    "        metrics_summary['Weighted']['FDR'] = round(weighted_fdr / total_support, 5)\n",
    "        metrics_summary['Weighted']['NPV'] = round(weighted_npv / total_support, 5)\n",
    "\n",
    "    # Convert the nested dictionary into a DataFrame\n",
    "    report_df = pd.DataFrame.from_dict({(i+\" \"+j): metrics_summary[i][j]  for i in metrics_summary.keys()  for j in metrics_summary[i].keys()}, orient='index').reset_index()\n",
    "    # Rename columns for clarity\n",
    "    report_df.columns = ['Metric Type', 'Value']    \n",
    "    if SHOW_RESULTS:\n",
    "        display(report_df)\n",
    "\n",
    "    if SAVE_RESULTS:      \n",
    "        prediction_report_filename = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_all_performance_report.csv\" )\n",
    "        # Save the DataFrame to a CSV file\n",
    "        report_df.to_csv(prediction_report_filename, index=True)\n",
    " \n",
    "    # return metrics_summary\n",
    "\n",
    "def calculate_accuracy(true_labels_y_test, predicted_labels):\n",
    "    # Ensure the inputs are NumPy arrays for element-wise comparison\n",
    "    true_labels_y_test = np.array(true_labels_y_test)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    \n",
    "    # Calculate the number of correct predictions\n",
    "    correct_predictions = np.sum(true_labels_y_test == predicted_labels)\n",
    "    \n",
    "    # Calculate the total number of predictions\n",
    "    total_predictions = len(true_labels_y_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    \n",
    "    # Round accuracy to 5 decimal places\n",
    "    accuracy = round(accuracy, 5)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b63a55-74b4-4982-9feb-e6776d23f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_classification_report(predicted_labels, target_labels):\n",
    "    report_dict = classification_report(target_labels, predicted_labels, output_dict=True)\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    if SHOW_RESULTS:\n",
    "        display(report_df)\n",
    "\n",
    "    if SAVE_RESULTS:        \n",
    "        prediction_report_filename = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_classification_report.csv\" )\n",
    "        # Save the DataFrame to a CSV file\n",
    "        report_df.to_csv(prediction_report_filename, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b36c2e-1a4a-4895-ba57-fa7fe0e6caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_save_test_results(test_generator, best_model):\n",
    "    # Evaluate the model on test data\n",
    "    test_loss, test_accuracy, test_f1_score = best_model.evaluate(test_generator) \n",
    "    \n",
    "    # print(f\"Test Loss: {test_loss}\")\n",
    "    # print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    # print(f\"Test F1 Score: {test_f1_score}\")\n",
    "    \n",
    "    test_results = {\n",
    "        'model_name': model_name,\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_f1_score': test_f1_score\n",
    "    }\n",
    "\n",
    "        # Convert the dictionary to a DataFrame\n",
    "    report_df = pd.DataFrame([test_results])\n",
    "    \n",
    "    if SHOW_RESULTS:\n",
    "        display(report_df)\n",
    "    \n",
    "    if SAVE_RESULTS:\n",
    "        path_to_save = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_model_performance.csv\")\n",
    "        # Save the DataFrame to a CSV file\n",
    "        report_df.to_csv(path_to_save , index=False)\n",
    "        del report_df, test_results\n",
    "    return  test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2181fbed-1eed-4090-81b7-32e7b309aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_performance_metrics_all(predicted_labels, target_labels, test_loss):\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(target_labels, predicted_labels)\n",
    "    precision_macro = precision_score(target_labels, predicted_labels, average='macro')\n",
    "    recall_macro = recall_score(target_labels, predicted_labels, average='macro')\n",
    "    f1_macro = f1_score_report(target_labels, predicted_labels, average='macro')\n",
    "\n",
    "    precision_weighted = precision_score(target_labels, predicted_labels, average='weighted')\n",
    "    recall_weighted = recall_score(target_labels, predicted_labels, average='weighted')\n",
    "    f1_weighted = f1_score_report(target_labels, predicted_labels, average='weighted')    \n",
    "    \n",
    "    # Create a dictionary to hold the metrics\n",
    "    test_results = {\n",
    "        'Model Name': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Macro Precision': precision_macro,\n",
    "        'Macro Recall': recall_macro,\n",
    "        'Macro F1-Score': f1_macro,\n",
    "        'Weighted Precision': precision_weighted,\n",
    "        'Weighted Recall': recall_weighted,\n",
    "        'Weighted F1-Score': f1_weighted,        \n",
    "        'Loss': test_loss\n",
    "    }\n",
    "    \n",
    "        # Convert the dictionary to a DataFrame\n",
    "    report_df = pd.DataFrame([test_results])\n",
    "    \n",
    "    if SHOW_RESULTS:\n",
    "        display(report_df)\n",
    "\n",
    "    if SAVE_RESULTS:\n",
    "        path_to_save = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_model_all_test_performance.csv\")\n",
    "        # Save the DataFrame to a CSV file\n",
    "        report_df.to_csv(path_to_save , index=False)\n",
    "        \n",
    "    del report_df, test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3608cfb",
   "metadata": {},
   "source": [
    "## MODEL CREATING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396763a9-b1c8-4426-8762-43aaac48796b",
   "metadata": {},
   "source": [
    "#### Original models from keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcedb7b-b005-4f20-a0a2-947608b110f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_basic_CNN():\n",
    "#     model = Sequential()\n",
    "    \n",
    "#     # Convolutional and pooling layers\n",
    "#     model.add(Conv2D(32, (3, 3), activation='relu', input_shape=MODEL_INPUT_SIZE))\n",
    "#     model.add(MaxPooling2D((2, 2)))\n",
    "#     model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#     model.add(MaxPooling2D((2, 2)))\n",
    "#     model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "#     model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "#     # Flatten the output for fully connected layers\n",
    "#     model.add(Flatten())\n",
    "    \n",
    "#     # Fully connected layers\n",
    "#     model.add(Dense(128, activation='relu'))\n",
    "#     # model.add(Dense(64, activation='relu'))\n",
    "#     model.add(Dense(NUM_CLASSES, activation='softmax'))  # Adjust the output size based on your problem\n",
    "\n",
    "\n",
    "#     optimizer = Adam(learning_rate=0.001)\n",
    "#     model.compile(loss=\"categorical_crossentropy\", \n",
    "#                   metrics=['accuracy', f1_score],\n",
    "#                   optimizer=optimizer)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970296e7-aea1-4c17-b6c4-3cac190f4d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications import InceptionV3\n",
    "# def get_InceptionV3(): \n",
    " \n",
    "#     # model = Sequential()\n",
    "#     # base_model = InceptionV3(input_shape=MODEL_INPUT_SIZE, include_top=False, weights=None)\n",
    "    \n",
    "#     # # Global average pooling and additional dense layers\n",
    "#     # x = Flatten()(base_model.output)\n",
    "#     # # x = Dense(512, activation='relu')(x)\n",
    "#     # predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "#     # # Build the functional model\n",
    "#     # model = Model(inputs=base_model.input, outputs=predictions)\n",
    "#     model = InceptionV3(input_shape=MODEL_INPUT_SIZE,  weights=None, classes=NUM_CLASSES)\n",
    "\n",
    "#     optimizer = Adam(learning_rate=INITIAL_LR)\n",
    "#     model.compile(loss=\"categorical_crossentropy\", \n",
    "#                   metrics=['accuracy', f1_score],\n",
    "#                   optimizer=optimizer)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135f9c2-6276-422a-b884-a6870803e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications import MobileNet#### WORKING\n",
    "# def get_MobileNet_original(): \n",
    " \n",
    "#     # model = Sequential()\n",
    "#     # base_model = MobileNet(input_shape=MODEL_INPUT_SIZE, include_top=False, weights=None)\n",
    "    \n",
    "#     # # Global average pooling and additional dense layers\n",
    "#     # x = Flatten()(base_model.output)\n",
    "#     # # x = Dense(512, activation='relu')(x)\n",
    "#     # predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "#     # # Build the functional model\n",
    "#     # model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#     model = MobileNet(input_shape=MODEL_INPUT_SIZE,  weights=None, classes=NUM_CLASSES)\n",
    "\n",
    "#     optimizer = Adam(learning_rate=0.001)\n",
    "#     model.compile(loss=\"categorical_crossentropy\", \n",
    "#                   metrics=['accuracy', f1_score],\n",
    "#                   optimizer=optimizer)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4024e91e-a29a-4666-a01a-05b891ab4ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications import Xception\n",
    "# def get_Xception(): \n",
    " \n",
    "#     # model = Sequential()\n",
    "#     # base_model = Xception(input_shape=MODEL_INPUT_SIZE, include_top=False, weights=None)\n",
    "    \n",
    "#     # # Global average pooling and additional dense layers\n",
    "#     # x = Flatten()(base_model.output)\n",
    "#     # # x = Dense(512, activation='relu')(x)\n",
    "#     # predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "#     # # Build the functional model\n",
    "#     # model = Model(inputs=base_model.input, outputs=predictions)\n",
    "#     model = Xception(input_shape=MODEL_INPUT_SIZE,  weights=None, classes=NUM_CLASSES)\n",
    "\n",
    "#     optimizer = Adam(learning_rate=INITIAL_LR)\n",
    "#     model.compile(loss=\"categorical_crossentropy\", \n",
    "#                   metrics=['accuracy', f1_score],\n",
    "#                   optimizer=optimizer)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7858c-b010-4bba-a9cd-a548de2aed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to build the model with an ensemble of feature extractors\n",
    "# def get_ensemble2_model(input_shape=MODEL_INPUT_SIZE,\n",
    "#                       dropout= 0.1,\n",
    "#                       activation='relu',\n",
    "#                       units= 1024,\n",
    "#                       nr_classes=NUM_CLASSES,\n",
    "#                       loss = \"categorical_crossentropy\", \n",
    "#                       retrain_backbone_network=False,\n",
    "#                       filters=7,\n",
    "#                       name='ensemble2'):\n",
    "\n",
    "\n",
    "#     inputs = Input(shape=input_shape)\n",
    "\n",
    "#     # Feature extraction from Xception\n",
    "#     Xception_base = Xception(include_top=False, input_tensor=Input(shape=input_shape), weights='imagenet')\n",
    "#     Xception_base.trainable = retrain_backbone_network # Freeze the layers of Xception\n",
    "#     Xception_features = Xception_base(inputs)\n",
    "#     Xception_features = GlobalAveragePooling2D()(Xception_features)\n",
    "\n",
    "#     # Feature extraction from DenseNet121\n",
    "#     densenet_base = DenseNet121(include_top=False, input_tensor=Input(shape=input_shape), weights='imagenet')\n",
    "#     densenet_base.trainable = retrain_backbone_network  # Freeze the layers of DenseNet121\n",
    "#     densenet_features = densenet_base(inputs)\n",
    "#     densenet_features = GlobalAveragePooling2D()(densenet_features)\n",
    "\n",
    "#     # Concatenate features\n",
    "#     concatenated_features = Concatenate()([Xception_features, densenet_features])\n",
    "\n",
    "#     if filters==3:\n",
    "#         filters = (3,3)\n",
    "#     elif filters==5:\n",
    "#         filters = (5,5)\n",
    "#     elif filters==7:\n",
    "#         filters = (7,7)\n",
    "#     elif filters==9:\n",
    "#         filters = (9,9)\n",
    "\n",
    "#     # # Add a tunable dropout layer\n",
    "#     # x = Dropout(dropout)(concatenated_features)\n",
    "\n",
    "#     # # A fully connected layer with a tunable number of units\n",
    "#     # x = Dense(units)(x)\n",
    "\n",
    "#     #     # Add a tunable dropout layer\n",
    "#     # x = Dropout(dropout)(x)\n",
    "\n",
    "#     # # A fully connected layer with a tunable number of units\n",
    "#     # x = Dense(units)(x)\n",
    "#         # Add a tunable dropout layer\n",
    "#     x = Dropout(dropout)(concatenated_features)\n",
    "\n",
    "#     # A fully connected layer with a tunable number of units\n",
    "#     x = Dense(units)(x)\n",
    "#     x = Activation(activation)(x)  # Add ReLU activation before batch normalization\n",
    "#     x = BatchNormalization()(x)\n",
    "\n",
    "#     # Add a tunable dropout layer\n",
    "#     x = Dropout(dropout)(x)\n",
    "\n",
    "#     # A fully connected layer with a tunable number of units again\n",
    "#     x = Dense(units)(x)\n",
    "#     x = Activation(activation)(x)  # Add ReLU activation before batch normalization\n",
    "#     x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "#     # Output layer\n",
    "#     predictions = Dense( nr_classes, activation='softmax')(x)\n",
    "\n",
    "#     # Create and compile the model\n",
    "#     model = Model(inputs=inputs, outputs=predictions)\n",
    "#     model.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(INITIAL_LR),\n",
    "#         loss=loss,\n",
    "#         metrics=['accuracy', f1_score]\n",
    "#     )\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c061742-26c9-4ec1-867c-43e03b396966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to build the model with an ensemble of feature extractors\n",
    "# def get_ensemble3_model(input_shape=MODEL_INPUT_SIZE,\n",
    "#                       dropout= 0.2,\n",
    "#                       activation='relu',\n",
    "#                       units= 512,\n",
    "#                       nr_classes=NUM_CLASSES,\n",
    "#                       loss = \"categorical_crossentropy\", \n",
    "#                       retrain_backbone_network=False,\n",
    "#                       filters=7,\n",
    "#                       name='ensemble2'):\n",
    "\n",
    "\n",
    "#     inputs = Input(shape=input_shape)\n",
    "\n",
    "#     # Feature extraction from DenseNet121\n",
    "#     densenet_base = DenseNet121(include_top=False, input_tensor=Input(shape=input_shape), weights='imagenet')\n",
    "#     densenet_base.trainable = retrain_backbone_network  # Freeze the layers of DenseNet121\n",
    "#     densenet_features = densenet_base(inputs)\n",
    "#     densenet_features = GlobalAveragePooling2D()(densenet_features)\n",
    "\n",
    "#     # Feature extraction from Xception\n",
    "#     Xception_base = Xception(include_top=False, input_tensor=Input(shape=input_shape), weights='imagenet')\n",
    "#     Xception_base.trainable = retrain_backbone_network # Freeze the layers of Xception\n",
    "#     Xception_features = Xception_base(inputs)\n",
    "#     Xception_features = GlobalAveragePooling2D()(Xception_features)\n",
    "\n",
    "\n",
    "#         # Feature extraction from Xception\n",
    "#     InceptionResNetV2_base = InceptionResNetV2(include_top=False, input_tensor=Input(shape=input_shape), weights='imagenet')\n",
    "#     InceptionResNetV2_base.trainable = retrain_backbone_network # Freeze the layers of Xception\n",
    "#     InceptionResNetV2_features = InceptionResNetV2_base(inputs)\n",
    "#     InceptionResNetV2_features = GlobalAveragePooling2D()(InceptionResNetV2_features)\n",
    "    \n",
    "\n",
    "#     # Concatenate the features\n",
    "#     concatenated_features = Concatenate()([densenet_features, Xception_features, InceptionResNetV2_features])\n",
    "\n",
    "#     x = Dropout(dropout)(concatenated_features)\n",
    "#     # A fully connected layer with a tunable number of units\n",
    "#     x = Dense(units)(x)\n",
    "#     x = Activation(activation)(x)  # Add ReLU activation before batch normalization\n",
    "#     x = BatchNormalization()(x)\n",
    "\n",
    "#     # Output layer\n",
    "#     predictions = Dense( nr_classes, activation='softmax')(x)\n",
    "\n",
    "#     # Create the final ensemble model\n",
    "#     model = Model(inputs=inputs, outputs=predictions)\n",
    "#     model.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(INITIAL_LR),\n",
    "#         loss=loss,\n",
    "#         metrics=['accuracy', f1_score]\n",
    "#     )\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ebc955-a60d-4b9b-9979-b830992c7781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import  DenseNet121\n",
    "def get_DenseNet121(): # almost proper--10 has high dense\n",
    " \n",
    "    # model = Sequential()\n",
    "    # base_model = DenseNet121(input_shape=MODEL_INPUT_SIZE, include_top=False, weights=None)\n",
    "    \n",
    "    # # Global average pooling and additional dense layers\n",
    "    # x = Flatten()(base_model.output)\n",
    "    # # x = Dense(512, activation='relu')(x)\n",
    "    # predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "    # # Build the functional model\n",
    "    # model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model = DenseNet121(input_shape=MODEL_INPUT_SIZE,  weights=None, classes=NUM_CLASSES)\n",
    "\n",
    "    optimizer = Adam(learning_rate=INITIAL_LR)\n",
    "    model.compile(loss=\"categorical_crossentropy\", \n",
    "                  metrics=['accuracy', f1_score],\n",
    "                  optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6308b58-680e-434c-ae75-07ea53e2cad2",
   "metadata": {},
   "source": [
    "# Train and Results\n",
    "- DOES NOT WORK---EfficientNetV2, Nasnet, Resnet, VGG16,\n",
    "- DOES NOT WORK--InceptionV3--ERROR: No matching distribution found for tensorflow-directml (checked with colab)\r\n",
    "- DOES NOT WORK--InceptionResNetV2--slighly works\n",
    "- original models selected: DenseNet121, -mobilenet X-ception, ,-Inception\r\n",
    "- My models selected: 'ensemble2' (70), Basic CNN v1(91), Basic CNN v2 (92), transfer_mobilenet (85), ensembleXception\r\n",
    "- tuned vit models:  T2TViT(47)\",---Cait (26), 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f45f4-dc50-47af-8be8-d94bcd287d19",
   "metadata": {},
   "source": [
    "### MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06642652-4393-4874-b0eb-e5fed0234b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model_name=='DenseNet121':\n",
    "#     model= get_DenseNet121()\n",
    "# elif model_name=='MobileNet':\n",
    "#     model= get_MobileNet_original()\n",
    "# elif model_name=='Xception':\n",
    "#     model= get_Xception()\n",
    "# elif model_name=='InceptionV3':\n",
    "#     model= get_InceptionV3()\n",
    "\n",
    "# ##Getting the model\n",
    "# model= get_MobileNet_original()\n",
    "model= get_DenseNet121()\n",
    "# model= get_Xception()\n",
    "# model= get_InceptionV3()\n",
    "# model = get_cait()\n",
    "# # ## Getting the model\n",
    "# model = get_basic_CNN() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eeabc6-ab0f-4054-ae45-9a8e2d7d2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training the model and saving the best model as a check point---0-120\n",
    "# best_model_name, train_history, total_time = fit_and_save_best_model(model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237e7be9-1c5f-4261-84d7-f4c8b15928e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the time to be reused\n",
    "# time_data = {\n",
    "#     \"model_name\": [model_name],\n",
    "#     \"run_time\": [total_time]\n",
    "# }\n",
    "\n",
    "# # Convert the dictionary to a pandas DataFrame\n",
    "# time_df = pd.DataFrame(time_data)\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# time_df.to_csv(PATH_SAVE_TIME, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0feef9-34e8-444f-ab78-2fe0f44e5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the history as csv file to be reused\n",
    "# history_df = pd.DataFrame(train_history.history)\n",
    "# history_df.to_csv(PATH_SAVE_HISTORY, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4e708-ce35-44b7-be76-8a4b2739bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plotting train history\n",
    "# plot_training_history(train_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b68166-6085-47f2-a43a-20152cef8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, load the best model\n",
    "best_model = ks.models.load_model(\"final_saved_outputs_no_augment\\\\DenseNet121_v2\\\\saved_models\\\\best_DenseNet121.h5\", custom_objects={'F1Score': F1Score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2753a059-e440-46b4-8514-36028c742da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the best model for Test results: loss, accuracy, f1_score\n",
    "# test_loss = show_save_test_results(test_generator, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ec9d2-c3fc-4343-ad66-42d582449842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the model\n",
    "predictions = best_model.predict(test_generator, verbose=1)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "true_labels_y_test =  np.array(test_generator.classes) # y_test  \n",
    "############################################################################################################]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324ae44-0e15-43fd-8183-c0f2a636bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def bootstrap_auc_ci(true_labels, predicted_probs, num_classes=NUM_CLASSES, n_iterations=100, seed=SEED, save_path=None):\n",
    "    \"\"\"\n",
    "    Computes bootstrapped AUC with 95% confidence interval for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        true_labels (np.array): Ground truth class labels (e.g., [0, 1, 2, ...])\n",
    "        predicted_probs (np.array): Predicted probabilities, shape = (n_samples, n_classes)\n",
    "        num_classes (int): Number of classes\n",
    "        n_iterations (int): Number of bootstrap iterations\n",
    "        seed (int): Random seed for reproducibility\n",
    "        save_path (str): Optional path to save AUC scores as CSV\n",
    "\n",
    "    Returns:\n",
    "        Prints mean AUC, std, and 95% CI\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    auc_scores = []\n",
    "\n",
    "    # One-hot encode true labels\n",
    "    true_labels_bin = label_binarize(true_labels, classes=np.arange(num_classes))\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        indices = np.random.choice(len(true_labels), size=len(true_labels), replace=True)\n",
    "        y_true_sample = true_labels_bin[indices]\n",
    "        y_pred_sample = predicted_probs[indices]\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true_sample, y_pred_sample, multi_class='ovr', average='macro')\n",
    "            auc_scores.append(auc)\n",
    "        except ValueError:\n",
    "            continue  # Skip if a class is missing in the sample\n",
    "\n",
    "    auc_scores = np.array(auc_scores)\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    std_auc = np.std(auc_scores)\n",
    "    ci_lower = np.percentile(auc_scores, 2.5)\n",
    "    ci_upper = np.percentile(auc_scores, 97.5)\n",
    "\n",
    "    print(f\"\\nBootstrapped Macro AUC ({n_iterations} runs):\")\n",
    "    print(f\"Mean AUC     : {mean_auc:.5f}\")\n",
    "    print(f\"Std Deviation: {std_auc:.5f}\")\n",
    "    print(f\"95% CI       : ({ci_lower:.5f}, {ci_upper:.5f})\")\n",
    "\n",
    "    if save_path:\n",
    "        df = pd.DataFrame({'AUC Score': auc_scores})\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "    return mean_auc, std_auc, (ci_lower, ci_upper)\n",
    "\n",
    "# Call the function\n",
    "bootstrap_auc_ci(\n",
    "    true_labels=true_labels_y_test,\n",
    "    predicted_probs=predictions,\n",
    "    save_path=os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_bootstrapped_auc.csv\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec521dc-00ee-4f65-8d32-1f8d3bd8e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Classification report\n",
    "# store_classification_report(predicted_labels=predicted_labels, \n",
    "#                             target_labels=true_labels_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce55e2-cb63-4f84-bbdc-53b4e7be2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For saving confusion matrix\n",
    "# show_save_confusion_matrix(predicted_labels=predicted_labels, \n",
    "#                            target_labels=true_labels_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5627005-a58b-4375-a1d2-5495fbd12700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Calculate all\n",
    "# calculate_test_performance_metrics_all(predicted_labels=predicted_labels, \n",
    "#                                        target_labels=true_labels_y_test,\n",
    "#                                        test_loss=test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b511d06-5d63-4831-9544-d92c856f0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = calculate_TF_TP_FP_FN(true_labels_y_test, predicted_labels)\n",
    "# acc = calculate_accuracy(true_labels_y_test, predicted_labels)\n",
    "# calculate_metrics(class_to_performance_data=data, accuracy=acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-13T13:42:48.933576",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
