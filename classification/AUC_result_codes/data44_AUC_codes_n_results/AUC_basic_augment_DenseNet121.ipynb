{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8891c7ce",
   "metadata": {
    "id": "8891c7ce",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "oXsIvnalrsWb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21356,
     "status": "ok",
     "timestamp": 1710557520337,
     "user": {
      "displayName": "Mahrin Mehrin",
      "userId": "03538273846602354774"
     },
     "user_tz": -60
    },
    "id": "oXsIvnalrsWb",
    "outputId": "fd581346-45ba-4c4d-b2b8-abba52931656"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0ff7e4-cf5e-4586-a1f7-eab01ae63a15",
   "metadata": {
    "id": "8e0ff7e4-cf5e-4586-a1f7-eab01ae63a15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mahri\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mahri\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:585: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os # read and manipulate local files\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score as f1_score_report\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import tensorflow.keras as ks\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from imgaug import augmenters as iaa # elastic_deformation\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Dense, BatchNormalization, Concatenate, GlobalAveragePooling2D\n",
    "\n",
    "# hide wornings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "################################################################################################\n",
    "# SETTING F1 SCORE\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "f1_score = F1Score(num_classes=4, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PMuxPpz9GjaY",
   "metadata": {
    "id": "PMuxPpz9GjaY"
   },
   "source": [
    "## Setting the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f806ed-e9db-4199-b7ee-c2fa11bd0dee",
   "metadata": {
    "id": "33f806ed-e9db-4199-b7ee-c2fa11bd0dee"
   },
   "outputs": [],
   "source": [
    "model_name = 'DenseNet121'#'InceptionV3'#'Xception'#'DenseNet121'#    'InceptionV3'##  'MobileNet', 'Xception', ensemble2, ensemble3, 'parallel_vit'\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "## SEETING THE PATHS\n",
    "PATH_TO_CODE =''\n",
    "# PATH_TO_CODE ='/content/drive/MyDrive/TRIAL_v1/classification'\n",
    "sys.path.append(PATH_TO_CODE)\n",
    "# DATASET_PATH = r'/content/drive/MyDrive/TRIAL_v1/classification/data44_resized_splited'\n",
    "DATASET_PATH = r'data44_resized_splited'\n",
    "DATASET_PATH_TRAIN = os.path.join(DATASET_PATH, 'train')\n",
    "DATASET_PATH_TEST = os.path.join(DATASET_PATH, 'test')\n",
    "\n",
    "\n",
    "PATH_TO_SAVE_RESULT = os.path.join(PATH_TO_CODE, 'saved_outputs_basic_augment', model_name)\n",
    "PATH_BEST_SAVE_WEIGHT = os.path.join(PATH_TO_SAVE_RESULT,model_name+'_saved_weights')\n",
    "PATH_TO_SAVE_MODEL = os.path.join(PATH_TO_SAVE_RESULT, 'saved_models')\n",
    "PATH_SAVE_HISTORY = os.path.join(PATH_TO_SAVE_RESULT,model_name+'_training_history.csv')\n",
    "PATH_SAVE_TIME = os.path.join(PATH_TO_SAVE_RESULT, model_name+'_training_time.csv')\n",
    "\n",
    "\n",
    "if not os.path.exists(PATH_TO_SAVE_RESULT):\n",
    "    # If it does not exist, create it\n",
    "    os.makedirs(PATH_TO_SAVE_RESULT)\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "IMG_HEIGHT = 200\n",
    "IMG_WIDTH = 200\n",
    "ORIGINAL_IMAGE_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
    "COLOR_CHANNEL = 3\n",
    "\n",
    "RESIZE_SHAPE = (128, 128)\n",
    "MODEL_INPUT_SIZE = (RESIZE_SHAPE[0], RESIZE_SHAPE[1], COLOR_CHANNEL)\n",
    "\n",
    "VALIDATION_SPLIT= 0.3\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 150\n",
    "\n",
    "################################################################################################\n",
    "AUGMENT = True\n",
    "AUGMENT_TYPE = 'basic' #'all'## basic, advanced,\n",
    "################################################################################################\n",
    "\n",
    "SAVE_RESULTS = True\n",
    "SHOW_RESULTS = True\n",
    "\n",
    "################################################################################################\n",
    "# Setting the seed\n",
    "SEED  = 123\n",
    "RNG = np.random.default_rng(SEED) # Random number generator\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "################################################################################################\n",
    "# Checkpoint parameters\n",
    "SCORE_TO_MONITOR = 'val_f1_score' # Score that checkpoints monitor during training\n",
    "SCORE_OBJECTIVE  = 'max'          # 'max' or 'min', specifies whether the objective is to maximize the score or minimize it.\n",
    "\n",
    "\n",
    "\n",
    "PATIENCE_EARLY_STOP = 12 # With no improvement in Loss, will stop training\n",
    "\n",
    "# # Checkpoint parameters\n",
    "REDUCTION_FACTOR = 0.5            # Factor which lr will be reduced with at plateau\n",
    "PATIENCE_LR_REDUCE=3      # Number of epochs with no improvement after which learning rate will be reduced\n",
    "COOLDOWN_EPOCHS  = 2              # How many epochs to wait after learning rate reduction before it can be reduced again\n",
    "MIN_LR = 1e-9\n",
    "INITIAL_LR = 0.0001 # with 0.001 got 99 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc029c0-dfb5-4571-a579-7fe7ca89cc2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1710557538028,
     "user": {
      "displayName": "Mahrin Mehrin",
      "userId": "03538273846602354774"
     },
     "user_tz": -60
    },
    "id": "5cc029c0-dfb5-4571-a579-7fe7ca89cc2a",
    "outputId": "cf5b3484-741c-46c2-d6b6-04884d7ffb67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_PATH data44_resized_splited\n",
      "DATASET_PATH_TRAIN data44_resized_splited\\train\n",
      "DATASET_PATH_TEST data44_resized_splited\\test\n",
      "PATH_SAVE_TIME  saved_outputs_basic_augment\\DenseNet121\\DenseNet121_training_time.csv\n",
      "PATH_SAVE_HISTORY saved_outputs_basic_augment\\DenseNet121\\DenseNet121_training_history.csv\n",
      "PATH_TO_SAVE_RESULT saved_outputs_basic_augment\\DenseNet121\n",
      "PATH_BEST_SAVE_WEIGHT saved_outputs_basic_augment\\DenseNet121\\DenseNet121_saved_weights\n",
      "PATH_TO_SAVE_MODEL saved_outputs_basic_augment\\DenseNet121\\saved_models\n"
     ]
    }
   ],
   "source": [
    "print(\"DATASET_PATH\", DATASET_PATH)\n",
    "print(\"DATASET_PATH_TRAIN\", DATASET_PATH_TRAIN)\n",
    "print(\"DATASET_PATH_TEST\", DATASET_PATH_TEST)\n",
    "\n",
    "print(\"PATH_SAVE_TIME \", PATH_SAVE_TIME )\n",
    "print(\"PATH_SAVE_HISTORY\", PATH_SAVE_HISTORY)\n",
    "print(\"PATH_TO_SAVE_RESULT\",PATH_TO_SAVE_RESULT)\n",
    "print(\"PATH_BEST_SAVE_WEIGHT\", PATH_BEST_SAVE_WEIGHT)\n",
    "print(\"PATH_TO_SAVE_MODEL\", PATH_TO_SAVE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae2d29-d089-412a-88a5-7b5017dee4f8",
   "metadata": {
    "id": "60ae2d29-d089-412a-88a5-7b5017dee4f8"
   },
   "source": [
    "## Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f4e78e-2715-4ec6-b86d-a032075066a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1710557538029,
     "user": {
      "displayName": "Mahrin Mehrin",
      "userId": "03538273846602354774"
     },
     "user_tz": -60
    },
    "id": "d0f4e78e-2715-4ec6-b86d-a032075066a9",
    "outputId": "210fd904-4766-40a7-eb09-500d779af20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mahri\\AppData\\Local\\Temp\\ipykernel_43928\\984659479.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU Available:  False\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30676387-9bdf-4015-83e8-db47734137d4",
   "metadata": {
    "id": "30676387-9bdf-4015-83e8-db47734137d4"
   },
   "source": [
    "## Image Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dada5c-1141-4914-96bd-a8ff36749e6d",
   "metadata": {
    "id": "69dada5c-1141-4914-96bd-a8ff36749e6d"
   },
   "source": [
    "### ElasticTransformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce2108f-7c14-40de-b590-d7e14856ae64",
   "metadata": {
    "id": "cce2108f-7c14-40de-b590-d7e14856ae64"
   },
   "outputs": [],
   "source": [
    "# Define an augmentation pipeline\n",
    "# Alpha parameter controls the intensity of the deformation\n",
    "# Sigma controls the smoothness of the deformation field.\n",
    "aug = iaa.Sequential([\n",
    "    iaa.ElasticTransformation(alpha=10, sigma=5)  # Apply elastic transformations , sigma=1\n",
    "])\n",
    "\n",
    "def elastic_deformation(image):\n",
    "    image_aug = aug(image=image)\n",
    "    return image_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e823091d-ce18-4444-845c-8a9bc459800a",
   "metadata": {
    "id": "e823091d-ce18-4444-845c-8a9bc459800a"
   },
   "outputs": [],
   "source": [
    "# Defining a function for creating the generators with given augmentation type\n",
    "def create_generators(dataset_path_train=DATASET_PATH_TRAIN,\n",
    "                      dataset_path_test=DATASET_PATH_TEST,\n",
    "                      valid_ratio=VALIDATION_SPLIT,\n",
    "                      augment= False,\n",
    "                      augment_type='basic'):\n",
    "\n",
    "    # If augmentation is True, apply data augmentation. Otherwise, only rescale.\n",
    "    if augment:\n",
    "        if augment_type == 'all':\n",
    "            datagen = ImageDataGenerator(\n",
    "                rescale=1./255, # Normalize images\n",
    "                validation_split=valid_ratio,  # Split ratio for validation set\n",
    "\n",
    "                # Geometric Transformations\n",
    "                rotation_range=10,  # degrees\n",
    "                width_shift_range=0.2,  # fraction of total width\n",
    "                height_shift_range=0.2,  # fraction of total height\n",
    "                shear_range=0.2,  # shear angle in counter-clockwise direction as radians\n",
    "                zoom_range=0.2,  # zoom range for random zoom\n",
    "                horizontal_flip=True,  # randomly flip images horizontally\n",
    "                # vertical_flip=True,  # randomly flip images vertically\n",
    "\n",
    "                # Pixel-Level Transformations\n",
    "                brightness_range=[0.8, 1.1],  # range for picking a brightness shift value\n",
    "                channel_shift_range=0.2,  # range for random channel shifts\n",
    "\n",
    "                # Advanced functions\n",
    "                preprocessing_function=elastic_deformation\n",
    "            )\n",
    "        elif augment_type == 'basic':\n",
    "            datagen = ImageDataGenerator(\n",
    "                rescale=1./255, # Normalize images\n",
    "                validation_split=valid_ratio,  # Split ratio for validation set\n",
    "\n",
    "                # Geometric Transformations\n",
    "                # rotation_range=10,  # degrees\n",
    "                zoom_range=0.2,  # zoom range for random zoom\n",
    "                # vertical_flip=True,  # randomly flip images vertically\n",
    "                horizontal_flip=True,  # randomly flip images horizontally\n",
    "            )\n",
    "    else:\n",
    "        # No augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "            validation_split=valid_ratio,  # Split ratio for validation set\n",
    "            rescale=1./255, # Normalize images\n",
    "        )\n",
    "\n",
    "    # Create training generator\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        dataset_path_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed = SEED,\n",
    "        class_mode=\"categorical\",  # Use \"categorical\" for multi-class classification\n",
    "        target_size=RESIZE_SHAPE,  # Adjust target size based on your model requirements\n",
    "        shuffle = True,\n",
    "        color_mode = 'rgb',\n",
    "        subset='training'  # Specify subset as 'training'\n",
    "    )\n",
    "\n",
    "    # Create validation generator\n",
    "    valid_generator = datagen.flow_from_directory(\n",
    "        dataset_path_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed = SEED,\n",
    "        class_mode=\"categorical\",  # Use \"categorical\" for multi-class classification\n",
    "        target_size=RESIZE_SHAPE,  # Adjust target size based on your model requirements\n",
    "        shuffle = True,\n",
    "        color_mode = 'rgb',\n",
    "        subset='validation'  # Specify subset as 'validation'\n",
    "    )\n",
    "\n",
    "    # For the test set, assuming no augmentation, just rescaling\n",
    "    test_datagen = ImageDataGenerator(\n",
    "                rescale=1./255# Normalize images\n",
    "    )\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        dataset_path_test,\n",
    "        class_mode=\"categorical\",  # Use \"categorical\" for multi-class classification\n",
    "        target_size=RESIZE_SHAPE,  # Adjust target size based on your model requirements\n",
    "        seed = SEED,\n",
    "        shuffle = False,\n",
    "        color_mode = 'rgb',\n",
    "        batch_size=BATCH_SIZE)\n",
    "\n",
    "    return train_generator, valid_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "269c38da-a92b-47d1-918c-3b19f84aa19c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13589,
     "status": "ok",
     "timestamp": 1710557551611,
     "user": {
      "displayName": "Mahrin Mehrin",
      "userId": "03538273846602354774"
     },
     "user_tz": -60
    },
    "id": "269c38da-a92b-47d1-918c-3b19f84aa19c",
    "outputId": "4e9a6ed8-9907-422b-a0d3-a5c1558a9f89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3324 images belonging to 4 classes.\n",
      "Found 1421 images belonging to 4 classes.\n",
      "Found 1187 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Creating the generators using functions\n",
    "train_generator, valid_generator, test_generator = create_generators(dataset_path_train=DATASET_PATH_TRAIN,\n",
    "                                                                     dataset_path_test = DATASET_PATH_TEST,\n",
    "                                                                     augment=AUGMENT,\n",
    "                                                                     augment_type = AUGMENT_TYPE,\n",
    "                                                                     valid_ratio = VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ca5e3-a160-4ee9-a451-3041c841fad9",
   "metadata": {
    "id": "da5ca5e3-a160-4ee9-a451-3041c841fad9"
   },
   "source": [
    "## Essential functions (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a134a6b-19df-438f-8d63-9e5c28dabaaf",
   "metadata": {
    "id": "0a134a6b-19df-438f-8d63-9e5c28dabaaf"
   },
   "outputs": [],
   "source": [
    "def fit_and_save_best_model(model_name, model, epochs=EPOCHS):\n",
    "    ##########################################################################################################\n",
    "    saved_best_model_name = 'best_'+model_name+'.h5'\n",
    "\n",
    "    model_saving_path = os.path.join(PATH_TO_SAVE_MODEL, saved_best_model_name)\n",
    "\n",
    "    # Criteria for early stopping\n",
    "    EarlyStop_callback = EarlyStopping(min_delta=0.0001, patience=PATIENCE_EARLY_STOP, restore_best_weights=True)\n",
    "\n",
    "    # Set up a model checkpoint to save the best model during training\n",
    "    best_model_callback= ModelCheckpoint(model_saving_path,\n",
    "                                          monitor=SCORE_TO_MONITOR,\n",
    "                                          save_best_only=True,\n",
    "                                          mode=SCORE_OBJECTIVE,\n",
    "                                          verbose=1)\n",
    "\n",
    "\n",
    "    # # Setup the ReduceLROnPlateau callback\n",
    "    reduce_LR = ReduceLROnPlateau(\n",
    "        factor=REDUCTION_FACTOR,      # Factor by which the learning rate will be reduced. new_lr = lr * factor\n",
    "        patience=PATIENCE_LR_REDUCE,      # Number of epochs with no improvement after which learning rate will be reduced.\n",
    "        verbose=1,       # int. 0: quiet, 1: update messages.\n",
    "        min_lr=MIN_LR,   # Lower bound on the learning rate.\n",
    "        cooldown = COOLDOWN_EPOCHS\n",
    "    )\n",
    "\n",
    "    my_callbacks = [best_model_callback , reduce_LR , EarlyStop_callback]\n",
    "    ##########################################################################################################\n",
    "    start_time = time.time()\n",
    "    # Fitting the model\n",
    "    train_history = model.fit(\n",
    "        train_generator,\n",
    "        epochs= epochs,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=valid_generator,\n",
    "        callbacks=my_callbacks,\n",
    "    )\n",
    "    total_time = time.time() -start_time\n",
    "    return saved_best_model_name, train_history, total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f7a2c-c72e-46d6-bdf6-ea7c300419c9",
   "metadata": {
    "id": "938f7a2c-c72e-46d6-bdf6-ea7c300419c9",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Essential functions (saving result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b86f6126",
   "metadata": {
    "id": "b86f6126",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_training_history(training_history_object, list_of_metrics=None):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        training_history_object:: Object returned by model.fit() function in keras\n",
    "        list_of_metrics        :: A list of metrics to be plotted. Use if you only\n",
    "                                  want to plot a subset of the total set of metrics\n",
    "                                  in the training history object. By Default it will\n",
    "                                  plot all of them in individual subplots.\n",
    "    \"\"\"\n",
    "    history_dict = training_history_object.history\n",
    "\n",
    "    ###################ADDDED NEW################################################\n",
    "    # Remove 'lr' and 'val_lr' keys from history_dict if they exist\n",
    "    history_dict.pop('lr', None)\n",
    "    history_dict.pop('val_lr', None)\n",
    "    #############################################################################\n",
    "\n",
    "    if list_of_metrics is None:\n",
    "        list_of_metrics = [key for key in list(history_dict.keys()) if 'val_' not in key]\n",
    "    trainHistDF = pd.DataFrame(history_dict)\n",
    "    # trainHistDF.head()\n",
    "    train_keys = list_of_metrics\n",
    "    valid_keys = ['val_' + key for key in train_keys]\n",
    "    nr_plots = len(train_keys)\n",
    "    fig, ax = plt.subplots(1,nr_plots,figsize=(5*nr_plots,4))\n",
    "    for i in range(len(train_keys)):\n",
    "        ax[i].plot(np.array(trainHistDF[train_keys[i]]), label='Training')\n",
    "        ax[i].plot(np.array(trainHistDF[valid_keys[i]]), label='Validation')\n",
    "        ax[i].set_xlabel('Epoch')\n",
    "        ax[i].set_title(train_keys[i])\n",
    "        ax[i].grid('on')\n",
    "        ax[i].legend()\n",
    "    fig.tight_layout\n",
    "    # plt.show()\n",
    "\n",
    "    if SAVE_RESULTS:\n",
    "        # Save the plot to a PDF\n",
    "        pdf_filename = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_training_history.pdf\")\n",
    "        with PdfPages(pdf_filename) as pdf:\n",
    "            pdf.savefig(fig)\n",
    "            if SHOW_RESULTS:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0848ff76-13fd-491e-bdda-546f69c4bd62",
   "metadata": {
    "id": "0848ff76-13fd-491e-bdda-546f69c4bd62"
   },
   "outputs": [],
   "source": [
    "def show_save_confusion_matrix(predicted_labels, target_labels):\n",
    "    cm = confusion_matrix(target_labels, predicted_labels)\n",
    "\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='g',  cmap='Greens')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    # plt.show()\n",
    "\n",
    "    # Save the plot to a PDF\n",
    "    if SAVE_RESULTS:\n",
    "        pdf_filename = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_confusion_matrix.pdf\")\n",
    "        with PdfPages(pdf_filename) as pdf:\n",
    "            pdf.savefig()  # saves the current figure into a pdf page\n",
    "            # plt.close()\n",
    "            if SHOW_RESULTS:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94d4d2fe-fff3-404e-9b5d-dcb57f615d7c",
   "metadata": {
    "id": "94d4d2fe-fff3-404e-9b5d-dcb57f615d7c"
   },
   "outputs": [],
   "source": [
    "def calculate_TF_TP_FP_FN(true_labels_y_test, predicted_labels):\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(true_labels_y_test, predicted_labels)\n",
    "\n",
    "    class_to_performance_data = {}\n",
    "    # Calculate TP, FP, TN, FN for each class\n",
    "    num_classes = cm.shape[0]\n",
    "    for cls in range(num_classes):\n",
    "        TP = cm[cls, cls]\n",
    "        FP = cm[:, cls].sum() - TP\n",
    "        FN = cm[cls, :].sum() - TP\n",
    "        TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "        # Calculate support for each class\n",
    "        support = TP + FN\n",
    "\n",
    "        class_to_performance_data[cls] = {'TP': TP, 'FP': FP, 'TN': TN, 'FN': FN, 'Support': support}\n",
    "    return class_to_performance_data\n",
    "\n",
    "\n",
    "def calculate_metrics(class_to_performance_data, accuracy, SHOW_RESULTS=True, SAVE_RESULTS=True):\n",
    "    metrics_summary = {\n",
    "        'overall':{},\n",
    "        'Macro': {},\n",
    "        'Weighted': {}\n",
    "    }\n",
    "\n",
    "    metrics_summary[ 'overall']= {'accuracy': accuracy}\n",
    "\n",
    "    # Lists to store metric values for macro averaging\n",
    "    precision_list, recall_list, f1_score_list = [], [], []\n",
    "    fpr_list, fnr_list, fdr_list, npv_list = [], [], [], []\n",
    "\n",
    "    # Variables for weighted sum of metrics\n",
    "    weighted_precision, weighted_recall, weighted_f1 = 0, 0, 0\n",
    "    weighted_fpr, weighted_fnr, weighted_fdr, weighted_npv = 0, 0, 0, 0\n",
    "    total_support = 0\n",
    "\n",
    "    # Calculate metrics for each class\n",
    "    for class_id, metrics in class_to_performance_data.items():\n",
    "        tp = metrics['TP']\n",
    "        fp = metrics['FP']\n",
    "        tn = metrics['TN']\n",
    "        fn = metrics['FN']\n",
    "        support = metrics['Support']\n",
    "\n",
    "        # Basic evaluation metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        # Additional evaluation metrics\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fnr = fn / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fdr = fp / (fp + tp) if (fp + tp) > 0 else 0\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "\n",
    "        # Append to lists for macro averaging\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_score_list.append(f1_score)\n",
    "        fpr_list.append(fpr)\n",
    "        fnr_list.append(fnr)\n",
    "        fdr_list.append(fdr)\n",
    "        npv_list.append(npv)\n",
    "\n",
    "        # Weighted sum of metrics\n",
    "        weighted_precision += precision * support\n",
    "        weighted_recall += recall * support\n",
    "        weighted_f1 += f1_score * support\n",
    "        weighted_fpr += fpr * support\n",
    "        weighted_fnr += fnr * support\n",
    "        weighted_fdr += fdr * support\n",
    "        weighted_npv += npv * support\n",
    "        total_support += support\n",
    "\n",
    "    # Calculate macro averages and round to 5 decimal places\n",
    "    metrics_summary['Macro']['Precision'] = round(sum(precision_list) / len(precision_list), 5)\n",
    "    metrics_summary['Macro']['Recall'] = round(sum(recall_list) / len(recall_list), 5)\n",
    "    metrics_summary['Macro']['F1-Score'] = round(sum(f1_score_list) / len(f1_score_list), 5)\n",
    "    metrics_summary['Macro']['FPR'] = round(sum(fpr_list) / len(fpr_list), 5)\n",
    "    metrics_summary['Macro']['FNR'] = round(sum(fnr_list) / len(fnr_list), 5)\n",
    "    metrics_summary['Macro']['FDR'] = round(sum(fdr_list) / len(fdr_list), 5)\n",
    "    metrics_summary['Macro']['NPV'] = round(sum(npv_list) / len(npv_list), 5)\n",
    "\n",
    "    # Calculate weighted averages and round to 5 decimal places\n",
    "    if total_support > 0:\n",
    "        metrics_summary['Weighted']['Precision'] = round(weighted_precision / total_support, 5)\n",
    "        metrics_summary['Weighted']['Recall'] = round(weighted_recall / total_support, 5)\n",
    "        metrics_summary['Weighted']['F1-Score'] = round(weighted_f1 / total_support, 5)\n",
    "        metrics_summary['Weighted']['FPR'] = round(weighted_fpr / total_support, 5)\n",
    "        metrics_summary['Weighted']['FNR'] = round(weighted_fnr / total_support, 5)\n",
    "        metrics_summary['Weighted']['FDR'] = round(weighted_fdr / total_support, 5)\n",
    "        metrics_summary['Weighted']['NPV'] = round(weighted_npv / total_support, 5)\n",
    "\n",
    "    # Convert the nested dictionary into a DataFrame\n",
    "    report_df = pd.DataFrame.from_dict({(i+\" \"+j): metrics_summary[i][j]  for i in metrics_summary.keys()  for j in metrics_summary[i].keys()}, orient='index').reset_index()\n",
    "    # Rename columns for clarity\n",
    "    report_df.columns = ['Metric Type', 'Value']\n",
    "    if SHOW_RESULTS:\n",
    "        display(report_df)\n",
    "\n",
    "    if SAVE_RESULTS:\n",
    "        # if not os.path.exists(PATH_TO_SAVE_RESULT):\n",
    "        # # If it does not exist, create it\n",
    "        #     os.makedirs(PATH_TO_SAVE_RESULT)\n",
    "\n",
    "        prediction_report_filename = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_all_performance_report.csv\" )\n",
    "        # Save the DataFrame to a CSV file\n",
    "        report_df.to_csv(prediction_report_filename, index=True)\n",
    "\n",
    "    # return metrics_summary\n",
    "\n",
    "def calculate_accuracy(true_labels_y_test, predicted_labels):\n",
    "    # Ensure the inputs are NumPy arrays for element-wise comparison\n",
    "    true_labels_y_test = np.array(true_labels_y_test)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "    # Calculate the number of correct predictions\n",
    "    correct_predictions = np.sum(true_labels_y_test == predicted_labels)\n",
    "\n",
    "    # Calculate the total number of predictions\n",
    "    total_predictions = len(true_labels_y_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "    # Round accuracy to 5 decimal places\n",
    "    accuracy = round(accuracy, 5)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11b63a55-74b4-4982-9feb-e6776d23f685",
   "metadata": {
    "id": "11b63a55-74b4-4982-9feb-e6776d23f685"
   },
   "outputs": [],
   "source": [
    "def store_classification_report(predicted_labels, target_labels):\n",
    "    report_dict = classification_report(target_labels, predicted_labels, output_dict=True)\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    if SHOW_RESULTS:\n",
    "        display(report_df)\n",
    "\n",
    "    if SAVE_RESULTS:\n",
    "        prediction_report_filename = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_classification_report.csv\" )\n",
    "        # Save the DataFrame to a CSV file\n",
    "        report_df.to_csv(prediction_report_filename, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56b36c2e-1a4a-4895-ba57-fa7fe0e6caa5",
   "metadata": {
    "id": "56b36c2e-1a4a-4895-ba57-fa7fe0e6caa5"
   },
   "outputs": [],
   "source": [
    "def show_save_test_results(test_generator, best_model):\n",
    "    # Evaluate the model on test data\n",
    "    test_loss, test_accuracy, test_f1_score = best_model.evaluate(test_generator)\n",
    "\n",
    "    # print(f\"Test Loss: {test_loss}\")\n",
    "    # print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    # print(f\"Test F1 Score: {test_f1_score}\")\n",
    "\n",
    "    test_results = {\n",
    "        'model_name': model_name,\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_f1_score': test_f1_score\n",
    "    }\n",
    "\n",
    "        # Convert the dictionary to a DataFrame\n",
    "    report_df = pd.DataFrame([test_results])\n",
    "\n",
    "    if SHOW_RESULTS:\n",
    "        display(report_df)\n",
    "\n",
    "\n",
    "    if SAVE_RESULTS:\n",
    "        path_to_save = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_model_performance.csv\")\n",
    "        # Save the DataFrame to a CSV file\n",
    "        report_df.to_csv(path_to_save , index=False)\n",
    "        del report_df, test_results\n",
    "    return  test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2181fbed-1eed-4090-81b7-32e7b309aebf",
   "metadata": {
    "id": "2181fbed-1eed-4090-81b7-32e7b309aebf"
   },
   "outputs": [],
   "source": [
    "def calculate_test_performance_metrics_all(predicted_labels, target_labels, test_loss):\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(target_labels, predicted_labels)\n",
    "    precision_macro = precision_score(target_labels, predicted_labels, average='macro')\n",
    "    recall_macro = recall_score(target_labels, predicted_labels, average='macro')\n",
    "    f1_macro = f1_score_report(target_labels, predicted_labels, average='macro')\n",
    "\n",
    "    precision_weighted = precision_score(target_labels, predicted_labels, average='weighted')\n",
    "    recall_weighted = recall_score(target_labels, predicted_labels, average='weighted')\n",
    "    f1_weighted = f1_score_report(target_labels, predicted_labels, average='weighted')\n",
    "\n",
    "    # Create a dictionary to hold the metrics\n",
    "    test_results = {\n",
    "        'Model Name': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Macro Precision': precision_macro,\n",
    "        'Macro Recall': recall_macro,\n",
    "        'Macro F1-Score': f1_macro,\n",
    "        'Weighted Precision': precision_weighted,\n",
    "        'Weighted Recall': recall_weighted,\n",
    "        'Weighted F1-Score': f1_weighted,\n",
    "        'Loss': test_loss\n",
    "    }\n",
    "\n",
    "        # Convert the dictionary to a DataFrame\n",
    "    report_df = pd.DataFrame([test_results])\n",
    "\n",
    "    if SHOW_RESULTS:\n",
    "        display(report_df)\n",
    "\n",
    "    if SAVE_RESULTS:\n",
    "        path_to_save = os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_model_all_test_performance.csv\")\n",
    "        # Save the DataFrame to a CSV file\n",
    "        report_df.to_csv(path_to_save , index=False)\n",
    "\n",
    "    del report_df, test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3608cfb",
   "metadata": {
    "id": "f3608cfb"
   },
   "source": [
    "# MODEL CREATING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TU7_2IfFDkbP",
   "metadata": {
    "id": "TU7_2IfFDkbP"
   },
   "source": [
    "### DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06ebc955-a60d-4b9b-9979-b830992c7781",
   "metadata": {
    "id": "06ebc955-a60d-4b9b-9979-b830992c7781"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import  DenseNet121\n",
    "def get_DenseNet121():\n",
    "\n",
    "    # # Build the functional model\n",
    "    model = DenseNet121(input_shape=MODEL_INPUT_SIZE,  weights=None, classes=NUM_CLASSES)\n",
    "\n",
    "    optimizer = Adam(learning_rate=INITIAL_LR )\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  metrics=['accuracy', f1_score],\n",
    "                  optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6308b58-680e-434c-ae75-07ea53e2cad2",
   "metadata": {
    "id": "f6308b58-680e-434c-ae75-07ea53e2cad2"
   },
   "source": [
    "# Train and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06642652-4393-4874-b0eb-e5fed0234b00",
   "metadata": {
    "id": "06642652-4393-4874-b0eb-e5fed0234b00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mahri\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if model_name=='DenseNet121':\n",
    "#     model= get_DenseNet121()\n",
    "# elif model_name=='MobileNet':\n",
    "#     model= get_MobileNet_original()\n",
    "# elif model_name=='Xception':\n",
    "#     model= get_Xception()\n",
    "# elif model_name=='InceptionV3':\n",
    "#     model= get_InceptionV3()\n",
    "\n",
    "# ##Getting the model\n",
    "# model= get_MobileNet_original()\n",
    "model= get_DenseNet121()\n",
    "# model= get_Xception()\n",
    "# model= get_InceptionV3()\n",
    "# model = get_cait()\n",
    "# # ## Getting the model\n",
    "# model = get_basic_CNN()\n",
    "# model = get_ensemble2_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b68166-6085-47f2-a43a-20152cef8268",
   "metadata": {
    "id": "c3b68166-6085-47f2-a43a-20152cef8268"
   },
   "outputs": [],
   "source": [
    "# Now, load the best model\n",
    "best_model = ks.models.load_model(\"saved_models_basic_augment\\\\DenseNet121\\\\saved_models\\\\best_DenseNet121.h5\", custom_objects={'F1Score': F1Score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ec9d2-c3fc-4343-ad66-42d582449842",
   "metadata": {
    "id": "a71ec9d2-c3fc-4343-ad66-42d582449842"
   },
   "outputs": [],
   "source": [
    "# Get predictions from the model\n",
    "predictions = best_model.predict(test_generator, verbose=1)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "true_labels_y_test =  np.array(test_generator.classes) # y_test\n",
    "############################################################################################################]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bfa96a-c392-4b7f-9893-6214eb289c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def bootstrap_auc_ci(true_labels, predicted_probs, num_classes=NUM_CLASSES, n_iterations=100, seed=SEED, save_path=None):\n",
    "    \"\"\"\n",
    "    Computes bootstrapped AUC with 95% confidence interval for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        true_labels (np.array): Ground truth class labels (e.g., [0, 1, 2, ...])\n",
    "        predicted_probs (np.array): Predicted probabilities, shape = (n_samples, n_classes)\n",
    "        num_classes (int): Number of classes\n",
    "        n_iterations (int): Number of bootstrap iterations\n",
    "        seed (int): Random seed for reproducibility\n",
    "        save_path (str): Optional path to save AUC scores as CSV\n",
    "\n",
    "    Returns:\n",
    "        Prints mean AUC, std, and 95% CI\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    auc_scores = []\n",
    "\n",
    "    # One-hot encode true labels\n",
    "    true_labels_bin = label_binarize(true_labels, classes=np.arange(num_classes))\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        indices = np.random.choice(len(true_labels), size=len(true_labels), replace=True)\n",
    "        y_true_sample = true_labels_bin[indices]\n",
    "        y_pred_sample = predicted_probs[indices]\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true_sample, y_pred_sample, multi_class='ovr', average='macro')\n",
    "            auc_scores.append(auc)\n",
    "        except ValueError:\n",
    "            continue  # Skip if a class is missing in the sample\n",
    "\n",
    "    auc_scores = np.array(auc_scores)\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    std_auc = np.std(auc_scores)\n",
    "    ci_lower = np.percentile(auc_scores, 2.5)\n",
    "    ci_upper = np.percentile(auc_scores, 97.5)\n",
    "\n",
    "    print(f\"\\nBootstrapped Macro AUC ({n_iterations} runs):\")\n",
    "    print(f\"Mean AUC     : {mean_auc:.5f}\")\n",
    "    print(f\"Std Deviation: {std_auc:.5f}\")\n",
    "    print(f\"95% CI       : ({ci_lower:.5f}, {ci_upper:.5f})\")\n",
    "\n",
    "    if save_path:\n",
    "        df = pd.DataFrame({'AUC Score': auc_scores})\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "    return mean_auc, std_auc, (ci_lower, ci_upper)\n",
    "\n",
    "# Call the function\n",
    "bootstrap_auc_ci(\n",
    "    true_labels=true_labels_y_test,\n",
    "    predicted_probs=predictions,\n",
    "    save_path=os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_bootstrapped_auc.csv\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "69dada5c-1141-4914-96bd-a8ff36749e6d",
    "da5ca5e3-a160-4ee9-a451-3041c841fad9",
    "938f7a2c-c72e-46d6-bdf6-ea7c300419c9",
    "F0UlityPEMxc",
    "IMT4sVSuDa56",
    "3xZyTkwcDeE2",
    "esq3i0AwDhYD",
    "TU7_2IfFDkbP",
    "-PiTU6R0Dq6y",
    "usVCdrmaDvxw",
    "dfdd7a6b-3ba9-4754-af22-fa0aff5bad4e"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-13T13:42:48.933576",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
