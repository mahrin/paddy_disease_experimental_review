{"cells":[{"cell_type":"markdown","id":"c4f40068-3bd2-4464-8c10-1ae560f13570","metadata":{"id":"c4f40068-3bd2-4464-8c10-1ae560f13570"},"source":["## Importing necessary modules"]},{"cell_type":"code","execution_count":null,"id":"pcyiTh4DiVge","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1904,"status":"ok","timestamp":1749655114965,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"},"user_tz":-360},"id":"pcyiTh4DiVge","outputId":"48656e3f-e100-4790-c5dc-5e0a13c3b450"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# !pip uninstall -y numpy pandas\n","# !pip install numpy==1.26.4 pandas==2.2.2\n","\n","# imgaug (compatible with older NumPy)\n","!pip install -q imgaug==0.4.0\n","\n","# Downgrade NumPy to avoid issues with imgaug (NumPy â‰¥ 2.0 breaks imgaug)\n","!pip install -q numpy==1.26.4\n","\n","!pip install tensorflow==2.15.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3gr2vVd1CGPI","executionInfo":{"status":"ok","timestamp":1749655122061,"user_tz":-360,"elapsed":7093,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"}},"outputId":"6bbe6ca9-7007-420e-efaf-395ad964269f"},"id":"3gr2vVd1CGPI","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow==2.15.0 in /usr/local/lib/python3.11/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.13.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.25.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.14.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.37.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.72.1)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.38.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2025.4.26)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n"]}]},{"cell_type":"code","execution_count":null,"id":"fa64290b-2570-4f24-920a-07a72a3660b9","metadata":{"id":"fa64290b-2570-4f24-920a-07a72a3660b9"},"outputs":[],"source":["import time\n","from tqdm import tqdm # Cool progress bar\n","\n","import random\n","import numpy as np\n","import pandas as pd\n","import sys\n","import os # read and manipulate local files\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","import cv2\n","import seaborn as sns\n","\n","from PIL import Image\n","\n","import tensorflow.keras as ks\n","import tensorflow as tf\n","\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import backend as K # F1-score metric\n","\n","from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.layers import   Lambda, Conv2D, MaxPool2D, UpSampling2D, BatchNormalization, Flatten\n","from tensorflow.keras.layers import  GlobalAveragePooling2D, Reshape, Multiply, Attention, add,Resizing,  Input, Dense\n","from tensorflow.keras.layers import Activation,AveragePooling2D, MaxPooling2D, Dropout, Conv2DTranspose, Concatenate\n","from tensorflow.keras.models import Model, Sequential\n","\n","# hide wornings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# from keras import ops# For deeplab3\n","\n","# import keras_cv # basnet\n","\n","from imgaug import augmenters as iaa ## for augmentation"]},{"cell_type":"markdown","id":"bb6fe714-f8fb-4f9d-9657-ca3373fd9779","metadata":{"id":"bb6fe714-f8fb-4f9d-9657-ca3373fd9779"},"source":["## Defining paths"]},{"cell_type":"code","execution_count":null,"id":"051a2f6e-d46f-44ae-bfce-e2eb91954b5f","metadata":{"id":"051a2f6e-d46f-44ae-bfce-e2eb91954b5f"},"outputs":[],"source":["model_name = 'Attention_UNet_2500_2nd_Run'\n","\n","################################################################################################\n","## SETTING THE PATHS\n","PATH_TO_CODE ='/content/drive/MyDrive/TRIAL_v1/segmentation_task'\n","# PATH_TO_CODE =''\n","sys.path.append(PATH_TO_CODE)\n","\n","# DATASET_PATH = r'C:\\Users\\ASUS\\Desktop\\segmentation & cropping\\segmentation_task\\segmentation_data43_resized_cropped_split'\n","DATASET_PATH = r'/content/drive/MyDrive/TRIAL_v1/segmentation_task/segmentation_data43_resized_cropped_split'\n","# DATASET_PATH = r'segmentation_data43_resized_cropped_split'\n","DATASET_PATH_TRAIN = os.path.join(DATASET_PATH, 'train')\n","DATASET_PATH_TEST = os.path.join(DATASET_PATH, 'test')\n","\n","\n","PATH_TO_SAVE_RESULT = os.path.join(PATH_TO_CODE, 'saved_outputs_segmentation_AUC', model_name)\n","PATH_BEST_SAVE_WEIGHT = os.path.join(PATH_TO_SAVE_RESULT,'saved_weights')\n","PATH_TO_SAVE_MODEL = os.path.join(PATH_TO_SAVE_RESULT, 'saved_models')\n","PATH_SAVE_HISTORY = os.path.join(PATH_TO_SAVE_RESULT, model_name+'_training_history.csv')\n","PATH_SAVE_TIME = os.path.join(PATH_TO_SAVE_RESULT, model_name+'_training_time.csv')\n","PATH_SAVE_AUGMENT_SAMPLE = os.path.join(PATH_TO_SAVE_RESULT, model_name+'_augmented_sample.pdf')\n","PATH_SAVE_PIXEL_PERCENTAGE_PLOT = os.path.join(PATH_TO_SAVE_RESULT, model_name+'_pixel_percentage.pdf')\n","# PATH_TO_SAVE_TUNER = os.path.join(PATH_TO_RESULT, 'saved_tuner_model')\n","################################################################################################\n","\n","if not os.path.exists(PATH_TO_SAVE_RESULT):\n"," # If it does not exist, create it\n","    os.makedirs(PATH_TO_SAVE_RESULT)\n","################################################################################################\n","IMG_HEIGHT = 256\n","IMG_WIDTH = 256\n","ORIGINAL_IMAGE_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n","COLOR_CHANNEL = 3\n","\n","RESIZE_SHAPE = ORIGINAL_IMAGE_SIZE #(128, 128) #ORIGINAL_IMAGE_SIZE#(128, 128)# # #ORIGINAL_IMAGE_SIZE# (128, 128)#\n","MODEL_INPUT_SIZE = (RESIZE_SHAPE[0], RESIZE_SHAPE[1], COLOR_CHANNEL)\n","\n","VALIDATION_SPLIT= 0.2\n","NUM_CLASSES = 2 # Disease and not disease\n","\n","BATCH_SIZE = 10\n","EPOCHS = 70\n","\n","TOTAL_DATA = 2500\n","################################################################################################\n","\n","################################################################################################\n","\n","SAVE_RESULTS = True\n","SHOW_RESULTS = True\n","\n","################################################################################################\n","# Setting the seed\n","SEED  = 123\n","RNG = np.random.default_rng(SEED) # Random number generator\n","tf.random.set_seed(SEED)\n","\n","################################################################################################\n","# Checkpoint parameters val_binary_io_u\n","SCORE_TO_MONITOR = 'val_binary_io_u' # Score that checkpoints monitor during training\n","SCORE_OBJECTIVE  = 'max'          # 'max' or 'min', specifies whether the objective is to maximize the score or minimize it.\n","PATIENCE_LR_REDUCE = 5\n","MIN_LR = 1e-8\n","REDUCTION_FACTOR = 0.5            # Factor which lr will be reduced with at plateau\n","# COOLDOWN_EPOCHS  = 2 #cooldown: Integer. Number of epochs to wait before resuming normal operation after the learning rate has been reduced.\n","\n","INITIAL_LR = 0.001"]},{"cell_type":"markdown","id":"2b874d87-e287-4dda-a0d6-79ff516adc3f","metadata":{"id":"2b874d87-e287-4dda-a0d6-79ff516adc3f","jp-MarkdownHeadingCollapsed":true},"source":["## Defining performance metrics\n","\n","Official segmentation metrics by keras: https://ks.io/api/metrics/segmentation_metrics/"]},{"cell_type":"code","execution_count":null,"id":"b8197196-670d-4e41-af29-452a87af68ba","metadata":{"id":"b8197196-670d-4e41-af29-452a87af68ba"},"outputs":[],"source":["def f1_score(y_true, y_pred): # Dice coefficient\n","    \"\"\"\n","    Calculate the F1 score, the harmonic mean of precision and recall, for binary classification.\n","\n","    Args:\n","        y_true (Tensor): True binary labels.\n","        y_pred (Tensor): Predicted probabilities.\n","\n","    Returns:\n","        float32: F1 score as a scalar.\n","    \"\"\"\n","    # True Positives: round product of y_true and y_pred\n","    TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    # Actual Positives: round y_true\n","    P = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    # Recall: TP / Actual Positives\n","    recall = TP / (P + K.epsilon())\n","\n","    # Predicted Positives: round y_pred\n","    Pred_P = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    # Precision: TP / Predicted Positives\n","    precision = TP / (Pred_P + K.epsilon())\n","\n","    # F1 Score: harmonic mean of precision and recall\n","    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n","\n","# source: https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryIoU\n","iou_score_binary = tf.keras.metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.5)\n","\n","def accuracy_score(y_true, y_pred):\n","    \"\"\"\n","    Calculate accuracy score between two binary masks.\n","    \"\"\"\n","    correct = np.sum(y_true == y_pred)  # Count correct predictions\n","    total = y_true.size  # Total number of pixels\n","    return correct / total  # Accuracy calculation\n","\n","def precision_score(groundtruth_mask, pred_mask):\n","    \"\"\"\n","    Calculate precision score between two binary masks.\n","    \"\"\"\n","    intersect = np.sum(pred_mask * groundtruth_mask)  # Calculate intersection\n","    total_pixel_pred = np.sum(pred_mask)  # Sum of predicted positives\n","    return np.mean(intersect / total_pixel_pred)  # Precision calculation\n","\n","def recall_score(groundtruth_mask, pred_mask):\n","    \"\"\"\n","    Calculate recall score between two binary masks.\n","    \"\"\"\n","    intersect = np.sum(pred_mask * groundtruth_mask)  # Calculate intersection\n","    total_pixel_truth = np.sum(groundtruth_mask)  # Sum of actual positives\n","    return np.mean(intersect / total_pixel_truth)  # Recall calculation"]},{"cell_type":"markdown","id":"ef00a1c4-00ef-4dc7-926d-638a5146c2c4","metadata":{"id":"ef00a1c4-00ef-4dc7-926d-638a5146c2c4","jp-MarkdownHeadingCollapsed":true},"source":["## Reading the data"]},{"cell_type":"code","execution_count":null,"id":"30e64959-9aaf-4396-8ca0-0378ea018ef6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43507,"status":"ok","timestamp":1749655172840,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"},"user_tz":-360},"id":"30e64959-9aaf-4396-8ca0-0378ea018ef6","outputId":"1e46e920-7d52-470b-9936-d9136f5b74e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 616 original images into x_train\n","Loaded 616 mask images into y_train\n","Loaded 23 original images into x_test\n","Loaded 23 mask images into y_test\n"]}],"source":["# Function to read images from a directory using Pillow\n","def read_images_from_directory(directory_path):\n","    images = []\n","    for filename in sorted(os.listdir(directory_path)):\n","        # filename= filename.lower()\n","\n","        if filename.endswith(('.png', '.jpg', '.JPG' )):  # Check for image file extensions\n","            img_path = os.path.join(directory_path, filename)\n","            img = Image.open(img_path)\n","            img_array = np.array(img)  # Convert the image to a numpy array if needed\n","            if img_array is not None:\n","                images.append(img_array)\n","    return images\n","\n","# Paths to masks and original images within the dataset\n","masks_path_train = os.path.join(DATASET_PATH_TRAIN, 'data43_masks_binarised')\n","originals_path_train = os.path.join(DATASET_PATH_TRAIN, 'data43_original')\n","\n","# Read images into variables\n","x_train = read_images_from_directory(originals_path_train)  # Original images\n","y_train = read_images_from_directory(masks_path_train)  # Masks\n","\n","# Paths to masks and original images within the test dataset\n","masks_path_test = os.path.join(DATASET_PATH_TEST, 'data43_masks_binarised')\n","originals_path_test = os.path.join(DATASET_PATH_TEST, 'data43_original')\n","\n","# Read images into variables\n","x_test = read_images_from_directory(originals_path_test)  # Original images\n","y_test = read_images_from_directory(masks_path_test)  # Masks\n","\n","# Now x_train contains original images, and y_train contains mask images\n","print(f\"Loaded {len(x_train)} original images into x_train\")\n","print(f\"Loaded {len(y_train)} mask images into y_train\")\n","\n","# Similarly, for test images\n","print(f\"Loaded {len(x_test)} original images into x_test\")\n","print(f\"Loaded {len(y_test)} mask images into y_test\")"]},{"cell_type":"markdown","id":"6833c433-4baf-4162-ad11-38b8c933e7c0","metadata":{"id":"6833c433-4baf-4162-ad11-38b8c933e7c0","jp-MarkdownHeadingCollapsed":true},"source":["## Augmentations"]},{"cell_type":"code","execution_count":null,"id":"37179298-69f2-4faf-9b69-2d91f63d710c","metadata":{"id":"37179298-69f2-4faf-9b69-2d91f63d710c"},"outputs":[],"source":["seq = iaa.Sequential([\n","    iaa.Fliplr(0.5),  # horizontally flip 50% of the images\n","    iaa.Flipud(0.2),  # vertically flip 20% of the images\n","    iaa.Affine(\n","        scale={\"x\": (0.8, 1.1), \"y\": (0.8, 1.1)},  # zoom in or out (80-120%)\n","        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},  # width & height shift (-20 to +20%)\n","        rotate=(-15, 15),  # rotation (-45 to 45 degrees)\n","        shear=(-16, 16)  # shear (-16 to 16 degrees)\n","    ),\n","    iaa.ElasticTransformation(alpha=(0, 2.0), sigma=0.25),  # apply elastic deformations\n","    iaa.GaussianBlur(sigma=(0, 2.0))  # apply Gaussian blur\n","])"]},{"cell_type":"code","execution_count":null,"id":"d401748b-eeef-4842-a449-b4c0b0b8e985","metadata":{"id":"d401748b-eeef-4842-a449-b4c0b0b8e985"},"outputs":[],"source":["# Function to augment a batch of images and masks\n","def augment_batch(images, masks, seq, batch_size):\n","    augmented_images = []\n","    augmented_masks = []\n","    while len(augmented_images) < batch_size:\n","        aug_images, aug_masks = seq(images=images, segmentation_maps=np.expand_dims(masks, axis=-1))\n","        augmented_images.extend(aug_images)\n","        augmented_masks.extend(aug_masks)\n","\n","        if len(augmented_images) >= batch_size:\n","            break\n","\n","    # Ensure we only take as many as we need to reach the desired batch_size\n","    augmented_images = augmented_images[:batch_size]\n","    augmented_masks = augmented_masks[:batch_size]\n","\n","    return np.array(augmented_images), np.squeeze(np.array(augmented_masks), axis=-1)"]},{"cell_type":"code","execution_count":null,"id":"e4233d82-f9ec-4f00-953f-10854ca1541c","metadata":{"id":"e4233d82-f9ec-4f00-953f-10854ca1541c"},"outputs":[],"source":["num_augmented_images_needed = TOTAL_DATA  - len(x_train)  # Calculate how many augmented images we need\n","if num_augmented_images_needed > 0:\n","    augmented_x, augmented_y = augment_batch(x_train, y_train, seq, num_augmented_images_needed)\n","    # Concatenate the original and augmented datasets\n","    combined_x_train = np.concatenate((x_train, augmented_x), axis=0)\n","    combined_y_train = np.concatenate((y_train, augmented_y), axis=0)\n","else:\n","    combined_x_train = x_train\n","    combined_y_train = y_train"]},{"cell_type":"code","execution_count":null,"id":"0c5523b2-b361-42ce-b60a-a01f69f5f4a1","metadata":{"id":"0c5523b2-b361-42ce-b60a-a01f69f5f4a1"},"outputs":[],"source":["x_train = combined_x_train\n","y_train = combined_y_train"]},{"cell_type":"markdown","id":"6b560e6d-8888-4f2a-8daf-e0b5340ac2f8","metadata":{"id":"6b560e6d-8888-4f2a-8daf-e0b5340ac2f8","jp-MarkdownHeadingCollapsed":true},"source":["## Preprocessing DO NOT RESIZE THE IMAGE-- IT GIVES FINE LINES IN THE OUTPUT"]},{"cell_type":"code","execution_count":null,"id":"345f792d-94b1-4954-872e-461bd735c001","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1133,"status":"ok","timestamp":1749655190212,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"},"user_tz":-360},"id":"345f792d-94b1-4954-872e-461bd735c001","outputId":"d7d25f41-ea88-4f9b-9676-c67cf0b8358c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 2500 original images into x_train\n","Loaded 2500 mask images into y_train\n","Loaded 23 original images into x_test\n","Loaded 23 mask images into y_test\n"]}],"source":["# # # Resizing\n","# x_train = [np.array(Image.fromarray(image).resize(RESIZE_SHAPE)) for image in x_train]\n","# y_train = [np.array(Image.fromarray(image).resize(RESIZE_SHAPE)) for image in y_train]\n","\n","# x_test = [np.array(Image.fromarray(image).resize(RESIZE_SHAPE)) for image in x_test]\n","# y_test = [np.array(Image.fromarray(image).resize(RESIZE_SHAPE)) for image in y_test]\n","\n","# Convert the list to a NumPy array\n","x_train = np.array(x_train)\n","y_train = np.array(y_train)\n","\n","x_test = np.array(x_test)\n","y_test = np.array(y_test)\n","\n","# Expand mask dimensions to (batch, 256, 256, 1)\n","y_train = np.expand_dims(y_train, axis=-1)\n","y_test = np.expand_dims(y_test, axis=-1)\n","\n","\n","# Normalizing input between [0,1]\n","x_train = x_train.astype(\"float32\")/ np.max(x_train)\n","x_test  = x_test.astype(\"float32\")/np.max(x_test)\n","\n","y_train = y_train.astype(\"float32\")/ np.max(y_train)\n","y_test  = y_test.astype(\"float32\")/np.max(y_test)\n","\n","# Now x_train contains original images, and y_train contains mask images\n","print(f\"Loaded {len(x_train)} original images into x_train\")\n","print(f\"Loaded {len(y_train)} mask images into y_train\")\n","\n","# Similarly, for test images\n","print(f\"Loaded {len(x_test)} original images into x_test\")\n","print(f\"Loaded {len(y_test)} mask images into y_test\")"]},{"cell_type":"markdown","id":"3631b8c9-b763-4574-8444-2034cad11cee","metadata":{"id":"3631b8c9-b763-4574-8444-2034cad11cee","jp-MarkdownHeadingCollapsed":true},"source":["## Essential functions"]},{"cell_type":"code","execution_count":null,"id":"73061e0d-ae45-45fb-9ba2-8099845466b3","metadata":{"id":"73061e0d-ae45-45fb-9ba2-8099845466b3"},"outputs":[],"source":["def fit_and_save_best_model(model_name, model, epochs=EPOCHS):\n","    ##########################################################################################################\n","    saved_best_model_name = 'best_'+model_name+'.h5'\n","\n","    model_saving_path = os.path.join(PATH_TO_SAVE_MODEL, saved_best_model_name)\n","    # Set up a model checkpoint to save the best model during training\n","    best_model_callback= ModelCheckpoint(model_saving_path,\n","                                          monitor=SCORE_TO_MONITOR,\n","                                          save_best_only=True,\n","                                          mode=SCORE_OBJECTIVE,\n","                                          verbose=1)\n","\n","    reduce_LR = ReduceLROnPlateau(\n","        factor=REDUCTION_FACTOR,      # Factor by which the learning rate will be reduced. new_lr = lr * factor\n","        patience=PATIENCE_LR_REDUCE, # original was 5      # Number of epochs with no improvement after which learning rate will be reduced.\n","        verbose=1,       # int. 0: quiet, 1: update messages.\n","        min_lr=MIN_LR   # Lower bound on the learning rate.\n","    )\n","\n","    my_callbacks = [best_model_callback , reduce_LR]\n","    ##########################################################################################################\n","\n","    start_time = time.time()\n","\n","    # Fitting the model\n","    train_history = model.fit(\n","        x_train,\n","        y_train,\n","        epochs= epochs,\n","        batch_size=BATCH_SIZE,\n","        validation_split= VALIDATION_SPLIT,\n","        callbacks=my_callbacks,\n","    )\n","    total_time = time.time() - start_time\n","\n","    return saved_best_model_name, train_history, total_time, model"]},{"cell_type":"markdown","id":"bc3dd825-59a8-43f1-9a3c-b720ebd250dc","metadata":{"id":"bc3dd825-59a8-43f1-9a3c-b720ebd250dc"},"source":["## Models\n","### Models worked\n","- Model01: Basic basic_unet\n","- Model02: unet_vgg16 (my own custom)\n","- Model03: TransUNet\n","- Model04: DeepLab-v3Plus\n","- Model05: ResUNet\n","- Model06: UNet_with_attention\n","- Model07: Multi-resUnet\n","- Model08: Inception_resnetV2 (my own custom)\n","\n"]},{"cell_type":"markdown","id":"BFPKzpxR5aIS","metadata":{"id":"BFPKzpxR5aIS"},"source":["### UNet_with_attention--works--Ran with batch size10 (memory issues)\n","---Works properly after 10 epochs--within 5 epoch--sometimes does not show anything"]},{"cell_type":"code","execution_count":null,"id":"OfLbIpNgIimq","metadata":{"id":"OfLbIpNgIimq"},"outputs":[],"source":["def conv_block(x, num_filters):\n","    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","\n","    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","\n","    return x\n","\n","def encoder_block(x, num_filters):\n","    x = conv_block(x, num_filters)\n","    p = MaxPool2D((2, 2))(x)\n","    return x, p\n","\n","def attention_gate(g, s, num_filters):\n","    Wg = Conv2D(num_filters, 1, padding=\"same\")(g)\n","    Wg = BatchNormalization()(Wg)\n","\n","    Ws = Conv2D(num_filters, 1, padding=\"same\")(s)\n","    Ws = BatchNormalization()(Ws)\n","\n","    out = Activation(\"relu\")(Wg + Ws)\n","    out = Conv2D(num_filters, 1, padding=\"same\")(out)\n","    out = Activation(\"sigmoid\")(out)\n","\n","    return out * s\n","\n","def decoder_block(x, s, num_filters):\n","    x = UpSampling2D(interpolation=\"bilinear\")(x)\n","    s = attention_gate(x, s, num_filters)\n","    x = Concatenate()([x, s])\n","    x = conv_block(x, num_filters)\n","    return x\n","\n","def UNet_with_attention():\n","    \"\"\" Inputs \"\"\"\n","\n","    ### Source: https://github.com/nikhilroxtomar/Semantic-Segmentation-Architecture/blob/main/TensorFlow/attention-unet.py\n","    inputs = Input(MODEL_INPUT_SIZE)\n","\n","    \"\"\" Encoder \"\"\"\n","    s1, p1 = encoder_block(inputs, 64)\n","    s2, p2 = encoder_block(p1, 128)\n","    s3, p3 = encoder_block(p2, 256)\n","\n","    b1 = conv_block(p3, 512)\n","\n","    \"\"\" Decoder \"\"\"\n","    d1 = decoder_block(b1, s3, 256)\n","    d2 = decoder_block(d1, s2, 128)\n","    d3 = decoder_block(d2, s1, 64)\n","\n","    \"\"\" Outputs \"\"\"\n","    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d3)\n","\n","    \"\"\" Model \"\"\"\n","    model = Model(inputs, outputs, name=\"Attention-UNET\")\n","    optimizer = Adam(learning_rate=INITIAL_LR)\n","    model.compile(loss='binary_crossentropy',\n","                  metrics=['accuracy', f1_score, iou_score_binary],\n","                  optimizer=optimizer)\n","\n","    return model"]},{"cell_type":"markdown","id":"c8391326-d459-4214-ad6c-efd1d12ad54a","metadata":{"id":"c8391326-d459-4214-ad6c-efd1d12ad54a"},"source":["## Train and Results"]},{"cell_type":"code","execution_count":null,"id":"3d3e72d6-6137-46fe-9a41-418999643dbb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":861,"status":"ok","timestamp":1749655191212,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"},"user_tz":-360},"id":"3d3e72d6-6137-46fe-9a41-418999643dbb","outputId":"c07b9c16-1b98-4e4e-f13a-64545fe46697"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"Attention-UNET\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n","                                                                                                  \n"," conv2d (Conv2D)             (None, 256, 256, 64)         1792      ['input_1[0][0]']             \n","                                                                                                  \n"," batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              \n"," Normalization)                                                                                   \n","                                                                                                  \n"," activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] \n","                                                                                                  \n"," conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          \n","                                                                                                  \n"," batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        \n"," D)                                                                                               \n","                                                                                                  \n"," conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       \n","                                                                                                  \n"," batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        \n","                                                                                                  \n"," batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     \n","                                                                                                  \n"," batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        \n","                                                                                                  \n"," batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     \n","                                                                                                  \n"," batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        \n","                                                                                                  \n"," batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," up_sampling2d (UpSampling2  (None, 64, 64, 512)          0         ['activation_7[0][0]']        \n"," D)                                                                                               \n","                                                                                                  \n"," conv2d_8 (Conv2D)           (None, 64, 64, 256)          131328    ['up_sampling2d[0][0]']       \n","                                                                                                  \n"," conv2d_9 (Conv2D)           (None, 64, 64, 256)          65792     ['activation_5[0][0]']        \n","                                                                                                  \n"," batch_normalization_8 (Bat  (None, 64, 64, 256)          1024      ['conv2d_8[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," batch_normalization_9 (Bat  (None, 64, 64, 256)          1024      ['conv2d_9[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," tf.__operators__.add (TFOp  (None, 64, 64, 256)          0         ['batch_normalization_8[0][0]'\n"," Lambda)                                                            , 'batch_normalization_9[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," activation_8 (Activation)   (None, 64, 64, 256)          0         ['tf.__operators__.add[0][0]']\n","                                                                                                  \n"," conv2d_10 (Conv2D)          (None, 64, 64, 256)          65792     ['activation_8[0][0]']        \n","                                                                                                  \n"," activation_9 (Activation)   (None, 64, 64, 256)          0         ['conv2d_10[0][0]']           \n","                                                                                                  \n"," tf.math.multiply (TFOpLamb  (None, 64, 64, 256)          0         ['activation_9[0][0]',        \n"," da)                                                                 'activation_5[0][0]']        \n","                                                                                                  \n"," concatenate (Concatenate)   (None, 64, 64, 768)          0         ['up_sampling2d[0][0]',       \n","                                                                     'tf.math.multiply[0][0]']    \n","                                                                                                  \n"," conv2d_11 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate[0][0]']         \n","                                                                                                  \n"," batch_normalization_10 (Ba  (None, 64, 64, 256)          1024      ['conv2d_11[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_10 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_10[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_12 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_10[0][0]']       \n","                                                                                                  \n"," batch_normalization_11 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_11 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_11[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," up_sampling2d_1 (UpSamplin  (None, 128, 128, 256)        0         ['activation_11[0][0]']       \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_13 (Conv2D)          (None, 128, 128, 128)        32896     ['up_sampling2d_1[0][0]']     \n","                                                                                                  \n"," conv2d_14 (Conv2D)          (None, 128, 128, 128)        16512     ['activation_3[0][0]']        \n","                                                                                                  \n"," batch_normalization_12 (Ba  (None, 128, 128, 128)        512       ['conv2d_13[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," batch_normalization_13 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," tf.__operators__.add_1 (TF  (None, 128, 128, 128)        0         ['batch_normalization_12[0][0]\n"," OpLambda)                                                          ',                            \n","                                                                     'batch_normalization_13[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," activation_12 (Activation)  (None, 128, 128, 128)        0         ['tf.__operators__.add_1[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_15 (Conv2D)          (None, 128, 128, 128)        16512     ['activation_12[0][0]']       \n","                                                                                                  \n"," activation_13 (Activation)  (None, 128, 128, 128)        0         ['conv2d_15[0][0]']           \n","                                                                                                  \n"," tf.math.multiply_1 (TFOpLa  (None, 128, 128, 128)        0         ['activation_13[0][0]',       \n"," mbda)                                                               'activation_3[0][0]']        \n","                                                                                                  \n"," concatenate_1 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_1[0][0]',     \n"," )                                                                   'tf.math.multiply_1[0][0]']  \n","                                                                                                  \n"," conv2d_16 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_1[0][0]']       \n","                                                                                                  \n"," batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_16[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_17 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       \n","                                                                                                  \n"," batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_17[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," up_sampling2d_2 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_18 (Conv2D)          (None, 256, 256, 64)         8256      ['up_sampling2d_2[0][0]']     \n","                                                                                                  \n"," conv2d_19 (Conv2D)          (None, 256, 256, 64)         4160      ['activation_1[0][0]']        \n","                                                                                                  \n"," batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_18[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," tf.__operators__.add_2 (TF  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]\n"," OpLambda)                                                          ',                            \n","                                                                     'batch_normalization_17[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," activation_16 (Activation)  (None, 256, 256, 64)         0         ['tf.__operators__.add_2[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_20 (Conv2D)          (None, 256, 256, 64)         4160      ['activation_16[0][0]']       \n","                                                                                                  \n"," activation_17 (Activation)  (None, 256, 256, 64)         0         ['conv2d_20[0][0]']           \n","                                                                                                  \n"," tf.math.multiply_2 (TFOpLa  (None, 256, 256, 64)         0         ['activation_17[0][0]',       \n"," mbda)                                                               'activation_1[0][0]']        \n","                                                                                                  \n"," concatenate_2 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_2[0][0]',     \n"," )                                                                   'tf.math.multiply_2[0][0]']  \n","                                                                                                  \n"," conv2d_21 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_2[0][0]']       \n","                                                                                                  \n"," batch_normalization_18 (Ba  (None, 256, 256, 64)         256       ['conv2d_21[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_18 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_18[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_22 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_18[0][0]']       \n","                                                                                                  \n"," batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_22[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_23 (Conv2D)          (None, 256, 256, 1)          65        ['activation_19[0][0]']       \n","                                                                                                  \n","==================================================================================================\n","Total params: 8143169 (31.06 MB)\n","Trainable params: 8135745 (31.04 MB)\n","Non-trainable params: 7424 (29.00 KB)\n","__________________________________________________________________________________________________\n"]}],"source":["# model= get_basic_unet() ## Model1\n","# model= get_unet_vgg16() ## Model2\n","# model= get_TransUNet() ## Model3\n","# model= get_DeepLabV3Plus() ## Model4\n","# model = get_ResUNet_v3() ## Model5\n","model = UNet_with_attention() ## Model6\n","# model = get_multiresunet() ## Model7\n","# model = get_unet_InceptionResNetV2() ## Model8\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"id":"e297b3da-e42c-4ee2-8dda-43cd0016bf00","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e297b3da-e42c-4ee2-8dda-43cd0016bf00","outputId":"c4d0825e-f7fe-438a-b564-9d8de2976abf","executionInfo":{"status":"ok","timestamp":1749732693674,"user_tz":-360,"elapsed":7594275,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/70\n","200/200 [==============================] - ETA: 0s - loss: 0.1641 - accuracy: 0.9591 - f1_score: 0.6177 - binary_io_u: 0.6618\n","Epoch 1: val_binary_io_u improved from -inf to 0.49738, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1118s 6s/step - loss: 0.1641 - accuracy: 0.9591 - f1_score: 0.6177 - binary_io_u: 0.6618 - val_loss: 0.3187 - val_accuracy: 0.9627 - val_f1_score: 0.0468 - val_binary_io_u: 0.4974 - lr: 0.0010\n","Epoch 2/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9807 - f1_score: 0.7190 - binary_io_u: 0.7803\n","Epoch 2: val_binary_io_u improved from 0.49738 to 0.54096, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0659 - accuracy: 0.9807 - f1_score: 0.7190 - binary_io_u: 0.7803 - val_loss: 0.1062 - val_accuracy: 0.9664 - val_f1_score: 0.1671 - val_binary_io_u: 0.5410 - lr: 0.0010\n","Epoch 3/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9815 - f1_score: 0.7373 - binary_io_u: 0.7898\n","Epoch 3: val_binary_io_u improved from 0.54096 to 0.72613, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1109s 6s/step - loss: 0.0575 - accuracy: 0.9815 - f1_score: 0.7373 - binary_io_u: 0.7898 - val_loss: 0.0643 - val_accuracy: 0.9794 - val_f1_score: 0.5856 - val_binary_io_u: 0.7261 - lr: 0.0010\n","Epoch 4/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.9825 - f1_score: 0.7484 - binary_io_u: 0.7979\n","Epoch 4: val_binary_io_u improved from 0.72613 to 0.79760, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1109s 6s/step - loss: 0.0534 - accuracy: 0.9825 - f1_score: 0.7484 - binary_io_u: 0.7979 - val_loss: 0.0522 - val_accuracy: 0.9836 - val_f1_score: 0.7060 - val_binary_io_u: 0.7976 - lr: 0.0010\n","Epoch 5/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9829 - f1_score: 0.7571 - binary_io_u: 0.8020\n","Epoch 5: val_binary_io_u improved from 0.79760 to 0.80731, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1107s 6s/step - loss: 0.0515 - accuracy: 0.9829 - f1_score: 0.7571 - binary_io_u: 0.8020 - val_loss: 0.0488 - val_accuracy: 0.9837 - val_f1_score: 0.7209 - val_binary_io_u: 0.8073 - lr: 0.0010\n","Epoch 6/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9835 - f1_score: 0.7651 - binary_io_u: 0.8096\n","Epoch 6: val_binary_io_u did not improve from 0.80731\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0491 - accuracy: 0.9835 - f1_score: 0.7651 - binary_io_u: 0.8096 - val_loss: 0.0714 - val_accuracy: 0.9732 - val_f1_score: 0.6261 - val_binary_io_u: 0.7411 - lr: 0.0010\n","Epoch 7/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9829 - f1_score: 0.7566 - binary_io_u: 0.8016\n","Epoch 7: val_binary_io_u did not improve from 0.80731\n","200/200 [==============================] - 1107s 6s/step - loss: 0.0506 - accuracy: 0.9829 - f1_score: 0.7566 - binary_io_u: 0.8016 - val_loss: 0.0528 - val_accuracy: 0.9815 - val_f1_score: 0.7238 - val_binary_io_u: 0.8005 - lr: 0.0010\n","Epoch 8/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9842 - f1_score: 0.7733 - binary_io_u: 0.8161\n","Epoch 8: val_binary_io_u did not improve from 0.80731\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0469 - accuracy: 0.9842 - f1_score: 0.7733 - binary_io_u: 0.8161 - val_loss: 0.0476 - val_accuracy: 0.9838 - val_f1_score: 0.7072 - val_binary_io_u: 0.8011 - lr: 0.0010\n","Epoch 9/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9843 - f1_score: 0.7759 - binary_io_u: 0.8172\n","Epoch 9: val_binary_io_u improved from 0.80731 to 0.80793, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1109s 6s/step - loss: 0.0462 - accuracy: 0.9843 - f1_score: 0.7759 - binary_io_u: 0.8172 - val_loss: 0.0555 - val_accuracy: 0.9825 - val_f1_score: 0.7356 - val_binary_io_u: 0.8079 - lr: 0.0010\n","Epoch 10/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9844 - f1_score: 0.7745 - binary_io_u: 0.8179\n","Epoch 10: val_binary_io_u improved from 0.80793 to 0.82004, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1109s 6s/step - loss: 0.0463 - accuracy: 0.9844 - f1_score: 0.7745 - binary_io_u: 0.8179 - val_loss: 0.0453 - val_accuracy: 0.9845 - val_f1_score: 0.7443 - val_binary_io_u: 0.8200 - lr: 0.0010\n","Epoch 11/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9844 - f1_score: 0.7763 - binary_io_u: 0.8187\n","Epoch 11: val_binary_io_u did not improve from 0.82004\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0456 - accuracy: 0.9844 - f1_score: 0.7763 - binary_io_u: 0.8187 - val_loss: 0.0693 - val_accuracy: 0.9773 - val_f1_score: 0.6488 - val_binary_io_u: 0.7448 - lr: 0.0010\n","Epoch 12/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9839 - f1_score: 0.7678 - binary_io_u: 0.8126\n","Epoch 12: val_binary_io_u did not improve from 0.82004\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0474 - accuracy: 0.9839 - f1_score: 0.7678 - binary_io_u: 0.8126 - val_loss: 0.0453 - val_accuracy: 0.9851 - val_f1_score: 0.7106 - val_binary_io_u: 0.8082 - lr: 0.0010\n","Epoch 13/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9847 - f1_score: 0.7786 - binary_io_u: 0.8206\n","Epoch 13: val_binary_io_u improved from 0.82004 to 0.82051, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0443 - accuracy: 0.9847 - f1_score: 0.7786 - binary_io_u: 0.8206 - val_loss: 0.0411 - val_accuracy: 0.9860 - val_f1_score: 0.7335 - val_binary_io_u: 0.8205 - lr: 0.0010\n","Epoch 14/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9851 - f1_score: 0.7892 - binary_io_u: 0.8260\n","Epoch 14: val_binary_io_u did not improve from 0.82051\n","200/200 [==============================] - 1107s 6s/step - loss: 0.0432 - accuracy: 0.9851 - f1_score: 0.7892 - binary_io_u: 0.8260 - val_loss: 0.0473 - val_accuracy: 0.9851 - val_f1_score: 0.7362 - val_binary_io_u: 0.8190 - lr: 0.0010\n","Epoch 15/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9855 - f1_score: 0.7911 - binary_io_u: 0.8299\n","Epoch 15: val_binary_io_u improved from 0.82051 to 0.82165, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0422 - accuracy: 0.9855 - f1_score: 0.7911 - binary_io_u: 0.8299 - val_loss: 0.0413 - val_accuracy: 0.9860 - val_f1_score: 0.7387 - val_binary_io_u: 0.8216 - lr: 0.0010\n","Epoch 16/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9858 - f1_score: 0.7955 - binary_io_u: 0.8321\n","Epoch 16: val_binary_io_u did not improve from 0.82165\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0415 - accuracy: 0.9858 - f1_score: 0.7955 - binary_io_u: 0.8321 - val_loss: 0.0408 - val_accuracy: 0.9859 - val_f1_score: 0.7250 - val_binary_io_u: 0.8173 - lr: 0.0010\n","Epoch 17/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9855 - f1_score: 0.7913 - binary_io_u: 0.8292\n","Epoch 17: val_binary_io_u improved from 0.82165 to 0.82989, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0423 - accuracy: 0.9855 - f1_score: 0.7913 - binary_io_u: 0.8292 - val_loss: 0.0396 - val_accuracy: 0.9863 - val_f1_score: 0.7528 - val_binary_io_u: 0.8299 - lr: 0.0010\n","Epoch 18/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9863 - f1_score: 0.8012 - binary_io_u: 0.8386\n","Epoch 18: val_binary_io_u improved from 0.82989 to 0.83442, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1110s 6s/step - loss: 0.0399 - accuracy: 0.9863 - f1_score: 0.8012 - binary_io_u: 0.8386 - val_loss: 0.0417 - val_accuracy: 0.9856 - val_f1_score: 0.7664 - val_binary_io_u: 0.8344 - lr: 0.0010\n","Epoch 19/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9859 - f1_score: 0.7963 - binary_io_u: 0.8339\n","Epoch 19: val_binary_io_u did not improve from 0.83442\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0410 - accuracy: 0.9859 - f1_score: 0.7963 - binary_io_u: 0.8339 - val_loss: 0.0487 - val_accuracy: 0.9831 - val_f1_score: 0.6708 - val_binary_io_u: 0.7757 - lr: 0.0010\n","Epoch 20/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9862 - f1_score: 0.7997 - binary_io_u: 0.8367\n","Epoch 20: val_binary_io_u improved from 0.83442 to 0.83450, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0400 - accuracy: 0.9862 - f1_score: 0.7997 - binary_io_u: 0.8367 - val_loss: 0.0399 - val_accuracy: 0.9861 - val_f1_score: 0.7589 - val_binary_io_u: 0.8345 - lr: 0.0010\n","Epoch 21/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9864 - f1_score: 0.8059 - binary_io_u: 0.8395\n","Epoch 21: val_binary_io_u did not improve from 0.83450\n","200/200 [==============================] - 1107s 6s/step - loss: 0.0393 - accuracy: 0.9864 - f1_score: 0.8059 - binary_io_u: 0.8395 - val_loss: 0.0390 - val_accuracy: 0.9865 - val_f1_score: 0.7494 - val_binary_io_u: 0.8338 - lr: 0.0010\n","Epoch 22/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9866 - f1_score: 0.8071 - binary_io_u: 0.8417\n","Epoch 22: val_binary_io_u did not improve from 0.83450\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0388 - accuracy: 0.9866 - f1_score: 0.8071 - binary_io_u: 0.8417 - val_loss: 0.0434 - val_accuracy: 0.9851 - val_f1_score: 0.7509 - val_binary_io_u: 0.8276 - lr: 0.0010\n","Epoch 23/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9868 - f1_score: 0.8095 - binary_io_u: 0.8439\n","Epoch 23: val_binary_io_u did not improve from 0.83450\n","200/200 [==============================] - 1107s 6s/step - loss: 0.0377 - accuracy: 0.9868 - f1_score: 0.8095 - binary_io_u: 0.8439 - val_loss: 0.0444 - val_accuracy: 0.9855 - val_f1_score: 0.7525 - val_binary_io_u: 0.8264 - lr: 0.0010\n","Epoch 24/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9868 - f1_score: 0.8088 - binary_io_u: 0.8437\n","Epoch 24: val_binary_io_u improved from 0.83450 to 0.84142, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1107s 6s/step - loss: 0.0379 - accuracy: 0.9868 - f1_score: 0.8088 - binary_io_u: 0.8437 - val_loss: 0.0372 - val_accuracy: 0.9873 - val_f1_score: 0.7674 - val_binary_io_u: 0.8414 - lr: 0.0010\n","Epoch 25/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9874 - f1_score: 0.8185 - binary_io_u: 0.8495\n","Epoch 25: val_binary_io_u did not improve from 0.84142\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0364 - accuracy: 0.9874 - f1_score: 0.8185 - binary_io_u: 0.8495 - val_loss: 0.0378 - val_accuracy: 0.9872 - val_f1_score: 0.7492 - val_binary_io_u: 0.8358 - lr: 0.0010\n","Epoch 26/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9870 - f1_score: 0.8136 - binary_io_u: 0.8460\n","Epoch 26: val_binary_io_u did not improve from 0.84142\n","200/200 [==============================] - 1105s 6s/step - loss: 0.0370 - accuracy: 0.9870 - f1_score: 0.8136 - binary_io_u: 0.8460 - val_loss: 0.0423 - val_accuracy: 0.9861 - val_f1_score: 0.7672 - val_binary_io_u: 0.8365 - lr: 0.0010\n","Epoch 27/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9876 - f1_score: 0.8203 - binary_io_u: 0.8519\n","Epoch 27: val_binary_io_u improved from 0.84142 to 0.84304, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0356 - accuracy: 0.9876 - f1_score: 0.8203 - binary_io_u: 0.8519 - val_loss: 0.0379 - val_accuracy: 0.9867 - val_f1_score: 0.7722 - val_binary_io_u: 0.8430 - lr: 0.0010\n","Epoch 28/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9875 - f1_score: 0.8167 - binary_io_u: 0.8509\n","Epoch 28: val_binary_io_u did not improve from 0.84304\n","200/200 [==============================] - 1107s 6s/step - loss: 0.0356 - accuracy: 0.9875 - f1_score: 0.8167 - binary_io_u: 0.8509 - val_loss: 0.0447 - val_accuracy: 0.9864 - val_f1_score: 0.7253 - val_binary_io_u: 0.8200 - lr: 0.0010\n","Epoch 29/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9875 - f1_score: 0.8195 - binary_io_u: 0.8504\n","Epoch 29: val_binary_io_u did not improve from 0.84304\n","\n","Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","200/200 [==============================] - 1105s 6s/step - loss: 0.0356 - accuracy: 0.9875 - f1_score: 0.8195 - binary_io_u: 0.8504 - val_loss: 0.0388 - val_accuracy: 0.9863 - val_f1_score: 0.7658 - val_binary_io_u: 0.8379 - lr: 0.0010\n","Epoch 30/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9880 - f1_score: 0.8281 - binary_io_u: 0.8569\n","Epoch 30: val_binary_io_u improved from 0.84304 to 0.84539, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1107s 6s/step - loss: 0.0343 - accuracy: 0.9880 - f1_score: 0.8281 - binary_io_u: 0.8569 - val_loss: 0.0343 - val_accuracy: 0.9881 - val_f1_score: 0.7665 - val_binary_io_u: 0.8454 - lr: 5.0000e-04\n","Epoch 31/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9889 - f1_score: 0.8407 - binary_io_u: 0.8660\n","Epoch 31: val_binary_io_u improved from 0.84539 to 0.85953, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1109s 6s/step - loss: 0.0314 - accuracy: 0.9889 - f1_score: 0.8407 - binary_io_u: 0.8660 - val_loss: 0.0319 - val_accuracy: 0.9886 - val_f1_score: 0.7958 - val_binary_io_u: 0.8595 - lr: 5.0000e-04\n","Epoch 32/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9890 - f1_score: 0.8412 - binary_io_u: 0.8671\n","Epoch 32: val_binary_io_u did not improve from 0.85953\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0307 - accuracy: 0.9890 - f1_score: 0.8412 - binary_io_u: 0.8671 - val_loss: 0.0315 - val_accuracy: 0.9888 - val_f1_score: 0.7933 - val_binary_io_u: 0.8578 - lr: 5.0000e-04\n","Epoch 33/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9889 - f1_score: 0.8421 - binary_io_u: 0.8669\n","Epoch 33: val_binary_io_u improved from 0.85953 to 0.86174, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1109s 6s/step - loss: 0.0309 - accuracy: 0.9889 - f1_score: 0.8421 - binary_io_u: 0.8669 - val_loss: 0.0309 - val_accuracy: 0.9889 - val_f1_score: 0.7992 - val_binary_io_u: 0.8617 - lr: 5.0000e-04\n","Epoch 34/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9891 - f1_score: 0.8442 - binary_io_u: 0.8682\n","Epoch 34: val_binary_io_u did not improve from 0.86174\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0304 - accuracy: 0.9891 - f1_score: 0.8442 - binary_io_u: 0.8682 - val_loss: 0.0322 - val_accuracy: 0.9887 - val_f1_score: 0.7850 - val_binary_io_u: 0.8558 - lr: 5.0000e-04\n","Epoch 35/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9893 - f1_score: 0.8444 - binary_io_u: 0.8710\n","Epoch 35: val_binary_io_u did not improve from 0.86174\n","200/200 [==============================] - 1112s 6s/step - loss: 0.0296 - accuracy: 0.9893 - f1_score: 0.8444 - binary_io_u: 0.8710 - val_loss: 0.0317 - val_accuracy: 0.9888 - val_f1_score: 0.7787 - val_binary_io_u: 0.8558 - lr: 5.0000e-04\n","Epoch 36/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9895 - f1_score: 0.8485 - binary_io_u: 0.8727\n","Epoch 36: val_binary_io_u improved from 0.86174 to 0.86198, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0292 - accuracy: 0.9895 - f1_score: 0.8485 - binary_io_u: 0.8727 - val_loss: 0.0309 - val_accuracy: 0.9890 - val_f1_score: 0.8008 - val_binary_io_u: 0.8620 - lr: 5.0000e-04\n","Epoch 37/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9894 - f1_score: 0.8461 - binary_io_u: 0.8726\n","Epoch 37: val_binary_io_u improved from 0.86198 to 0.86364, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1109s 6s/step - loss: 0.0291 - accuracy: 0.9894 - f1_score: 0.8461 - binary_io_u: 0.8726 - val_loss: 0.0307 - val_accuracy: 0.9891 - val_f1_score: 0.8084 - val_binary_io_u: 0.8636 - lr: 5.0000e-04\n","Epoch 38/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9896 - f1_score: 0.8502 - binary_io_u: 0.8739\n","Epoch 38: val_binary_io_u did not improve from 0.86364\n","200/200 [==============================] - 1107s 6s/step - loss: 0.0288 - accuracy: 0.9896 - f1_score: 0.8502 - binary_io_u: 0.8739 - val_loss: 0.0331 - val_accuracy: 0.9887 - val_f1_score: 0.7759 - val_binary_io_u: 0.8544 - lr: 5.0000e-04\n","Epoch 39/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9898 - f1_score: 0.8534 - binary_io_u: 0.8767\n","Epoch 39: val_binary_io_u did not improve from 0.86364\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0280 - accuracy: 0.9898 - f1_score: 0.8534 - binary_io_u: 0.8767 - val_loss: 0.0301 - val_accuracy: 0.9892 - val_f1_score: 0.8019 - val_binary_io_u: 0.8620 - lr: 5.0000e-04\n","Epoch 40/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9896 - f1_score: 0.8499 - binary_io_u: 0.8748\n","Epoch 40: val_binary_io_u improved from 0.86364 to 0.86467, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0282 - accuracy: 0.9896 - f1_score: 0.8499 - binary_io_u: 0.8748 - val_loss: 0.0293 - val_accuracy: 0.9894 - val_f1_score: 0.8034 - val_binary_io_u: 0.8647 - lr: 5.0000e-04\n","Epoch 41/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9898 - f1_score: 0.8535 - binary_io_u: 0.8769\n","Epoch 41: val_binary_io_u did not improve from 0.86467\n","200/200 [==============================] - 1107s 6s/step - loss: 0.0280 - accuracy: 0.9898 - f1_score: 0.8535 - binary_io_u: 0.8769 - val_loss: 0.0317 - val_accuracy: 0.9890 - val_f1_score: 0.7841 - val_binary_io_u: 0.8564 - lr: 5.0000e-04\n","Epoch 42/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9900 - f1_score: 0.8572 - binary_io_u: 0.8785\n","Epoch 42: val_binary_io_u did not improve from 0.86467\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0275 - accuracy: 0.9900 - f1_score: 0.8572 - binary_io_u: 0.8785 - val_loss: 0.0325 - val_accuracy: 0.9889 - val_f1_score: 0.8017 - val_binary_io_u: 0.8588 - lr: 5.0000e-04\n","Epoch 43/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9903 - f1_score: 0.8606 - binary_io_u: 0.8825\n","Epoch 43: val_binary_io_u did not improve from 0.86467\n","200/200 [==============================] - 1105s 6s/step - loss: 0.0263 - accuracy: 0.9903 - f1_score: 0.8606 - binary_io_u: 0.8825 - val_loss: 0.0292 - val_accuracy: 0.9895 - val_f1_score: 0.7947 - val_binary_io_u: 0.8638 - lr: 5.0000e-04\n","Epoch 44/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9904 - f1_score: 0.8622 - binary_io_u: 0.8835\n","Epoch 44: val_binary_io_u did not improve from 0.86467\n","200/200 [==============================] - 1105s 6s/step - loss: 0.0261 - accuracy: 0.9904 - f1_score: 0.8622 - binary_io_u: 0.8835 - val_loss: 0.0296 - val_accuracy: 0.9894 - val_f1_score: 0.8052 - val_binary_io_u: 0.8630 - lr: 5.0000e-04\n","Epoch 45/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9904 - f1_score: 0.8603 - binary_io_u: 0.8830\n","Epoch 45: val_binary_io_u improved from 0.86467 to 0.86731, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0259 - accuracy: 0.9904 - f1_score: 0.8603 - binary_io_u: 0.8830 - val_loss: 0.0290 - val_accuracy: 0.9897 - val_f1_score: 0.8063 - val_binary_io_u: 0.8673 - lr: 5.0000e-04\n","Epoch 46/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9905 - f1_score: 0.8637 - binary_io_u: 0.8847\n","Epoch 46: val_binary_io_u improved from 0.86731 to 0.86813, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1107s 6s/step - loss: 0.0255 - accuracy: 0.9905 - f1_score: 0.8637 - binary_io_u: 0.8847 - val_loss: 0.0290 - val_accuracy: 0.9896 - val_f1_score: 0.8121 - val_binary_io_u: 0.8681 - lr: 5.0000e-04\n","Epoch 47/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9905 - f1_score: 0.8651 - binary_io_u: 0.8849\n","Epoch 47: val_binary_io_u did not improve from 0.86813\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0255 - accuracy: 0.9905 - f1_score: 0.8651 - binary_io_u: 0.8849 - val_loss: 0.0295 - val_accuracy: 0.9895 - val_f1_score: 0.8086 - val_binary_io_u: 0.8678 - lr: 5.0000e-04\n","Epoch 48/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9906 - f1_score: 0.8629 - binary_io_u: 0.8858\n","Epoch 48: val_binary_io_u did not improve from 0.86813\n","200/200 [==============================] - 1105s 6s/step - loss: 0.0251 - accuracy: 0.9906 - f1_score: 0.8629 - binary_io_u: 0.8858 - val_loss: 0.0327 - val_accuracy: 0.9888 - val_f1_score: 0.8068 - val_binary_io_u: 0.8617 - lr: 5.0000e-04\n","Epoch 49/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9909 - f1_score: 0.8693 - binary_io_u: 0.8888\n","Epoch 49: val_binary_io_u did not improve from 0.86813\n","200/200 [==============================] - 1105s 6s/step - loss: 0.0242 - accuracy: 0.9909 - f1_score: 0.8693 - binary_io_u: 0.8888 - val_loss: 0.0287 - val_accuracy: 0.9897 - val_f1_score: 0.8152 - val_binary_io_u: 0.8672 - lr: 5.0000e-04\n","Epoch 50/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9910 - f1_score: 0.8746 - binary_io_u: 0.8910\n","Epoch 50: val_binary_io_u did not improve from 0.86813\n","200/200 [==============================] - 1105s 6s/step - loss: 0.0238 - accuracy: 0.9910 - f1_score: 0.8746 - binary_io_u: 0.8910 - val_loss: 0.0320 - val_accuracy: 0.9894 - val_f1_score: 0.7923 - val_binary_io_u: 0.8615 - lr: 5.0000e-04\n","Epoch 51/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9910 - f1_score: 0.8723 - binary_io_u: 0.8909\n","Epoch 51: val_binary_io_u improved from 0.86813 to 0.87062, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0236 - accuracy: 0.9910 - f1_score: 0.8723 - binary_io_u: 0.8909 - val_loss: 0.0291 - val_accuracy: 0.9897 - val_f1_score: 0.8154 - val_binary_io_u: 0.8706 - lr: 5.0000e-04\n","Epoch 52/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9911 - f1_score: 0.8759 - binary_io_u: 0.8921\n","Epoch 52: val_binary_io_u improved from 0.87062 to 0.87249, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1107s 6s/step - loss: 0.0235 - accuracy: 0.9911 - f1_score: 0.8759 - binary_io_u: 0.8921 - val_loss: 0.0277 - val_accuracy: 0.9900 - val_f1_score: 0.8219 - val_binary_io_u: 0.8725 - lr: 5.0000e-04\n","Epoch 53/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9912 - f1_score: 0.8735 - binary_io_u: 0.8926\n","Epoch 53: val_binary_io_u improved from 0.87249 to 0.87599, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0232 - accuracy: 0.9912 - f1_score: 0.8735 - binary_io_u: 0.8926 - val_loss: 0.0260 - val_accuracy: 0.9904 - val_f1_score: 0.8251 - val_binary_io_u: 0.8760 - lr: 5.0000e-04\n","Epoch 54/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9914 - f1_score: 0.8773 - binary_io_u: 0.8949\n","Epoch 54: val_binary_io_u did not improve from 0.87599\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0225 - accuracy: 0.9914 - f1_score: 0.8773 - binary_io_u: 0.8949 - val_loss: 0.0282 - val_accuracy: 0.9901 - val_f1_score: 0.8201 - val_binary_io_u: 0.8735 - lr: 5.0000e-04\n","Epoch 55/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9914 - f1_score: 0.8782 - binary_io_u: 0.8950\n","Epoch 55: val_binary_io_u improved from 0.87599 to 0.87683, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0226 - accuracy: 0.9914 - f1_score: 0.8782 - binary_io_u: 0.8950 - val_loss: 0.0261 - val_accuracy: 0.9904 - val_f1_score: 0.8270 - val_binary_io_u: 0.8768 - lr: 5.0000e-04\n","Epoch 56/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9915 - f1_score: 0.8794 - binary_io_u: 0.8963\n","Epoch 56: val_binary_io_u did not improve from 0.87683\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0221 - accuracy: 0.9915 - f1_score: 0.8794 - binary_io_u: 0.8963 - val_loss: 0.0269 - val_accuracy: 0.9902 - val_f1_score: 0.8278 - val_binary_io_u: 0.8764 - lr: 5.0000e-04\n","Epoch 57/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9916 - f1_score: 0.8786 - binary_io_u: 0.8980\n","Epoch 57: val_binary_io_u improved from 0.87683 to 0.87684, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0218 - accuracy: 0.9916 - f1_score: 0.8786 - binary_io_u: 0.8980 - val_loss: 0.0279 - val_accuracy: 0.9901 - val_f1_score: 0.8309 - val_binary_io_u: 0.8768 - lr: 5.0000e-04\n","Epoch 58/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9916 - f1_score: 0.8799 - binary_io_u: 0.8973\n","Epoch 58: val_binary_io_u did not improve from 0.87684\n","\n","Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0218 - accuracy: 0.9916 - f1_score: 0.8799 - binary_io_u: 0.8973 - val_loss: 0.0269 - val_accuracy: 0.9900 - val_f1_score: 0.8312 - val_binary_io_u: 0.8761 - lr: 5.0000e-04\n","Epoch 59/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9921 - f1_score: 0.8895 - binary_io_u: 0.9037\n","Epoch 59: val_binary_io_u improved from 0.87684 to 0.88395, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0201 - accuracy: 0.9921 - f1_score: 0.8895 - binary_io_u: 0.9037 - val_loss: 0.0246 - val_accuracy: 0.9909 - val_f1_score: 0.8401 - val_binary_io_u: 0.8840 - lr: 2.5000e-04\n","Epoch 60/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9925 - f1_score: 0.8968 - binary_io_u: 0.9078\n","Epoch 60: val_binary_io_u improved from 0.88395 to 0.88520, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1109s 6s/step - loss: 0.0191 - accuracy: 0.9925 - f1_score: 0.8968 - binary_io_u: 0.9078 - val_loss: 0.0244 - val_accuracy: 0.9910 - val_f1_score: 0.8426 - val_binary_io_u: 0.8852 - lr: 2.5000e-04\n","Epoch 61/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9924 - f1_score: 0.8958 - binary_io_u: 0.9075\n","Epoch 61: val_binary_io_u did not improve from 0.88520\n","200/200 [==============================] - 1107s 6s/step - loss: 0.0192 - accuracy: 0.9924 - f1_score: 0.8958 - binary_io_u: 0.9075 - val_loss: 0.0255 - val_accuracy: 0.9909 - val_f1_score: 0.8364 - val_binary_io_u: 0.8836 - lr: 2.5000e-04\n","Epoch 62/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9925 - f1_score: 0.8949 - binary_io_u: 0.9085\n","Epoch 62: val_binary_io_u did not improve from 0.88520\n","200/200 [==============================] - 1104s 6s/step - loss: 0.0189 - accuracy: 0.9925 - f1_score: 0.8949 - binary_io_u: 0.9085 - val_loss: 0.0254 - val_accuracy: 0.9908 - val_f1_score: 0.8363 - val_binary_io_u: 0.8827 - lr: 2.5000e-04\n","Epoch 63/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9926 - f1_score: 0.8987 - binary_io_u: 0.9097\n","Epoch 63: val_binary_io_u improved from 0.88520 to 0.88521, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0186 - accuracy: 0.9926 - f1_score: 0.8987 - binary_io_u: 0.9097 - val_loss: 0.0243 - val_accuracy: 0.9910 - val_f1_score: 0.8449 - val_binary_io_u: 0.8852 - lr: 2.5000e-04\n","Epoch 64/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9926 - f1_score: 0.8960 - binary_io_u: 0.9099\n","Epoch 64: val_binary_io_u did not improve from 0.88521\n","200/200 [==============================] - 1104s 6s/step - loss: 0.0186 - accuracy: 0.9926 - f1_score: 0.8960 - binary_io_u: 0.9099 - val_loss: 0.0247 - val_accuracy: 0.9909 - val_f1_score: 0.8410 - val_binary_io_u: 0.8843 - lr: 2.5000e-04\n","Epoch 65/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9927 - f1_score: 0.8985 - binary_io_u: 0.9109\n","Epoch 65: val_binary_io_u improved from 0.88521 to 0.88523, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0182 - accuracy: 0.9927 - f1_score: 0.8985 - binary_io_u: 0.9109 - val_loss: 0.0246 - val_accuracy: 0.9911 - val_f1_score: 0.8448 - val_binary_io_u: 0.8852 - lr: 2.5000e-04\n","Epoch 66/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9928 - f1_score: 0.8981 - binary_io_u: 0.9113\n","Epoch 66: val_binary_io_u did not improve from 0.88523\n","200/200 [==============================] - 1108s 6s/step - loss: 0.0182 - accuracy: 0.9928 - f1_score: 0.8981 - binary_io_u: 0.9113 - val_loss: 0.0256 - val_accuracy: 0.9909 - val_f1_score: 0.8359 - val_binary_io_u: 0.8816 - lr: 2.5000e-04\n","Epoch 67/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9929 - f1_score: 0.9009 - binary_io_u: 0.9126\n","Epoch 67: val_binary_io_u did not improve from 0.88523\n","200/200 [==============================] - 1103s 6s/step - loss: 0.0178 - accuracy: 0.9929 - f1_score: 0.9009 - binary_io_u: 0.9126 - val_loss: 0.0252 - val_accuracy: 0.9909 - val_f1_score: 0.8426 - val_binary_io_u: 0.8836 - lr: 2.5000e-04\n","Epoch 68/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9929 - f1_score: 0.8995 - binary_io_u: 0.9124\n","Epoch 68: val_binary_io_u did not improve from 0.88523\n","\n","Epoch 68: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","200/200 [==============================] - 1105s 6s/step - loss: 0.0179 - accuracy: 0.9929 - f1_score: 0.8995 - binary_io_u: 0.9124 - val_loss: 0.0251 - val_accuracy: 0.9909 - val_f1_score: 0.8433 - val_binary_io_u: 0.8850 - lr: 2.5000e-04\n","Epoch 69/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9931 - f1_score: 0.9049 - binary_io_u: 0.9156\n","Epoch 69: val_binary_io_u improved from 0.88523 to 0.88677, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1105s 6s/step - loss: 0.0171 - accuracy: 0.9931 - f1_score: 0.9049 - binary_io_u: 0.9156 - val_loss: 0.0246 - val_accuracy: 0.9911 - val_f1_score: 0.8462 - val_binary_io_u: 0.8868 - lr: 1.2500e-04\n","Epoch 70/70\n","200/200 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9932 - f1_score: 0.9039 - binary_io_u: 0.9168\n","Epoch 70: val_binary_io_u improved from 0.88677 to 0.88718, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Attention_UNet_2500_2nd_Run/saved_models/best_Attention_UNet_2500_2nd_Run.h5\n","200/200 [==============================] - 1106s 6s/step - loss: 0.0168 - accuracy: 0.9932 - f1_score: 0.9039 - binary_io_u: 0.9168 - val_loss: 0.0244 - val_accuracy: 0.9912 - val_f1_score: 0.8485 - val_binary_io_u: 0.8872 - lr: 1.2500e-04\n"]}],"source":["# training the model and saving the best model as a check point\n","best_model_name, train_history, total_time, best_model = fit_and_save_best_model(model_name, model)"]},{"cell_type":"code","execution_count":null,"id":"641b7d4d-76cc-494c-a44b-de0ae483a4f0","metadata":{"id":"641b7d4d-76cc-494c-a44b-de0ae483a4f0"},"outputs":[],"source":["# # Saving the time to be reused\n","# time_data = {\n","#     \"model_name\": [model_name],\n","#     \"run_time\": [total_time]\n","# }\n","\n","# # Convert the dictionary to a pandas DataFrame\n","# time_df = pd.DataFrame(time_data)\n","\n","# # Save the DataFrame to a CSV file\n","# time_df.to_csv(PATH_SAVE_TIME, index=False)"]},{"cell_type":"code","execution_count":null,"id":"a26d84ec-2973-4715-9bc2-7c456050d6f1","metadata":{"id":"a26d84ec-2973-4715-9bc2-7c456050d6f1"},"outputs":[],"source":["# # Saving the history as csv file to be reused\n","# history_df = pd.DataFrame(train_history.history)\n","# history_df.to_csv(PATH_SAVE_HISTORY, index=False)"]},{"cell_type":"code","execution_count":null,"id":"182eead8-6f8e-4534-9666-1a54c3d82c59","metadata":{"id":"182eead8-6f8e-4534-9666-1a54c3d82c59"},"outputs":[],"source":["# # plotting train history\n","# plot_training_history(train_history)"]},{"cell_type":"markdown","id":"58066d61-295d-4a1e-bd34-35fad6fddeb6","metadata":{"id":"58066d61-295d-4a1e-bd34-35fad6fddeb6"},"source":["## Loading the best model"]},{"cell_type":"code","execution_count":null,"id":"2efedf22-df76-4922-b09c-6357388b6458","metadata":{"id":"2efedf22-df76-4922-b09c-6357388b6458"},"outputs":[],"source":["# ##Now, load the best model\n","# best_model = ks.models.load_model(os.path.join(PATH_TO_SAVE_MODEL, best_model_name),\n","#                                   custom_objects={'f1_score': f1_score,\n","#                                                   'binary_io_u':iou_score_binary})\n","\n","# # best_model =model"]},{"cell_type":"markdown","id":"5b6e079b-b8bf-4be8-97d5-bcbc737b4f75","metadata":{"id":"5b6e079b-b8bf-4be8-97d5-bcbc737b4f75"},"source":["## Visualize model predictions"]},{"cell_type":"code","execution_count":null,"id":"0e84b3ee-95d2-46ac-aee4-77e8515e600a","metadata":{"id":"0e84b3ee-95d2-46ac-aee4-77e8515e600a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749732697874,"user_tz":-360,"elapsed":4120,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"}},"outputId":"66140ce5-720c-4584-8328-8aba5e778629"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 4s 4s/step\n"]}],"source":["# Get predictions from the model\n","predictions = best_model.predict(x_test, verbose=1)\n","\n","USER_DETERMINED_THRESHOLD = 0.5\n","thresholded_predictions  = (predictions  >= USER_DETERMINED_THRESHOLD)\n","# Removing the color channel\n","thresholded_predictions_without_color_channel = np.squeeze(thresholded_predictions, axis=-1)"]},{"cell_type":"markdown","source":["# AUC"],"metadata":{"id":"yPK1Ih3I_u5_"},"id":"yPK1Ih3I_u5_"},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\n","\n","# Flatten masks for AUC computation\n","y_true_flat = y_test.flatten()\n","y_score_flat = predictions.flatten()  # Predicted probabilities"],"metadata":{"id":"I8lQ0e-8_v02"},"id":"I8lQ0e-8_v02","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Bootstrapping AUC\n","n_iterations = 100\n","rng = np.random.default_rng(SEED)\n","bootstrap_aucs = []\n","\n","for _ in range(n_iterations):\n","    indices = rng.integers(0, len(y_true_flat), len(y_true_flat))\n","    y_true_sample = y_true_flat[indices]\n","    y_score_sample = y_score_flat[indices]\n","\n","    # Ensure both classes are present in the sample\n","    if len(np.unique(y_true_sample)) < 2:\n","        continue\n","\n","    auc = roc_auc_score(y_true_sample, y_score_sample)\n","    bootstrap_aucs.append(auc)"],"metadata":{"id":"cggKXQcy_v42"},"id":"cggKXQcy_v42","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate statistics\n","mean_auc = np.mean(bootstrap_aucs)\n","std_auc = np.std(bootstrap_aucs)\n","lower_ci = np.percentile(bootstrap_aucs, 2.5)\n","upper_ci = np.percentile(bootstrap_aucs, 97.5)\n","\n","# Print AUC statistics\n","print(f\"Bootstrapped AUC: Mean={mean_auc:.4f}, Std={std_auc:.4f}, 95% CI=({lower_ci:.4f}, {upper_ci:.4f})\")\n","\n","# Optional saving\n","if SAVE_RESULTS:\n","    auc_df = pd.DataFrame({'AUC Scores': bootstrap_aucs})\n","    auc_df.to_csv(os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_bootstrapped_auc.csv\"), index=False)"],"metadata":{"id":"yRi6YGLx_v_O"},"id":"yRi6YGLx_v_O","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate statistics\n","mean_auc = np.mean(bootstrap_aucs)\n","std_auc = np.std(bootstrap_aucs)\n","lower_ci = np.percentile(bootstrap_aucs, 2.5)\n","upper_ci = np.percentile(bootstrap_aucs, 97.5)\n","\n","# Print AUC statistics\n","print(f\"Bootstrapped AUC: Mean={mean_auc:.4f}, Std={std_auc:.4f}, 95% CI=({lower_ci:.4f}, {upper_ci:.4f})\")\n","\n","# Optional saving\n","if SAVE_RESULTS:\n","    auc_df = pd.DataFrame({'AUC Scores': bootstrap_aucs})\n","    auc_df.to_csv(os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_bootstrapped_auc.csv\"), index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnTVJJWuOA-b","executionInfo":{"status":"ok","timestamp":1749733088245,"user_tz":-360,"elapsed":10,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"}},"outputId":"b94143ce-3449-41eb-c193-f4f22ad5d57e"},"id":"wnTVJJWuOA-b","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bootstrapped AUC: Mean=0.9912, Std=0.0002, 95% CI=(0.9908, 0.9915)\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}