{"cells":[{"cell_type":"markdown","id":"c4f40068-3bd2-4464-8c10-1ae560f13570","metadata":{"id":"c4f40068-3bd2-4464-8c10-1ae560f13570"},"source":["## Importing necessary modules"]},{"cell_type":"code","execution_count":null,"id":"pcyiTh4DiVge","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3718,"status":"ok","timestamp":1749581839423,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"},"user_tz":-360},"id":"pcyiTh4DiVge","outputId":"6fa9ef90-801f-4967-bccc-1cb480bdc329"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip uninstall -y numpy pandas\n","!pip install numpy==1.26.4 pandas==2.2.2\n","\n","# imgaug (compatible with older NumPy)\n","!pip install -q imgaug==0.4.0\n","\n","# Downgrade NumPy to avoid issues with imgaug (NumPy ≥ 2.0 breaks imgaug)\n","!pip install -q numpy==1.26.4\n","\n","!pip install tensorflow==2.15.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_rTLOWltJpBT","executionInfo":{"status":"ok","timestamp":1749581886112,"user_tz":-360,"elapsed":46690,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"}},"outputId":"7088dbd4-aa40-4eb8-b404-aa275a759c8c"},"id":"_rTLOWltJpBT","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: numpy 1.26.4\n","Uninstalling numpy-1.26.4:\n","  Successfully uninstalled numpy-1.26.4\n","Found existing installation: pandas 2.2.2\n","Uninstalling pandas-2.2.2:\n","  Successfully uninstalled pandas-2.2.2\n","Collecting numpy==1.26.4\n","  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","Collecting pandas==2.2.2\n","  Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n","Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n","Installing collected packages: numpy, pandas\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n","tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n","tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.26.4 pandas-2.2.2\n","Collecting tensorflow==2.15.0\n","  Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.13.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (18.1.1)\n","Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n","  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (24.2)\n","Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n","  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.14.0)\n","Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n","  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.37.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.72.1)\n","Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n","  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n","Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n","  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n","  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.38.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2025.4.26)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n","Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, ml-dtypes, keras, tensorboard, tensorflow\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.17.2\n","    Uninstalling wrapt-1.17.2:\n","      Successfully uninstalled wrapt-1.17.2\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.5\n","    Uninstalling protobuf-5.29.5:\n","      Successfully uninstalled protobuf-5.29.5\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml_dtypes 0.5.1\n","    Uninstalling ml_dtypes-0.5.1:\n","      Successfully uninstalled ml_dtypes-0.5.1\n","  Attempting uninstall: keras\n","    Found existing installation: keras 3.8.0\n","    Uninstalling keras-3.8.0:\n","      Successfully uninstalled keras-3.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.19.0\n","    Uninstalling tensorboard-2.19.0:\n","      Successfully uninstalled tensorboard-2.19.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.19.0\n","    Uninstalling tensorflow-2.19.0:\n","      Successfully uninstalled tensorflow-2.19.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.0 which is incompatible.\n","jax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.2.0 which is incompatible.\n","tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n","ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n","tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n","grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n","tensorstore 0.1.74 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 protobuf-4.25.8 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"]}]},{"cell_type":"code","execution_count":null,"id":"fa64290b-2570-4f24-920a-07a72a3660b9","metadata":{"id":"fa64290b-2570-4f24-920a-07a72a3660b9"},"outputs":[],"source":["import time\n","from tqdm import tqdm # Cool progress bar\n","\n","import random\n","import numpy as np\n","import pandas as pd\n","import sys\n","import os # read and manipulate local files\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","import cv2\n","import seaborn as sns\n","\n","from PIL import Image\n","\n","import tensorflow.keras as ks\n","import tensorflow as tf\n","\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import backend as K # F1-score metric\n","\n","from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.layers import   Lambda, Conv2D, MaxPool2D, UpSampling2D, BatchNormalization, Flatten\n","from tensorflow.keras.layers import  GlobalAveragePooling2D, Reshape, Multiply, Attention, add,Resizing,  Input, Dense\n","from tensorflow.keras.layers import Activation,AveragePooling2D, MaxPooling2D, Dropout, Conv2DTranspose, Concatenate\n","from tensorflow.keras.models import Model, Sequential\n","\n","# hide wornings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# from keras import ops# For deeplab3\n","\n","# import keras_cv # basnet\n","\n","from imgaug import augmenters as iaa ## for augmentation"]},{"cell_type":"markdown","id":"bb6fe714-f8fb-4f9d-9657-ca3373fd9779","metadata":{"id":"bb6fe714-f8fb-4f9d-9657-ca3373fd9779"},"source":["## Defining paths"]},{"cell_type":"code","execution_count":null,"id":"051a2f6e-d46f-44ae-bfce-e2eb91954b5f","metadata":{"id":"051a2f6e-d46f-44ae-bfce-e2eb91954b5f"},"outputs":[],"source":["model_name = 'Basic_Unet_1500'\n","\n","################################################################################################\n","## SETTING THE PATHS\n","PATH_TO_CODE ='/content/drive/MyDrive/TRIAL_v1/segmentation_task'\n","# PATH_TO_CODE =''\n","sys.path.append(PATH_TO_CODE)\n","\n","# DATASET_PATH = r'C:\\Users\\ASUS\\Desktop\\segmentation & cropping\\segmentation_task\\segmentation_data43_resized_cropped_split'\n","DATASET_PATH = r'/content/drive/MyDrive/TRIAL_v1/segmentation_task/segmentation_data43_resized_cropped_split'\n","# DATASET_PATH = r'segmentation_data43_resized_cropped_split'\n","DATASET_PATH_TRAIN = os.path.join(DATASET_PATH, 'train')\n","DATASET_PATH_TEST = os.path.join(DATASET_PATH, 'test')\n","\n","\n","PATH_TO_SAVE_RESULT = os.path.join(PATH_TO_CODE, 'saved_outputs_segmentation_AUC', model_name)\n","PATH_BEST_SAVE_WEIGHT = os.path.join(PATH_TO_SAVE_RESULT,'saved_weights')\n","PATH_TO_SAVE_MODEL = os.path.join(PATH_TO_SAVE_RESULT, 'saved_models')\n","PATH_SAVE_HISTORY = os.path.join(PATH_TO_SAVE_RESULT, model_name+'_training_history.csv')\n","PATH_SAVE_TIME = os.path.join(PATH_TO_SAVE_RESULT, model_name+'_training_time.csv')\n","PATH_SAVE_AUGMENT_SAMPLE = os.path.join(PATH_TO_SAVE_RESULT, model_name+'_augmented_sample.pdf')\n","PATH_SAVE_PIXEL_PERCENTAGE_PLOT = os.path.join(PATH_TO_SAVE_RESULT, model_name+'_pixel_percentage.pdf')\n","# PATH_TO_SAVE_TUNER = os.path.join(PATH_TO_RESULT, 'saved_tuner_model')\n","################################################################################################\n","\n","if not os.path.exists(PATH_TO_SAVE_RESULT):\n"," # If it does not exist, create it\n","    os.makedirs(PATH_TO_SAVE_RESULT)\n","################################################################################################\n","IMG_HEIGHT = 256\n","IMG_WIDTH = 256\n","ORIGINAL_IMAGE_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n","COLOR_CHANNEL = 3\n","\n","RESIZE_SHAPE = ORIGINAL_IMAGE_SIZE #(128, 128) #ORIGINAL_IMAGE_SIZE#(128, 128)# # #ORIGINAL_IMAGE_SIZE# (128, 128)#\n","MODEL_INPUT_SIZE = (RESIZE_SHAPE[0], RESIZE_SHAPE[1], COLOR_CHANNEL)\n","\n","VALIDATION_SPLIT= 0.2\n","NUM_CLASSES = 2 # Disease and not disease\n","\n","BATCH_SIZE = 10\n","EPOCHS = 60\n","\n","TOTAL_DATA = 1500\n","################################################################################################\n","\n","################################################################################################\n","\n","SAVE_RESULTS = True\n","SHOW_RESULTS = True\n","\n","################################################################################################\n","# Setting the seed\n","SEED  = 123\n","RNG = np.random.default_rng(SEED) # Random number generator\n","tf.random.set_seed(SEED)\n","\n","################################################################################################\n","# Checkpoint parameters val_binary_io_u\n","SCORE_TO_MONITOR = 'val_binary_io_u' # Score that checkpoints monitor during training\n","SCORE_OBJECTIVE  = 'max'          # 'max' or 'min', specifies whether the objective is to maximize the score or minimize it.\n","PATIENCE_LR_REDUCE = 5\n","MIN_LR = 1e-8\n","REDUCTION_FACTOR = 0.5            # Factor which lr will be reduced with at plateau\n","COOLDOWN_EPOCHS  = 2 #cooldown: Integer. Number of epochs to wait before resuming normal operation after the learning rate has been reduced.\n","\n","INITIAL_LR = 0.001"]},{"cell_type":"markdown","id":"2b874d87-e287-4dda-a0d6-79ff516adc3f","metadata":{"id":"2b874d87-e287-4dda-a0d6-79ff516adc3f"},"source":["## Defining performance metrics\n","\n","Official segmentation metrics by keras: https://ks.io/api/metrics/segmentation_metrics/"]},{"cell_type":"code","execution_count":null,"id":"b8197196-670d-4e41-af29-452a87af68ba","metadata":{"id":"b8197196-670d-4e41-af29-452a87af68ba"},"outputs":[],"source":["def f1_score(y_true, y_pred): # Dice coefficient\n","    \"\"\"\n","    Calculate the F1 score, the harmonic mean of precision and recall, for binary classification.\n","\n","    Args:\n","        y_true (Tensor): True binary labels.\n","        y_pred (Tensor): Predicted probabilities.\n","\n","    Returns:\n","        float32: F1 score as a scalar.\n","    \"\"\"\n","    # True Positives: round product of y_true and y_pred\n","    TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    # Actual Positives: round y_true\n","    P = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    # Recall: TP / Actual Positives\n","    recall = TP / (P + K.epsilon())\n","\n","    # Predicted Positives: round y_pred\n","    Pred_P = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    # Precision: TP / Predicted Positives\n","    precision = TP / (Pred_P + K.epsilon())\n","\n","    # F1 Score: harmonic mean of precision and recall\n","    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n","\n","# source: https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryIoU\n","iou_score_binary = tf.keras.metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.5)\n","\n","def accuracy_score(y_true, y_pred):\n","    \"\"\"\n","    Calculate accuracy score between two binary masks.\n","    \"\"\"\n","    correct = np.sum(y_true == y_pred)  # Count correct predictions\n","    total = y_true.size  # Total number of pixels\n","    return correct / total  # Accuracy calculation\n","\n","def precision_score(groundtruth_mask, pred_mask):\n","    \"\"\"\n","    Calculate precision score between two binary masks.\n","    \"\"\"\n","    intersect = np.sum(pred_mask * groundtruth_mask)  # Calculate intersection\n","    total_pixel_pred = np.sum(pred_mask)  # Sum of predicted positives\n","    return np.mean(intersect / total_pixel_pred)  # Precision calculation\n","\n","def recall_score(groundtruth_mask, pred_mask):\n","    \"\"\"\n","    Calculate recall score between two binary masks.\n","    \"\"\"\n","    intersect = np.sum(pred_mask * groundtruth_mask)  # Calculate intersection\n","    total_pixel_truth = np.sum(groundtruth_mask)  # Sum of actual positives\n","    return np.mean(intersect / total_pixel_truth)  # Recall calculation"]},{"cell_type":"markdown","id":"ef00a1c4-00ef-4dc7-926d-638a5146c2c4","metadata":{"id":"ef00a1c4-00ef-4dc7-926d-638a5146c2c4"},"source":["## Reading the data"]},{"cell_type":"code","execution_count":null,"id":"30e64959-9aaf-4396-8ca0-0378ea018ef6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66152,"status":"ok","timestamp":1749581960189,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"},"user_tz":-360},"id":"30e64959-9aaf-4396-8ca0-0378ea018ef6","outputId":"e6693f73-270d-48d8-9ff9-12d18df2aa6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 616 original images into x_train\n","Loaded 616 mask images into y_train\n","Loaded 23 original images into x_test\n","Loaded 23 mask images into y_test\n"]}],"source":["# Function to read images from a directory using Pillow\n","def read_images_from_directory(directory_path):\n","    images = []\n","    for filename in sorted(os.listdir(directory_path)):\n","        # filename= filename.lower()\n","\n","        if filename.endswith(('.png', '.jpg', '.JPG' )):  # Check for image file extensions\n","            img_path = os.path.join(directory_path, filename)\n","            img = Image.open(img_path)\n","            img_array = np.array(img)  # Convert the image to a numpy array if needed\n","            if img_array is not None:\n","                images.append(img_array)\n","    return images\n","\n","# Paths to masks and original images within the dataset\n","masks_path_train = os.path.join(DATASET_PATH_TRAIN, 'data43_masks_binarised')\n","originals_path_train = os.path.join(DATASET_PATH_TRAIN, 'data43_original')\n","\n","# Read images into variables\n","x_train = read_images_from_directory(originals_path_train)  # Original images\n","y_train = read_images_from_directory(masks_path_train)  # Masks\n","\n","# Paths to masks and original images within the test dataset\n","masks_path_test = os.path.join(DATASET_PATH_TEST, 'data43_masks_binarised')\n","originals_path_test = os.path.join(DATASET_PATH_TEST, 'data43_original')\n","\n","# Read images into variables\n","x_test = read_images_from_directory(originals_path_test)  # Original images\n","y_test = read_images_from_directory(masks_path_test)  # Masks\n","\n","# Now x_train contains original images, and y_train contains mask images\n","print(f\"Loaded {len(x_train)} original images into x_train\")\n","print(f\"Loaded {len(y_train)} mask images into y_train\")\n","\n","# Similarly, for test images\n","print(f\"Loaded {len(x_test)} original images into x_test\")\n","print(f\"Loaded {len(y_test)} mask images into y_test\")"]},{"cell_type":"markdown","id":"6833c433-4baf-4162-ad11-38b8c933e7c0","metadata":{"id":"6833c433-4baf-4162-ad11-38b8c933e7c0"},"source":["## Augmentations"]},{"cell_type":"code","execution_count":null,"id":"37179298-69f2-4faf-9b69-2d91f63d710c","metadata":{"id":"37179298-69f2-4faf-9b69-2d91f63d710c"},"outputs":[],"source":["seq = iaa.Sequential([\n","    iaa.Fliplr(0.5),  # horizontally flip 50% of the images\n","    iaa.Flipud(0.2),  # vertically flip 20% of the images\n","    iaa.Affine(\n","        scale={\"x\": (0.8, 1.1), \"y\": (0.8, 1.1)},  # zoom in or out (80-120%)\n","        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},  # width & height shift (-20 to +20%)\n","        rotate=(-15, 15),  # rotation (-45 to 45 degrees)\n","        shear=(-16, 16)  # shear (-16 to 16 degrees)\n","    ),\n","    iaa.ElasticTransformation(alpha=(0, 2.0), sigma=0.25),  # apply elastic deformations\n","    iaa.GaussianBlur(sigma=(0, 2.0))  # apply Gaussian blur\n","])"]},{"cell_type":"code","execution_count":null,"id":"d401748b-eeef-4842-a449-b4c0b0b8e985","metadata":{"id":"d401748b-eeef-4842-a449-b4c0b0b8e985"},"outputs":[],"source":["# Function to augment a batch of images and masks\n","def augment_batch(images, masks, seq, batch_size):\n","    augmented_images = []\n","    augmented_masks = []\n","    while len(augmented_images) < batch_size:\n","        aug_images, aug_masks = seq(images=images, segmentation_maps=np.expand_dims(masks, axis=-1))\n","        augmented_images.extend(aug_images)\n","        augmented_masks.extend(aug_masks)\n","\n","        if len(augmented_images) >= batch_size:\n","            break\n","\n","    # Ensure we only take as many as we need to reach the desired batch_size\n","    augmented_images = augmented_images[:batch_size]\n","    augmented_masks = augmented_masks[:batch_size]\n","\n","    return np.array(augmented_images), np.squeeze(np.array(augmented_masks), axis=-1)"]},{"cell_type":"code","execution_count":null,"id":"e4233d82-f9ec-4f00-953f-10854ca1541c","metadata":{"id":"e4233d82-f9ec-4f00-953f-10854ca1541c"},"outputs":[],"source":["num_augmented_images_needed = TOTAL_DATA  - len(x_train)  # Calculate how many augmented images we need\n","if num_augmented_images_needed > 0:\n","    augmented_x, augmented_y = augment_batch(x_train, y_train, seq, num_augmented_images_needed)\n","    # Concatenate the original and augmented datasets\n","    combined_x_train = np.concatenate((x_train, augmented_x), axis=0)\n","    combined_y_train = np.concatenate((y_train, augmented_y), axis=0)\n","else:\n","    combined_x_train = x_train\n","    combined_y_train = y_train"]},{"cell_type":"code","execution_count":null,"id":"0c5523b2-b361-42ce-b60a-a01f69f5f4a1","metadata":{"id":"0c5523b2-b361-42ce-b60a-a01f69f5f4a1"},"outputs":[],"source":["x_train = combined_x_train\n","y_train = combined_y_train"]},{"cell_type":"markdown","id":"6b560e6d-8888-4f2a-8daf-e0b5340ac2f8","metadata":{"id":"6b560e6d-8888-4f2a-8daf-e0b5340ac2f8"},"source":["## Preprocessing DO NOT RESIZE THE IMAGE-- IT GIVES FINE LINES IN THE OUTPUT"]},{"cell_type":"code","execution_count":null,"id":"345f792d-94b1-4954-872e-461bd735c001","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":725,"status":"ok","timestamp":1749581968855,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"},"user_tz":-360},"id":"345f792d-94b1-4954-872e-461bd735c001","outputId":"d4582124-59f7-4fd2-e156-77396f3823ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 1500 original images into x_train\n","Loaded 1500 mask images into y_train\n","Loaded 23 original images into x_test\n","Loaded 23 mask images into y_test\n"]}],"source":["# # # Resizing\n","# x_train = [np.array(Image.fromarray(image).resize(RESIZE_SHAPE)) for image in x_train]\n","# y_train = [np.array(Image.fromarray(image).resize(RESIZE_SHAPE)) for image in y_train]\n","\n","# x_test = [np.array(Image.fromarray(image).resize(RESIZE_SHAPE)) for image in x_test]\n","# y_test = [np.array(Image.fromarray(image).resize(RESIZE_SHAPE)) for image in y_test]\n","\n","# Convert the list to a NumPy array\n","x_train = np.array(x_train)\n","y_train = np.array(y_train)\n","\n","x_test = np.array(x_test)\n","y_test = np.array(y_test)\n","\n","# Expand mask dimensions to (batch, 256, 256, 1)\n","y_train = np.expand_dims(y_train, axis=-1)\n","y_test = np.expand_dims(y_test, axis=-1)\n","\n","# Expand mask dimensions to (batch, 256, 256, 1)\n","y_train = np.expand_dims(y_train, axis=-1)\n","y_test = np.expand_dims(y_test, axis=-1)\n","\n","# Normalizing input between [0,1]\n","x_train = x_train.astype(\"float32\")/ np.max(x_train)\n","x_test  = x_test.astype(\"float32\")/np.max(x_test)\n","\n","y_train = y_train.astype(\"float32\")/ np.max(y_train)\n","y_test  = y_test.astype(\"float32\")/np.max(y_test)\n","\n","# Now x_train contains original images, and y_train contains mask images\n","print(f\"Loaded {len(x_train)} original images into x_train\")\n","print(f\"Loaded {len(y_train)} mask images into y_train\")\n","\n","# Similarly, for test images\n","print(f\"Loaded {len(x_test)} original images into x_test\")\n","print(f\"Loaded {len(y_test)} mask images into y_test\")"]},{"cell_type":"markdown","id":"3631b8c9-b763-4574-8444-2034cad11cee","metadata":{"id":"3631b8c9-b763-4574-8444-2034cad11cee"},"source":["## Essential functions"]},{"cell_type":"code","execution_count":null,"id":"73061e0d-ae45-45fb-9ba2-8099845466b3","metadata":{"id":"73061e0d-ae45-45fb-9ba2-8099845466b3"},"outputs":[],"source":["def fit_and_save_best_model(model_name, model, epochs=EPOCHS):\n","    ##########################################################################################################\n","    saved_best_model_name = 'best_'+model_name+'.h5'\n","\n","    model_saving_path = os.path.join(PATH_TO_SAVE_MODEL, saved_best_model_name)\n","    # Set up a model checkpoint to save the best model during training\n","    best_model_callback= ModelCheckpoint(model_saving_path,\n","                                          monitor=SCORE_TO_MONITOR,\n","                                          save_best_only=True,\n","                                          mode=SCORE_OBJECTIVE,\n","                                          verbose=1)\n","\n","    reduce_LR = ReduceLROnPlateau(\n","        factor=REDUCTION_FACTOR,      # Factor by which the learning rate will be reduced. new_lr = lr * factor\n","        patience=PATIENCE_LR_REDUCE, # original was 5      # Number of epochs with no improvement after which learning rate will be reduced.\n","        verbose=1,       # int. 0: quiet, 1: update messages.\n","        min_lr=MIN_LR   # Lower bound on the learning rate.\n","    )\n","\n","    my_callbacks = [best_model_callback , reduce_LR]\n","    ##########################################################################################################\n","\n","    start_time = time.time()\n","\n","    # Fitting the model\n","    train_history = model.fit(\n","        x_train,\n","        y_train,\n","        epochs= epochs,\n","        batch_size=BATCH_SIZE,\n","        validation_split= VALIDATION_SPLIT,\n","        callbacks=my_callbacks,\n","    )\n","    total_time = time.time() - start_time\n","\n","    return saved_best_model_name, train_history, total_time"]},{"cell_type":"markdown","id":"bc3dd825-59a8-43f1-9a3c-b720ebd250dc","metadata":{"id":"bc3dd825-59a8-43f1-9a3c-b720ebd250dc"},"source":["## Models\n","### Models worked\n","- Model01: Basic basic_unet\n","- Model02: unet_vgg16 (my own custom)\n","- Model03: TransUNet\n","- Model04: DeepLab-v3Plus\n","- Model05: ResUNet\n","- Model06: UNet_with_attention\n","- Model07: Multi-resUnet\n","- Model08: Inception_resnetV2 (my own custom)\n","\n"]},{"cell_type":"markdown","id":"8e96faa1-4889-4fcb-a5a7-4197c9e8dc5d","metadata":{"id":"8e96faa1-4889-4fcb-a5a7-4197c9e8dc5d"},"source":["### Basic basic_unet"]},{"cell_type":"code","execution_count":null,"id":"deef969b-9d01-4e95-81a3-e6807226c9ff","metadata":{"id":"deef969b-9d01-4e95-81a3-e6807226c9ff"},"outputs":[],"source":["def conv2d_block(input_tensor, n_filters, kernel_size = 3):\n","    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n","    # first layer\n","    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n","              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n","    x = Activation('relu')(x)\n","\n","    # second layer\n","    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n","              kernel_initializer = 'he_normal', padding = 'same')(x)\n","    x = Activation('relu')(x)\n","\n","    return x\n","\n","def get_basic_unet(n_filters = 16):\n","    \"\"\"\n","    Version of U-Net with dropout and size preservation (padding= 'same')--from class note\n","    \"\"\"\n","    input_img = Input(shape=MODEL_INPUT_SIZE)\n","    # Contracting Path\n","    c1 = conv2d_block(input_img, n_filters * 1, kernel_size=3)\n","    p1 = MaxPooling2D((2, 2))(c1)\n","\n","    c2 = conv2d_block(p1, n_filters * 2, kernel_size=3)\n","    p2 = MaxPooling2D((2, 2))(c2)\n","\n","    c3 = conv2d_block(p2, n_filters * 4, kernel_size=3)\n","    p3 = MaxPooling2D((2, 2))(c3)\n","\n","    c4 = conv2d_block(p3, n_filters * 8, kernel_size=3)\n","    p4 = MaxPooling2D((2, 2))(c4)\n","\n","    c5 = conv2d_block(p4, n_filters * 16, kernel_size=3)\n","\n","    # Expansive Path\n","    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides=(2, 2), padding='same')(c5)\n","    u6 = Concatenate()([u6, c4])\n","    c6 = conv2d_block(u6, n_filters * 8, kernel_size=3)\n","\n","    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides=(2, 2), padding='same')(c6)\n","    u7 = Concatenate()([u7, c3])\n","    c7 = conv2d_block(u7, n_filters * 4, kernel_size=3)\n","\n","    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides=(2, 2), padding='same')(c7)\n","    u8 = Concatenate()([u8, c2])\n","    c8 = conv2d_block(u8, n_filters * 2, kernel_size=3)\n","\n","    u9 = Conv2DTranspose(n_filters, (3, 3), strides=(2, 2), padding='same')(c8)\n","    u9 = Concatenate()([u9, c1])\n","    c9 = conv2d_block(u9, n_filters, kernel_size=3)\n","\n","    outputs = Conv2D(NUM_CLASSES-1, (1, 1), activation='sigmoid')(c9)\n","    model = Model(inputs=[input_img], outputs=[outputs])\n","\n","    optimizer = Adam(learning_rate=INITIAL_LR)\n","    model.compile(loss='binary_crossentropy',\n","                  metrics=['accuracy', f1_score, iou_score_binary],\n","                  optimizer=optimizer)\n","\n","    return model"]},{"cell_type":"markdown","id":"c8391326-d459-4214-ad6c-efd1d12ad54a","metadata":{"id":"c8391326-d459-4214-ad6c-efd1d12ad54a"},"source":["## Train and Results"]},{"cell_type":"code","execution_count":null,"id":"3d3e72d6-6137-46fe-9a41-418999643dbb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":446,"status":"ok","timestamp":1749581969398,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"},"user_tz":-360},"id":"3d3e72d6-6137-46fe-9a41-418999643dbb","outputId":"d5eb8df4-87a3-40e3-8aeb-acf734891bdc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n","                                                                                                  \n"," conv2d (Conv2D)             (None, 256, 256, 16)         448       ['input_1[0][0]']             \n","                                                                                                  \n"," activation (Activation)     (None, 256, 256, 16)         0         ['conv2d[0][0]']              \n","                                                                                                  \n"," conv2d_1 (Conv2D)           (None, 256, 256, 16)         2320      ['activation[0][0]']          \n","                                                                                                  \n"," activation_1 (Activation)   (None, 256, 256, 16)         0         ['conv2d_1[0][0]']            \n","                                                                                                  \n"," max_pooling2d (MaxPooling2  (None, 128, 128, 16)         0         ['activation_1[0][0]']        \n"," D)                                                                                               \n","                                                                                                  \n"," conv2d_2 (Conv2D)           (None, 128, 128, 32)         4640      ['max_pooling2d[0][0]']       \n","                                                                                                  \n"," activation_2 (Activation)   (None, 128, 128, 32)         0         ['conv2d_2[0][0]']            \n","                                                                                                  \n"," conv2d_3 (Conv2D)           (None, 128, 128, 32)         9248      ['activation_2[0][0]']        \n","                                                                                                  \n"," activation_3 (Activation)   (None, 128, 128, 32)         0         ['conv2d_3[0][0]']            \n","                                                                                                  \n"," max_pooling2d_1 (MaxPoolin  (None, 64, 64, 32)           0         ['activation_3[0][0]']        \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_4 (Conv2D)           (None, 64, 64, 64)           18496     ['max_pooling2d_1[0][0]']     \n","                                                                                                  \n"," activation_4 (Activation)   (None, 64, 64, 64)           0         ['conv2d_4[0][0]']            \n","                                                                                                  \n"," conv2d_5 (Conv2D)           (None, 64, 64, 64)           36928     ['activation_4[0][0]']        \n","                                                                                                  \n"," activation_5 (Activation)   (None, 64, 64, 64)           0         ['conv2d_5[0][0]']            \n","                                                                                                  \n"," max_pooling2d_2 (MaxPoolin  (None, 32, 32, 64)           0         ['activation_5[0][0]']        \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_6 (Conv2D)           (None, 32, 32, 128)          73856     ['max_pooling2d_2[0][0]']     \n","                                                                                                  \n"," activation_6 (Activation)   (None, 32, 32, 128)          0         ['conv2d_6[0][0]']            \n","                                                                                                  \n"," conv2d_7 (Conv2D)           (None, 32, 32, 128)          147584    ['activation_6[0][0]']        \n","                                                                                                  \n"," activation_7 (Activation)   (None, 32, 32, 128)          0         ['conv2d_7[0][0]']            \n","                                                                                                  \n"," max_pooling2d_3 (MaxPoolin  (None, 16, 16, 128)          0         ['activation_7[0][0]']        \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_8 (Conv2D)           (None, 16, 16, 256)          295168    ['max_pooling2d_3[0][0]']     \n","                                                                                                  \n"," activation_8 (Activation)   (None, 16, 16, 256)          0         ['conv2d_8[0][0]']            \n","                                                                                                  \n"," conv2d_9 (Conv2D)           (None, 16, 16, 256)          590080    ['activation_8[0][0]']        \n","                                                                                                  \n"," activation_9 (Activation)   (None, 16, 16, 256)          0         ['conv2d_9[0][0]']            \n","                                                                                                  \n"," conv2d_transpose (Conv2DTr  (None, 32, 32, 128)          295040    ['activation_9[0][0]']        \n"," anspose)                                                                                         \n","                                                                                                  \n"," concatenate (Concatenate)   (None, 32, 32, 256)          0         ['conv2d_transpose[0][0]',    \n","                                                                     'activation_7[0][0]']        \n","                                                                                                  \n"," conv2d_10 (Conv2D)          (None, 32, 32, 128)          295040    ['concatenate[0][0]']         \n","                                                                                                  \n"," activation_10 (Activation)  (None, 32, 32, 128)          0         ['conv2d_10[0][0]']           \n","                                                                                                  \n"," conv2d_11 (Conv2D)          (None, 32, 32, 128)          147584    ['activation_10[0][0]']       \n","                                                                                                  \n"," activation_11 (Activation)  (None, 32, 32, 128)          0         ['conv2d_11[0][0]']           \n","                                                                                                  \n"," conv2d_transpose_1 (Conv2D  (None, 64, 64, 64)           73792     ['activation_11[0][0]']       \n"," Transpose)                                                                                       \n","                                                                                                  \n"," concatenate_1 (Concatenate  (None, 64, 64, 128)          0         ['conv2d_transpose_1[0][0]',  \n"," )                                                                   'activation_5[0][0]']        \n","                                                                                                  \n"," conv2d_12 (Conv2D)          (None, 64, 64, 64)           73792     ['concatenate_1[0][0]']       \n","                                                                                                  \n"," activation_12 (Activation)  (None, 64, 64, 64)           0         ['conv2d_12[0][0]']           \n","                                                                                                  \n"," conv2d_13 (Conv2D)          (None, 64, 64, 64)           36928     ['activation_12[0][0]']       \n","                                                                                                  \n"," activation_13 (Activation)  (None, 64, 64, 64)           0         ['conv2d_13[0][0]']           \n","                                                                                                  \n"," conv2d_transpose_2 (Conv2D  (None, 128, 128, 32)         18464     ['activation_13[0][0]']       \n"," Transpose)                                                                                       \n","                                                                                                  \n"," concatenate_2 (Concatenate  (None, 128, 128, 64)         0         ['conv2d_transpose_2[0][0]',  \n"," )                                                                   'activation_3[0][0]']        \n","                                                                                                  \n"," conv2d_14 (Conv2D)          (None, 128, 128, 32)         18464     ['concatenate_2[0][0]']       \n","                                                                                                  \n"," activation_14 (Activation)  (None, 128, 128, 32)         0         ['conv2d_14[0][0]']           \n","                                                                                                  \n"," conv2d_15 (Conv2D)          (None, 128, 128, 32)         9248      ['activation_14[0][0]']       \n","                                                                                                  \n"," activation_15 (Activation)  (None, 128, 128, 32)         0         ['conv2d_15[0][0]']           \n","                                                                                                  \n"," conv2d_transpose_3 (Conv2D  (None, 256, 256, 16)         4624      ['activation_15[0][0]']       \n"," Transpose)                                                                                       \n","                                                                                                  \n"," concatenate_3 (Concatenate  (None, 256, 256, 32)         0         ['conv2d_transpose_3[0][0]',  \n"," )                                                                   'activation_1[0][0]']        \n","                                                                                                  \n"," conv2d_16 (Conv2D)          (None, 256, 256, 16)         4624      ['concatenate_3[0][0]']       \n","                                                                                                  \n"," activation_16 (Activation)  (None, 256, 256, 16)         0         ['conv2d_16[0][0]']           \n","                                                                                                  \n"," conv2d_17 (Conv2D)          (None, 256, 256, 16)         2320      ['activation_16[0][0]']       \n","                                                                                                  \n"," activation_17 (Activation)  (None, 256, 256, 16)         0         ['conv2d_17[0][0]']           \n","                                                                                                  \n"," conv2d_18 (Conv2D)          (None, 256, 256, 1)          17        ['activation_17[0][0]']       \n","                                                                                                  \n","==================================================================================================\n","Total params: 2158705 (8.23 MB)\n","Trainable params: 2158705 (8.23 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["model= get_basic_unet() ## Model1\n","# model= get_unet_vgg16() ## Model2\n","# model= get_TransUNet() ## Model3\n","# model= get_DeepLabV3Plus() ## Model4\n","# model = get_ResUNet_v3() ## Model5\n","# model = UNet_with_attention() ## Model6\n","# model = get_multiresunet() ## Model7\n","# model = get_unet_InceptionResNetV2() ## Model8\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"id":"e297b3da-e42c-4ee2-8dda-43cd0016bf00","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e297b3da-e42c-4ee2-8dda-43cd0016bf00","executionInfo":{"status":"ok","timestamp":1749592870690,"user_tz":-360,"elapsed":10901291,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"}},"outputId":"77f78df3-ae35-4c80-eee8-04f3226195c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/60\n","120/120 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9579 - f1_score: 0.0838 - binary_io_u: 0.5066\n","Epoch 1: val_binary_io_u improved from -inf to 0.67598, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 185s 2s/step - loss: 0.1446 - accuracy: 0.9579 - f1_score: 0.0838 - binary_io_u: 0.5066 - val_loss: 0.0812 - val_accuracy: 0.9726 - val_f1_score: 0.5031 - val_binary_io_u: 0.6760 - lr: 0.0010\n","Epoch 2/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9748 - f1_score: 0.6158 - binary_io_u: 0.7276\n","Epoch 2: val_binary_io_u improved from 0.67598 to 0.74847, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 180s 2s/step - loss: 0.0775 - accuracy: 0.9748 - f1_score: 0.6158 - binary_io_u: 0.7276 - val_loss: 0.0615 - val_accuracy: 0.9805 - val_f1_score: 0.6353 - val_binary_io_u: 0.7485 - lr: 0.0010\n","Epoch 3/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9790 - f1_score: 0.6998 - binary_io_u: 0.7710\n","Epoch 3: val_binary_io_u improved from 0.74847 to 0.77143, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 177s 1s/step - loss: 0.0657 - accuracy: 0.9790 - f1_score: 0.6998 - binary_io_u: 0.7710 - val_loss: 0.0585 - val_accuracy: 0.9808 - val_f1_score: 0.6761 - val_binary_io_u: 0.7714 - lr: 0.0010\n","Epoch 4/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9801 - f1_score: 0.7251 - binary_io_u: 0.7846\n","Epoch 4: val_binary_io_u did not improve from 0.77143\n","120/120 [==============================] - 179s 1s/step - loss: 0.0605 - accuracy: 0.9801 - f1_score: 0.7251 - binary_io_u: 0.7846 - val_loss: 0.0578 - val_accuracy: 0.9812 - val_f1_score: 0.6144 - val_binary_io_u: 0.7429 - lr: 0.0010\n","Epoch 5/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9807 - f1_score: 0.7291 - binary_io_u: 0.7886\n","Epoch 5: val_binary_io_u did not improve from 0.77143\n","120/120 [==============================] - 179s 1s/step - loss: 0.0586 - accuracy: 0.9807 - f1_score: 0.7291 - binary_io_u: 0.7886 - val_loss: 0.0652 - val_accuracy: 0.9770 - val_f1_score: 0.6551 - val_binary_io_u: 0.7552 - lr: 0.0010\n","Epoch 6/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9817 - f1_score: 0.7504 - binary_io_u: 0.7995\n","Epoch 6: val_binary_io_u improved from 0.77143 to 0.77702, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 179s 1s/step - loss: 0.0559 - accuracy: 0.9817 - f1_score: 0.7504 - binary_io_u: 0.7995 - val_loss: 0.0530 - val_accuracy: 0.9826 - val_f1_score: 0.6758 - val_binary_io_u: 0.7770 - lr: 0.0010\n","Epoch 7/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9812 - f1_score: 0.7355 - binary_io_u: 0.7932\n","Epoch 7: val_binary_io_u did not improve from 0.77702\n","120/120 [==============================] - 179s 1s/step - loss: 0.0568 - accuracy: 0.9812 - f1_score: 0.7355 - binary_io_u: 0.7932 - val_loss: 0.0541 - val_accuracy: 0.9819 - val_f1_score: 0.6282 - val_binary_io_u: 0.7503 - lr: 0.0010\n","Epoch 8/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0546 - accuracy: 0.9818 - f1_score: 0.7537 - binary_io_u: 0.8007\n","Epoch 8: val_binary_io_u improved from 0.77702 to 0.78471, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 182s 2s/step - loss: 0.0546 - accuracy: 0.9818 - f1_score: 0.7537 - binary_io_u: 0.8007 - val_loss: 0.0512 - val_accuracy: 0.9827 - val_f1_score: 0.6948 - val_binary_io_u: 0.7847 - lr: 0.0010\n","Epoch 9/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9820 - f1_score: 0.7541 - binary_io_u: 0.8024\n","Epoch 9: val_binary_io_u did not improve from 0.78471\n","120/120 [==============================] - 180s 1s/step - loss: 0.0538 - accuracy: 0.9820 - f1_score: 0.7541 - binary_io_u: 0.8024 - val_loss: 0.0536 - val_accuracy: 0.9827 - val_f1_score: 0.6490 - val_binary_io_u: 0.7663 - lr: 0.0010\n","Epoch 10/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9822 - f1_score: 0.7591 - binary_io_u: 0.8043\n","Epoch 10: val_binary_io_u improved from 0.78471 to 0.79081, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 181s 2s/step - loss: 0.0536 - accuracy: 0.9822 - f1_score: 0.7591 - binary_io_u: 0.8043 - val_loss: 0.0512 - val_accuracy: 0.9828 - val_f1_score: 0.7076 - val_binary_io_u: 0.7908 - lr: 0.0010\n","Epoch 11/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9830 - f1_score: 0.7671 - binary_io_u: 0.8126\n","Epoch 11: val_binary_io_u did not improve from 0.79081\n","120/120 [==============================] - 181s 2s/step - loss: 0.0510 - accuracy: 0.9830 - f1_score: 0.7671 - binary_io_u: 0.8126 - val_loss: 0.0493 - val_accuracy: 0.9834 - val_f1_score: 0.7024 - val_binary_io_u: 0.7899 - lr: 0.0010\n","Epoch 12/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9827 - f1_score: 0.7577 - binary_io_u: 0.8096\n","Epoch 12: val_binary_io_u did not improve from 0.79081\n","120/120 [==============================] - 180s 2s/step - loss: 0.0508 - accuracy: 0.9827 - f1_score: 0.7577 - binary_io_u: 0.8096 - val_loss: 0.0647 - val_accuracy: 0.9802 - val_f1_score: 0.6539 - val_binary_io_u: 0.7602 - lr: 0.0010\n","Epoch 13/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9827 - f1_score: 0.7591 - binary_io_u: 0.8087\n","Epoch 13: val_binary_io_u improved from 0.79081 to 0.79503, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 182s 2s/step - loss: 0.0516 - accuracy: 0.9827 - f1_score: 0.7591 - binary_io_u: 0.8087 - val_loss: 0.0484 - val_accuracy: 0.9835 - val_f1_score: 0.7094 - val_binary_io_u: 0.7950 - lr: 0.0010\n","Epoch 14/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9828 - f1_score: 0.7674 - binary_io_u: 0.8111\n","Epoch 14: val_binary_io_u did not improve from 0.79503\n","120/120 [==============================] - 184s 2s/step - loss: 0.0505 - accuracy: 0.9828 - f1_score: 0.7674 - binary_io_u: 0.8111 - val_loss: 0.0516 - val_accuracy: 0.9830 - val_f1_score: 0.6546 - val_binary_io_u: 0.7705 - lr: 0.0010\n","Epoch 15/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9836 - f1_score: 0.7757 - binary_io_u: 0.8184\n","Epoch 15: val_binary_io_u improved from 0.79503 to 0.79716, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 185s 2s/step - loss: 0.0484 - accuracy: 0.9836 - f1_score: 0.7757 - binary_io_u: 0.8184 - val_loss: 0.0479 - val_accuracy: 0.9837 - val_f1_score: 0.7094 - val_binary_io_u: 0.7972 - lr: 0.0010\n","Epoch 16/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9840 - f1_score: 0.7824 - binary_io_u: 0.8216\n","Epoch 16: val_binary_io_u improved from 0.79716 to 0.80044, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 185s 2s/step - loss: 0.0475 - accuracy: 0.9840 - f1_score: 0.7824 - binary_io_u: 0.8216 - val_loss: 0.0479 - val_accuracy: 0.9840 - val_f1_score: 0.7177 - val_binary_io_u: 0.8004 - lr: 0.0010\n","Epoch 17/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9837 - f1_score: 0.7749 - binary_io_u: 0.8184\n","Epoch 17: val_binary_io_u did not improve from 0.80044\n","120/120 [==============================] - 183s 2s/step - loss: 0.0482 - accuracy: 0.9837 - f1_score: 0.7749 - binary_io_u: 0.8184 - val_loss: 0.0484 - val_accuracy: 0.9842 - val_f1_score: 0.7042 - val_binary_io_u: 0.7918 - lr: 0.0010\n","Epoch 18/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9840 - f1_score: 0.7815 - binary_io_u: 0.8224\n","Epoch 18: val_binary_io_u did not improve from 0.80044\n","120/120 [==============================] - 183s 2s/step - loss: 0.0474 - accuracy: 0.9840 - f1_score: 0.7815 - binary_io_u: 0.8224 - val_loss: 0.0492 - val_accuracy: 0.9837 - val_f1_score: 0.6864 - val_binary_io_u: 0.7865 - lr: 0.0010\n","Epoch 19/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9838 - f1_score: 0.7804 - binary_io_u: 0.8201\n","Epoch 19: val_binary_io_u did not improve from 0.80044\n","120/120 [==============================] - 183s 2s/step - loss: 0.0472 - accuracy: 0.9838 - f1_score: 0.7804 - binary_io_u: 0.8201 - val_loss: 0.0487 - val_accuracy: 0.9837 - val_f1_score: 0.7146 - val_binary_io_u: 0.7989 - lr: 0.0010\n","Epoch 20/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9848 - f1_score: 0.7924 - binary_io_u: 0.8301\n","Epoch 20: val_binary_io_u improved from 0.80044 to 0.80649, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 184s 2s/step - loss: 0.0448 - accuracy: 0.9848 - f1_score: 0.7924 - binary_io_u: 0.8301 - val_loss: 0.0450 - val_accuracy: 0.9848 - val_f1_score: 0.7296 - val_binary_io_u: 0.8065 - lr: 0.0010\n","Epoch 21/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9850 - f1_score: 0.7959 - binary_io_u: 0.8321\n","Epoch 21: val_binary_io_u did not improve from 0.80649\n","120/120 [==============================] - 186s 2s/step - loss: 0.0438 - accuracy: 0.9850 - f1_score: 0.7959 - binary_io_u: 0.8321 - val_loss: 0.0446 - val_accuracy: 0.9853 - val_f1_score: 0.7152 - val_binary_io_u: 0.8029 - lr: 0.0010\n","Epoch 22/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9853 - f1_score: 0.7987 - binary_io_u: 0.8348\n","Epoch 22: val_binary_io_u did not improve from 0.80649\n","120/120 [==============================] - 184s 2s/step - loss: 0.0432 - accuracy: 0.9853 - f1_score: 0.7987 - binary_io_u: 0.8348 - val_loss: 0.0459 - val_accuracy: 0.9846 - val_f1_score: 0.6859 - val_binary_io_u: 0.7887 - lr: 0.0010\n","Epoch 23/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9844 - f1_score: 0.7848 - binary_io_u: 0.8256\n","Epoch 23: val_binary_io_u did not improve from 0.80649\n","120/120 [==============================] - 183s 2s/step - loss: 0.0463 - accuracy: 0.9844 - f1_score: 0.7848 - binary_io_u: 0.8256 - val_loss: 0.0514 - val_accuracy: 0.9822 - val_f1_score: 0.7060 - val_binary_io_u: 0.7928 - lr: 0.0010\n","Epoch 24/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9848 - f1_score: 0.7965 - binary_io_u: 0.8313\n","Epoch 24: val_binary_io_u did not improve from 0.80649\n","120/120 [==============================] - 183s 2s/step - loss: 0.0445 - accuracy: 0.9848 - f1_score: 0.7965 - binary_io_u: 0.8313 - val_loss: 0.0450 - val_accuracy: 0.9848 - val_f1_score: 0.7231 - val_binary_io_u: 0.8054 - lr: 0.0010\n","Epoch 25/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9854 - f1_score: 0.8053 - binary_io_u: 0.8367\n","Epoch 25: val_binary_io_u improved from 0.80649 to 0.81450, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 185s 2s/step - loss: 0.0425 - accuracy: 0.9854 - f1_score: 0.8053 - binary_io_u: 0.8367 - val_loss: 0.0427 - val_accuracy: 0.9857 - val_f1_score: 0.7310 - val_binary_io_u: 0.8145 - lr: 0.0010\n","Epoch 26/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9857 - f1_score: 0.8007 - binary_io_u: 0.8395\n","Epoch 26: val_binary_io_u improved from 0.81450 to 0.81812, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 183s 2s/step - loss: 0.0415 - accuracy: 0.9857 - f1_score: 0.8007 - binary_io_u: 0.8395 - val_loss: 0.0424 - val_accuracy: 0.9859 - val_f1_score: 0.7412 - val_binary_io_u: 0.8181 - lr: 0.0010\n","Epoch 27/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9860 - f1_score: 0.8068 - binary_io_u: 0.8421\n","Epoch 27: val_binary_io_u did not improve from 0.81812\n","120/120 [==============================] - 180s 1s/step - loss: 0.0411 - accuracy: 0.9860 - f1_score: 0.8068 - binary_io_u: 0.8421 - val_loss: 0.0427 - val_accuracy: 0.9857 - val_f1_score: 0.7233 - val_binary_io_u: 0.8110 - lr: 0.0010\n","Epoch 28/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9863 - f1_score: 0.8167 - binary_io_u: 0.8461\n","Epoch 28: val_binary_io_u did not improve from 0.81812\n","120/120 [==============================] - 184s 2s/step - loss: 0.0398 - accuracy: 0.9863 - f1_score: 0.8167 - binary_io_u: 0.8461 - val_loss: 0.0431 - val_accuracy: 0.9854 - val_f1_score: 0.7157 - val_binary_io_u: 0.8054 - lr: 0.0010\n","Epoch 29/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9863 - f1_score: 0.8096 - binary_io_u: 0.8454\n","Epoch 29: val_binary_io_u did not improve from 0.81812\n","120/120 [==============================] - 184s 2s/step - loss: 0.0401 - accuracy: 0.9863 - f1_score: 0.8096 - binary_io_u: 0.8454 - val_loss: 0.0419 - val_accuracy: 0.9859 - val_f1_score: 0.7318 - val_binary_io_u: 0.8122 - lr: 0.0010\n","Epoch 30/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9867 - f1_score: 0.8209 - binary_io_u: 0.8495\n","Epoch 30: val_binary_io_u did not improve from 0.81812\n","120/120 [==============================] - 184s 2s/step - loss: 0.0388 - accuracy: 0.9867 - f1_score: 0.8209 - binary_io_u: 0.8495 - val_loss: 0.0408 - val_accuracy: 0.9858 - val_f1_score: 0.7348 - val_binary_io_u: 0.8149 - lr: 0.0010\n","Epoch 31/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9869 - f1_score: 0.8206 - binary_io_u: 0.8521\n","Epoch 31: val_binary_io_u did not improve from 0.81812\n","120/120 [==============================] - 183s 2s/step - loss: 0.0377 - accuracy: 0.9869 - f1_score: 0.8206 - binary_io_u: 0.8521 - val_loss: 0.0447 - val_accuracy: 0.9851 - val_f1_score: 0.6943 - val_binary_io_u: 0.7935 - lr: 0.0010\n","Epoch 32/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9870 - f1_score: 0.8228 - binary_io_u: 0.8534\n","Epoch 32: val_binary_io_u did not improve from 0.81812\n","120/120 [==============================] - 181s 2s/step - loss: 0.0376 - accuracy: 0.9870 - f1_score: 0.8228 - binary_io_u: 0.8534 - val_loss: 0.0423 - val_accuracy: 0.9860 - val_f1_score: 0.7362 - val_binary_io_u: 0.8124 - lr: 0.0010\n","Epoch 33/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9871 - f1_score: 0.8231 - binary_io_u: 0.8542\n","Epoch 33: val_binary_io_u did not improve from 0.81812\n","120/120 [==============================] - 182s 2s/step - loss: 0.0370 - accuracy: 0.9871 - f1_score: 0.8231 - binary_io_u: 0.8542 - val_loss: 0.0406 - val_accuracy: 0.9864 - val_f1_score: 0.7353 - val_binary_io_u: 0.8154 - lr: 0.0010\n","Epoch 34/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9873 - f1_score: 0.8273 - binary_io_u: 0.8561\n","Epoch 34: val_binary_io_u improved from 0.81812 to 0.82808, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 182s 2s/step - loss: 0.0361 - accuracy: 0.9873 - f1_score: 0.8273 - binary_io_u: 0.8561 - val_loss: 0.0402 - val_accuracy: 0.9864 - val_f1_score: 0.7620 - val_binary_io_u: 0.8281 - lr: 0.0010\n","Epoch 35/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9875 - f1_score: 0.8293 - binary_io_u: 0.8581\n","Epoch 35: val_binary_io_u did not improve from 0.82808\n","120/120 [==============================] - 181s 2s/step - loss: 0.0363 - accuracy: 0.9875 - f1_score: 0.8293 - binary_io_u: 0.8581 - val_loss: 0.0397 - val_accuracy: 0.9868 - val_f1_score: 0.7461 - val_binary_io_u: 0.8243 - lr: 0.0010\n","Epoch 36/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9878 - f1_score: 0.8354 - binary_io_u: 0.8613\n","Epoch 36: val_binary_io_u did not improve from 0.82808\n","120/120 [==============================] - 182s 2s/step - loss: 0.0353 - accuracy: 0.9878 - f1_score: 0.8354 - binary_io_u: 0.8613 - val_loss: 0.0409 - val_accuracy: 0.9864 - val_f1_score: 0.7317 - val_binary_io_u: 0.8178 - lr: 0.0010\n","Epoch 37/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9875 - f1_score: 0.8265 - binary_io_u: 0.8578\n","Epoch 37: val_binary_io_u improved from 0.82808 to 0.82826, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 183s 2s/step - loss: 0.0357 - accuracy: 0.9875 - f1_score: 0.8265 - binary_io_u: 0.8578 - val_loss: 0.0378 - val_accuracy: 0.9871 - val_f1_score: 0.7536 - val_binary_io_u: 0.8283 - lr: 0.0010\n","Epoch 38/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9882 - f1_score: 0.8405 - binary_io_u: 0.8650\n","Epoch 38: val_binary_io_u did not improve from 0.82826\n","120/120 [==============================] - 182s 2s/step - loss: 0.0336 - accuracy: 0.9882 - f1_score: 0.8405 - binary_io_u: 0.8650 - val_loss: 0.0380 - val_accuracy: 0.9871 - val_f1_score: 0.7493 - val_binary_io_u: 0.8252 - lr: 0.0010\n","Epoch 39/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9882 - f1_score: 0.8372 - binary_io_u: 0.8652\n","Epoch 39: val_binary_io_u improved from 0.82826 to 0.82979, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 181s 2s/step - loss: 0.0335 - accuracy: 0.9882 - f1_score: 0.8372 - binary_io_u: 0.8652 - val_loss: 0.0386 - val_accuracy: 0.9866 - val_f1_score: 0.7632 - val_binary_io_u: 0.8298 - lr: 0.0010\n","Epoch 40/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9883 - f1_score: 0.8412 - binary_io_u: 0.8667\n","Epoch 40: val_binary_io_u did not improve from 0.82979\n","120/120 [==============================] - 179s 1s/step - loss: 0.0332 - accuracy: 0.9883 - f1_score: 0.8412 - binary_io_u: 0.8667 - val_loss: 0.0380 - val_accuracy: 0.9869 - val_f1_score: 0.7580 - val_binary_io_u: 0.8289 - lr: 0.0010\n","Epoch 41/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9887 - f1_score: 0.8467 - binary_io_u: 0.8706\n","Epoch 41: val_binary_io_u did not improve from 0.82979\n","120/120 [==============================] - 181s 2s/step - loss: 0.0320 - accuracy: 0.9887 - f1_score: 0.8467 - binary_io_u: 0.8706 - val_loss: 0.0373 - val_accuracy: 0.9872 - val_f1_score: 0.7487 - val_binary_io_u: 0.8277 - lr: 0.0010\n","Epoch 42/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9886 - f1_score: 0.8387 - binary_io_u: 0.8691\n","Epoch 42: val_binary_io_u did not improve from 0.82979\n","120/120 [==============================] - 181s 2s/step - loss: 0.0322 - accuracy: 0.9886 - f1_score: 0.8387 - binary_io_u: 0.8691 - val_loss: 0.0394 - val_accuracy: 0.9868 - val_f1_score: 0.7575 - val_binary_io_u: 0.8278 - lr: 0.0010\n","Epoch 43/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9888 - f1_score: 0.8486 - binary_io_u: 0.8713\n","Epoch 43: val_binary_io_u did not improve from 0.82979\n","120/120 [==============================] - 181s 2s/step - loss: 0.0314 - accuracy: 0.9888 - f1_score: 0.8486 - binary_io_u: 0.8713 - val_loss: 0.0372 - val_accuracy: 0.9871 - val_f1_score: 0.7523 - val_binary_io_u: 0.8266 - lr: 0.0010\n","Epoch 44/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9888 - f1_score: 0.8436 - binary_io_u: 0.8722\n","Epoch 44: val_binary_io_u did not improve from 0.82979\n","120/120 [==============================] - 181s 2s/step - loss: 0.0318 - accuracy: 0.9888 - f1_score: 0.8436 - binary_io_u: 0.8722 - val_loss: 0.0389 - val_accuracy: 0.9873 - val_f1_score: 0.7541 - val_binary_io_u: 0.8276 - lr: 0.0010\n","Epoch 45/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9887 - f1_score: 0.8479 - binary_io_u: 0.8706\n","Epoch 45: val_binary_io_u improved from 0.82979 to 0.83349, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 182s 2s/step - loss: 0.0317 - accuracy: 0.9887 - f1_score: 0.8479 - binary_io_u: 0.8706 - val_loss: 0.0375 - val_accuracy: 0.9873 - val_f1_score: 0.7683 - val_binary_io_u: 0.8335 - lr: 0.0010\n","Epoch 46/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9894 - f1_score: 0.8559 - binary_io_u: 0.8787\n","Epoch 46: val_binary_io_u improved from 0.83349 to 0.83408, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 193s 2s/step - loss: 0.0294 - accuracy: 0.9894 - f1_score: 0.8559 - binary_io_u: 0.8787 - val_loss: 0.0370 - val_accuracy: 0.9876 - val_f1_score: 0.7644 - val_binary_io_u: 0.8341 - lr: 0.0010\n","Epoch 47/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9894 - f1_score: 0.8554 - binary_io_u: 0.8783\n","Epoch 47: val_binary_io_u did not improve from 0.83408\n","120/120 [==============================] - 181s 2s/step - loss: 0.0298 - accuracy: 0.9894 - f1_score: 0.8554 - binary_io_u: 0.8783 - val_loss: 0.0373 - val_accuracy: 0.9875 - val_f1_score: 0.7605 - val_binary_io_u: 0.8315 - lr: 0.0010\n","Epoch 48/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9896 - f1_score: 0.8584 - binary_io_u: 0.8798\n","Epoch 48: val_binary_io_u improved from 0.83408 to 0.83487, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 181s 2s/step - loss: 0.0291 - accuracy: 0.9896 - f1_score: 0.8584 - binary_io_u: 0.8798 - val_loss: 0.0377 - val_accuracy: 0.9873 - val_f1_score: 0.7641 - val_binary_io_u: 0.8349 - lr: 0.0010\n","Epoch 49/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9894 - f1_score: 0.8547 - binary_io_u: 0.8781\n","Epoch 49: val_binary_io_u did not improve from 0.83487\n","120/120 [==============================] - 182s 2s/step - loss: 0.0295 - accuracy: 0.9894 - f1_score: 0.8547 - binary_io_u: 0.8781 - val_loss: 0.0383 - val_accuracy: 0.9872 - val_f1_score: 0.7490 - val_binary_io_u: 0.8259 - lr: 0.0010\n","Epoch 50/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9884 - f1_score: 0.8402 - binary_io_u: 0.8677\n","Epoch 50: val_binary_io_u did not improve from 0.83487\n","120/120 [==============================] - 180s 1s/step - loss: 0.0328 - accuracy: 0.9884 - f1_score: 0.8402 - binary_io_u: 0.8677 - val_loss: 0.0379 - val_accuracy: 0.9874 - val_f1_score: 0.7666 - val_binary_io_u: 0.8343 - lr: 0.0010\n","Epoch 51/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9898 - f1_score: 0.8599 - binary_io_u: 0.8825\n","Epoch 51: val_binary_io_u did not improve from 0.83487\n","\n","Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","120/120 [==============================] - 180s 2s/step - loss: 0.0281 - accuracy: 0.9898 - f1_score: 0.8599 - binary_io_u: 0.8825 - val_loss: 0.0373 - val_accuracy: 0.9877 - val_f1_score: 0.7622 - val_binary_io_u: 0.8339 - lr: 0.0010\n","Epoch 52/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9906 - f1_score: 0.8740 - binary_io_u: 0.8910\n","Epoch 52: val_binary_io_u improved from 0.83487 to 0.84042, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 181s 2s/step - loss: 0.0256 - accuracy: 0.9906 - f1_score: 0.8740 - binary_io_u: 0.8910 - val_loss: 0.0354 - val_accuracy: 0.9881 - val_f1_score: 0.7745 - val_binary_io_u: 0.8404 - lr: 5.0000e-04\n","Epoch 53/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9910 - f1_score: 0.8743 - binary_io_u: 0.8953\n","Epoch 53: val_binary_io_u improved from 0.84042 to 0.84280, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 181s 2s/step - loss: 0.0241 - accuracy: 0.9910 - f1_score: 0.8743 - binary_io_u: 0.8953 - val_loss: 0.0362 - val_accuracy: 0.9882 - val_f1_score: 0.7796 - val_binary_io_u: 0.8428 - lr: 5.0000e-04\n","Epoch 54/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9912 - f1_score: 0.8773 - binary_io_u: 0.8974\n","Epoch 54: val_binary_io_u did not improve from 0.84280\n","120/120 [==============================] - 180s 1s/step - loss: 0.0235 - accuracy: 0.9912 - f1_score: 0.8773 - binary_io_u: 0.8974 - val_loss: 0.0384 - val_accuracy: 0.9881 - val_f1_score: 0.7807 - val_binary_io_u: 0.8423 - lr: 5.0000e-04\n","Epoch 55/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9913 - f1_score: 0.8804 - binary_io_u: 0.8984\n","Epoch 55: val_binary_io_u improved from 0.84280 to 0.84424, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 180s 2s/step - loss: 0.0233 - accuracy: 0.9913 - f1_score: 0.8804 - binary_io_u: 0.8984 - val_loss: 0.0363 - val_accuracy: 0.9883 - val_f1_score: 0.7829 - val_binary_io_u: 0.8442 - lr: 5.0000e-04\n","Epoch 56/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9915 - f1_score: 0.8828 - binary_io_u: 0.9005\n","Epoch 56: val_binary_io_u did not improve from 0.84424\n","120/120 [==============================] - 179s 1s/step - loss: 0.0227 - accuracy: 0.9915 - f1_score: 0.8828 - binary_io_u: 0.9005 - val_loss: 0.0383 - val_accuracy: 0.9881 - val_f1_score: 0.7772 - val_binary_io_u: 0.8403 - lr: 5.0000e-04\n","Epoch 57/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9915 - f1_score: 0.8851 - binary_io_u: 0.9015\n","Epoch 57: val_binary_io_u did not improve from 0.84424\n","\n","Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","120/120 [==============================] - 177s 1s/step - loss: 0.0223 - accuracy: 0.9915 - f1_score: 0.8851 - binary_io_u: 0.9015 - val_loss: 0.0374 - val_accuracy: 0.9882 - val_f1_score: 0.7796 - val_binary_io_u: 0.8422 - lr: 5.0000e-04\n","Epoch 58/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9919 - f1_score: 0.8924 - binary_io_u: 0.9056\n","Epoch 58: val_binary_io_u improved from 0.84424 to 0.84491, saving model to /content/drive/MyDrive/TRIAL_v1/segmentation_task/saved_outputs_segmentation_AUC/Basic_Unet_1500/saved_models/best_Basic_Unet_1500.h5\n","120/120 [==============================] - 179s 1s/step - loss: 0.0211 - accuracy: 0.9919 - f1_score: 0.8924 - binary_io_u: 0.9056 - val_loss: 0.0374 - val_accuracy: 0.9883 - val_f1_score: 0.7844 - val_binary_io_u: 0.8449 - lr: 2.5000e-04\n","Epoch 59/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9921 - f1_score: 0.8962 - binary_io_u: 0.9084\n","Epoch 59: val_binary_io_u did not improve from 0.84491\n","120/120 [==============================] - 179s 1s/step - loss: 0.0203 - accuracy: 0.9921 - f1_score: 0.8962 - binary_io_u: 0.9084 - val_loss: 0.0392 - val_accuracy: 0.9883 - val_f1_score: 0.7781 - val_binary_io_u: 0.8420 - lr: 2.5000e-04\n","Epoch 60/60\n","120/120 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9922 - f1_score: 0.8968 - binary_io_u: 0.9091\n","Epoch 60: val_binary_io_u did not improve from 0.84491\n","120/120 [==============================] - 179s 1s/step - loss: 0.0201 - accuracy: 0.9922 - f1_score: 0.8968 - binary_io_u: 0.9091 - val_loss: 0.0383 - val_accuracy: 0.9883 - val_f1_score: 0.7839 - val_binary_io_u: 0.8438 - lr: 2.5000e-04\n"]}],"source":["# training the model and saving the best model as a check point\n","best_model_name, train_history, total_time = fit_and_save_best_model(model_name, model)"]},{"cell_type":"markdown","id":"58066d61-295d-4a1e-bd34-35fad6fddeb6","metadata":{"id":"58066d61-295d-4a1e-bd34-35fad6fddeb6"},"source":["## Loading the best model"]},{"cell_type":"code","execution_count":null,"id":"2efedf22-df76-4922-b09c-6357388b6458","metadata":{"id":"2efedf22-df76-4922-b09c-6357388b6458"},"outputs":[],"source":["##Now, load the best model\n","best_model = ks.models.load_model(os.path.join(PATH_TO_SAVE_MODEL, best_model_name),\n","                                  custom_objects={'f1_score': f1_score,\n","                                                  'binary_io_u':iou_score_binary})\n","\n","# best_model =model"]},{"cell_type":"markdown","id":"5b6e079b-b8bf-4be8-97d5-bcbc737b4f75","metadata":{"id":"5b6e079b-b8bf-4be8-97d5-bcbc737b4f75"},"source":["## Visualize model predictions"]},{"cell_type":"code","execution_count":null,"id":"0e84b3ee-95d2-46ac-aee4-77e8515e600a","metadata":{"id":"0e84b3ee-95d2-46ac-aee4-77e8515e600a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749592879025,"user_tz":-360,"elapsed":1021,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"}},"outputId":"f2f8ea20-463d-43a3-f8b1-2611509ef7f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 1s/step\n"]}],"source":["# Get predictions from the model\n","predictions = best_model.predict(x_test, verbose=1)\n","\n","USER_DETERMINED_THRESHOLD = 0.5\n","thresholded_predictions  = (predictions  >= USER_DETERMINED_THRESHOLD)\n","# Removing the color channel\n","thresholded_predictions_without_color_channel = np.squeeze(thresholded_predictions, axis=-1)"]},{"cell_type":"markdown","source":["# AUC"],"metadata":{"id":"Y1AxBTO-I2ZY"},"id":"Y1AxBTO-I2ZY"},{"cell_type":"code","source":["# Add this section *after* prediction is made with the best model\n","# i.e., after: predictions = best_model.predict(x_test, verbose=1)\n","\n","from sklearn.metrics import roc_auc_score\n","\n","# Flatten masks for AUC computation\n","y_true_flat = y_test.flatten()\n","y_score_flat = predictions.flatten()  # Predicted probabilities"],"metadata":{"id":"G4cmkCP-I3mP"},"id":"G4cmkCP-I3mP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Bootstrapping AUC\n","n_iterations = 100\n","rng = np.random.default_rng(SEED)\n","bootstrap_aucs = []\n","\n","for _ in range(n_iterations):\n","    indices = rng.integers(0, len(y_true_flat), len(y_true_flat))\n","    y_true_sample = y_true_flat[indices]\n","    y_score_sample = y_score_flat[indices]\n","\n","    # Ensure both classes are present in the sample\n","    if len(np.unique(y_true_sample)) < 2:\n","        continue\n","\n","    auc = roc_auc_score(y_true_sample, y_score_sample)\n","    bootstrap_aucs.append(auc)"],"metadata":{"id":"5WlUncgNI3p_"},"id":"5WlUncgNI3p_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate statistics\n","mean_auc = np.mean(bootstrap_aucs)\n","std_auc = np.std(bootstrap_aucs)\n","lower_ci = np.percentile(bootstrap_aucs, 2.5)\n","upper_ci = np.percentile(bootstrap_aucs, 97.5)\n","\n","# Print AUC statistics\n","print(f\"Bootstrapped AUC: Mean={mean_auc:.4f}, Std={std_auc:.4f}, 95% CI=({lower_ci:.4f}, {upper_ci:.4f})\")\n","\n","# Optional saving\n","if SAVE_RESULTS:\n","    auc_df = pd.DataFrame({'AUC Scores': bootstrap_aucs})\n","    auc_df.to_csv(os.path.join(PATH_TO_SAVE_RESULT, f\"{model_name}_bootstrapped_auc.csv\"), index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"shKoLcaQI3vX","executionInfo":{"status":"ok","timestamp":1749592955515,"user_tz":-360,"elapsed":14,"user":{"displayName":"Mahrin Mehrin","userId":"03538273846602354774"}},"outputId":"168c9634-2113-4ad3-f570-f0fbc0b6690f"},"id":"shKoLcaQI3vX","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bootstrapped AUC: Mean=0.9868, Std=0.0002, 95% CI=(0.9864, 0.9872)\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["2b874d87-e287-4dda-a0d6-79ff516adc3f"],"gpuType":"T4","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}